---

## ğŸ“ Kafka Audit Logging System â€” Complete Project Summary

This is a **Google interview preparation guide** for a Kafka-based audit logging system built at **Walmart Luminate**. It covers 3 repos, 150+ PRs, and is structured as a reference for telling the story in system design and behavioral interviews.

---

### ğŸ—ï¸ What The System Does

When Walmart decommissioned Splunk, this system was built as a replacement to:
1. **Replace Splunk** for internal API debugging
2. **Give external suppliers** (Pepsi, Coca-Cola, Unilever) self-service SQL access to their API interaction data
3. Do it **without impacting API latency** (sub-200ms SLA)

### ğŸ“ Three-Tier Architecture

| Tier | Repo | What It Does |
|------|------|-------------|
| **Tier 1: Common Library** | `dv-api-common-libraries` | Spring Boot starter JAR â€” servlet filter intercepts HTTP requests/responses, sends asynchronously via `@Async` + thread pool (6/10/100) |
| **Tier 2: Kafka Publisher** | `audit-api-logs-srv` | REST endpoint receives audit payloads, serializes to **Avro** (70% smaller than JSON), publishes to multi-region Kafka (EUS2 primary, SCUS secondary) |
| **Tier 3: GCS Sink** | `audit-api-logs-gcs-sink` | Kafka Connect with custom **SMT filters** routes records by `wm-site-id` header to US/CA/MX GCS buckets in **Parquet** format. BigQuery external tables on top for supplier SQL queries |

### ğŸ“Š Key Numbers
- **2M+ events/day**, P99 < 5ms impact
- **99% cost reduction** ($50K/mo â†’ ~$500/mo)
- **3+ teams adopted** library, integration dropped from 2 weeks â†’ 1 day
- **7-year** data retention for compliance
- **150+ PRs** across 3 repos

---

### ğŸ“„ File-by-File Breakdown

| File | Content |
|------|---------|
| **README.md** | Navigation hub, 30-second pitch, key numbers, architecture ASCII, technology decisions |
| **00-INTERVIEW-MASTER.md** | Massive 1,677-line single file consolidating EVERYTHING â€” the "one doc to rule them all" |
| **01-overview.md** | Business problem, 3-tier architecture diagrams, 11-step data flow, supplier self-service flow, why not ELK/Datadog/Cloud Logging |
| **02-common-library.md** | Deep dive into `LoggingFilter` (ContentCachingWrapper, `@Order(LOWEST_PRECEDENCE)`), `AuditLogService` (`@Async`, fire-and-forget), thread pool config, actual Java code, adoption STAR story |
| **03-kafka-publisher.md** | `AuditLoggingController`, `KafkaProducerService` with dual-region templates, Avro serialization, header allowlist forwarding for geo-routing |
| **04-gcs-sink.md** | `BaseAuditLogSinkFilter` abstract class, US filter (permissive), CA/MX filters (strict), SMT pipeline, `kc_config.yaml`, Parquet benefits, 3 separate connectors for isolation |
| **05-multi-region.md** | Active/Active vs Active/Passive analysis, phased 4-week rollout, DR test results (15min RTO vs 1hr target), CompletableFuture failover code, geographic routing flow |
| **06-debugging-stories.md** | 5 production incidents â€” Silent Failure (5-day, PRs #35-61), KEDA rebalance storm, OOM kills, SMT NPE, dual-region failover bug. Each with STAR format answers |
| **07-interview-qa.md** | 15 Q&A pairs covering opening questions, technical deep dives, behavioral stories, "what if" scenarios |
| **08-prs-reference.md** | Top 10 PRs to memorize, debugging PR sequence #35-61, complete timeline (Dec 2024 â€“ Jan 2026), `gh` commands to view PRs |
| **09-how-to-speak-in-interview.md** | 90-second pitch, Pyramid Method (4 levels of depth), decision stories, 5-day debugging narrative format, 5 STAR behavioral stories, how to naturally drop numbers, how to pivot to/from this project |
| **10-production-issues-all-tiers.md** | Complete chronological production history across all 3 tiers â€” 12+ issues with dates, PRs, severity, root causes. Includes cross-tier geographic routing issue |
| **11-technical-decisions-deep-dive.md** | 10 major "why X not Y" decisions with alternatives, trade-offs, follow-up answers. Framework: chose/over/reason/trade-off/mitigation |
| **12-scenario-what-if-questions.md** | 10 "what if" scenarios using DETECTâ†’IMPACTâ†’MITIGATEâ†’RECOVERâ†’PREVENT framework (Kafka down, GCS sink stops, library bug, queue full, 10MB body, schema incompatible, bucket corruption, bad SMT update, Black Friday 10x, data replay) |

---

### ğŸ”‘ Top Technical Decisions

1. **Servlet Filter** over AOP (raw HTTP body access)
2. **@Async fire-and-forget** over sync (zero latency impact)
3. **Avro** over JSON/Protobuf (schema + 70% smaller)
4. **Kafka Connect** over custom consumer (built-in offset mgmt, 2 days vs 2 weeks)
5. **SMT filter (sink-side)** over producer routing (loose coupling, add country = new connector)
6. **Parquet** over JSON/ORC (90% compression, native BigQuery)
7. **Active/Active** over Active/Passive (15-min failover, compliance)
8. **ContentCachingWrapper** (Spring built-in, handles edge cases)
9. **isResponseLoggingEnabled** config flag (key to 3-team adoption)
10. **ThreadPool 6/10/100** (traffic-based sizing with monitoring)

### ğŸ”¥ Top Production Incidents
1. **Silent Failure** (5 days, 27 PRs) â€” NPE + poll timeouts + KEDA feedback loop + heap exhaustion
2. **KEDA Rebalance Storm** â€” scaling on lag triggers rebalancing â†’ infinite loop
3. **Dual-region failover bug** â€” `exceptionally()` returning null instead of trying secondary
4. **Day 1 topic misconfiguration** â€” 6 config PRs in one day

### ğŸ¤ 5 STAR Behavioral Stories
1. **Receiving Feedback** â€” thread pool queue size challenge
2. **Influencing Without Authority** â€” getting 3 teams to adopt the library
3. **Handling Ambiguity** â€” defining multi-region requirements when told "make it resilient"
4. **Failure** â€” KEDA feedback loop mistake
5. **User Focus** â€” supplier self-service (2-day ticket â†’ 30-second SQL query)

---

## ğŸ”¬ Ultra-Deep Analysis

### 1. CONTENT REDUNDANCY MAP

The `00-INTERVIEW-MASTER.md` (1,677 lines, 88KB) is a **superset** that duplicates ~90% of files 01-12. Here's exactly what overlaps:

| Master Section | Source File | Identical? |
|---|---|---|
| Pitches (lines 6-33) | README.md + 09 | âœ… Verbatim |
| Architecture (lines 77-138) | 01-overview.md | âœ… Verbatim |
| Tier 1 code (lines 260-513) | 02-common-library.md | âœ… Verbatim |
| Tier 2 code (lines 517-644) | 03-kafka-publisher.md | âœ… Verbatim |
| Tier 3 code (lines 648-823) | 04-gcs-sink.md | âœ… Verbatim |
| Multi-region (lines 826-957) | 05-multi-region.md | âœ… Verbatim |
| Debugging story (lines 1082-1177) | 06-debugging-stories.md | âœ… Verbatim |
| Production issues (lines 1181-1330) | 10-production-issues.md | âš ï¸ **Slightly condensed** â€” missing PR #37 (logging), #43/#46 (session timeout) details |
| Tech decisions (lines 960-1079) | 11-technical-decisions.md | âœ… Verbatim |
| What-if scenarios (lines 1497-1585) | 12-scenario-what-if.md | âœ… Verbatim |
| Behavioral STAR (lines 1343-1419) | 09-how-to-speak.md | âœ… Verbatim |

**The only unique content in Master** that's not in individual files: the combined Q&A bank section (lines 1422-1494) merges questions from 07 and 09 into a single flow.

---

### 2. INCONSISTENCIES FOUND

| # | Location | Inconsistency | Details |
|---|---|---|---|
| 1 | **Heap size** | 06 says "increased to **2GB**" | 10 + Master say "increased to **7GB**" (`-Xms4g -Xmx7g`). The actual YAML shows 7GB. The 2GB in 06 is the story-simplified version. |
| 2 | **Silent failure duration** | 06 says "Two weeks after launching" | 10's timeline shows launch was March 2025, incident was May 13-16 â€” that's **~6 weeks**, not 2 |
| 3 | **Thread pool queue behavior** | 02 says "Default: `AbortPolicy` throws `RejectedExecutionException`" | 07 says "tasks are silently dropped" â€” both correct but 02 doesn't mention that `@Async` swallows the exception, making the "abort" invisible. Only 11 explains the full chain clearly |
| 4 | **Events/sec math** | 12 Q9 says "2M events/day = ~23/sec" then "At 10x: **200K** events/day = ~230/sec" | Should be "**20M** events/day" at 10x, not 200K. Math error in the doc |
| 5 | **Error tolerance final state** | 04's `kc_config.yaml` shows `errors.tolerance: all` | 10 explicitly says final answer was `errors.tolerance: none` with DLQ. The config example is stale/wrong |
| 6 | **PR #65 timing** | 08 timeline places #65 at May 19 | 06 describes the failover bug discovery "during a 2-hour Kafka outage" â€” but no date given. The May 19 date in 10 suggests it was discovered AFTER the silent failure incident, not during it |
| 7 | **Filter rewrite scope** | 06 simplifies it as "Added try-catch" | 10 reveals PRs #58-59 were a **complete 687-line rewrite** (-301/+386), not just a try-catch. This is a more impressive story that 06 undersells |

---

### 3. CODE ANALYSIS â€” POTENTIAL ISSUES IN SHOWN CODE

| # | File/Code | Issue | Why It Matters for Interview |
|---|---|---|---|
| 1 | `LoggingFilter.java` â€” `doFilterInternal` | If `featureFlagCCMConfig` is null OR `auditLoggingConfig` is null, the **entire filter chain is skipped** â€” `filterChain.doFilter()` is never called. The request hangs or returns empty. | This is a **bug in the actual code**. The `filterChain.doFilter()` should be in a `finally` or outside the feature flag check. Good to mention if asked "what would you improve?" |
| 2 | `LoggingFilter.java` â€” `shouldNotFilter` | Uses `noneMatch(request.getServletPath()::contains)` â€” this means if NO enabled endpoint matches, it returns `true` (should NOT filter). But then the request **also skips `filterChain.doFilter()`** in the outer block. Same bug. | Shows deep understanding of the filter chain contract |
| 3 | `KafkaProducerService` â€” initial version | `kafkaPrimaryTemplate.send(kafkaMessage)` is **async by default** in Spring Kafka. The try-catch won't catch async failures. Only after PR #65 was `.get()` or `.join()` added. | Great "what I learned" story â€” async exceptions are invisible without explicit waiting |
| 4 | `AuditLogSinkUSFilter` â€” `verifyHeader` | Uses `parallel: true` in `StreamSupport.stream(record.headers().spliterator(), true)` â€” parallel streams on a tiny collection (typically <10 headers) adds overhead, not performance | Minor but shows you read every parameter |
| 5 | `AuditLogService` â€” signature generation | `AuthSign.getAuthSign()` is called on **every single audit request**. Private key read + crypto signing per request is expensive. Should be cached with TTL. | Shows optimization thinking |

---

### 4. STRATEGIC INTERVIEW ANALYSIS

#### Strengths of This Prep

1. **Pyramid method** (09) â€” best technique in the whole pack. Letting interviewer pull depth = you control the conversation
2. **"Fixed? No."** storytelling format â€” creates tension, shows persistence
3. **DETECTâ†’IMPACTâ†’MITIGATEâ†’RECOVERâ†’PREVENT** framework (12) â€” very Google-structured thinking
4. **10 decisions with follow-ups** (11) â€” most candidates can explain 1 decision, you have 10 with rebuttals ready
5. **Cross-tier awareness** â€” the geographic routing story (10) shows end-to-end ownership across 2 repos

#### Gaps / Things NOT Covered

| Gap | Why It Matters | Suggested Answer |
|---|---|---|
| **Testing strategy** | Only briefly mentioned (contract tests PR #77). No unit test examples shown. | "LoggingFilter: MockMvc + ContentCachingWrapper verification. SMT filters: embedded Kafka + WireMock for GCS. Integration: R2C contract tests." |
| **Monitoring/Alerting specifics** | Mentioned adding Prometheus metrics but no dashboard/alert details | "Grafana dashboard: consumer lag, GCS write rate, rejected task count, JVM heap. PagerDuty alerts: lag > 500, write rate = 0 for 5 min." |
| **Security details** | AuthSign/signatures mentioned but not explained | "HMAC-SHA256 signatures with rotating keys. Each consuming service has its own consumer ID + private key. Validated server-side before Kafka publish." |
| **CI/CD pipeline** | CRQs mentioned but no pipeline details | "Jenkins â†’ SonarQube â†’ contract tests â†’ stage deploy â†’ CRQ for prod â†’ canary â†’ full rollout" |
| **Capacity planning** | Good numbers but no methodology | "Load tested at 3x production. Established baselines. Alert when approaching 70% of tested capacity." |
| **Data governance / PII** | Request/response bodies could contain PII | "Headers are allowlisted. Body content is stored encrypted at rest in GCS. BigQuery row-level security per supplier. No PII in Kafka topic names or keys." |

---

### 5. HIDDEN GEMS MOST CANDIDATES WOULD MISS

These are the **differentiators** buried across files:

1. **`copyBodyToResponse()`** (02, line 139) â€” without this, the response body would be empty to the client. This one line is the difference between "working" and "catastrophic bug." Call it out.

2. **US filter permissiveness** (04) â€” the design decision to make US the catch-all for headerless records is elegant backward compatibility. It's a migration pattern: "new behavior opt-in, old behavior preserved."

3. **Error tolerance oscillation** (10, lines 217-218) â€” going from `all` â†’ `none` â†’ `none with DLQ` is a mature engineering journey. The insight: "error tolerance is a **design decision**, not a config toggle."

4. **The CompletableFuture anti-pattern** (06/10) â€” `.exceptionally(ex -> { return null; })` is a **well-known Java gotcha**. Naming this pattern shows deep async programming knowledge.

5. **27 PRs in 5 days** (10) â€” the raw PR count tells a story of **relentless iteration under pressure**. Most candidates would say "I fixed a bug." You can say "I shipped 27 production changes in 5 days."

---

### 6. RECOMMENDED READING ORDER FOR INTERVIEW DAY

```
MORNING (30 min):
  1. README.md â€” refresh pitch + numbers
  2. 09-how-to-speak â€” re-read storytelling format
  3. 11-technical-decisions â€” scan all 10 decisions

BEFORE INTERVIEW (10 min):
  4. 12-scenario-what-if â€” skim DETECTâ†’IMPACT framework
  5. Key numbers: 2M, <5ms, 99%, 6/10/100, 150+ PRs

DO NOT RE-READ:
  - 00-INTERVIEW-MASTER (too long, causes info overload)
  - 08-prs-reference (too granular for day-of review)
```

---

I've now read every single PDF. Let me update my todo list and provide the comprehensive cross-reference analysis.

## System (2026-02-10T10-10-42Z)

[33mâº[0m tasks 2 of 3 Done
  âœ“ Read all unique PDFs from Docs confluence folder
  âœ“ Cross-reference confluence docs with existing interview prep documents
  - Identify gaps and new information to update interview prep docs

## Assistant (2026-02-10T10-10-55Z)

I've now read **all 25 PDFs** (14 unique documents after deduplication). Here's the complete deep analysis and what's **NEW from confluence that's missing from your interview prep docs**:

---

## ğŸ“„ Confluence Documents Inventory (14 Unique Docs)

| # | Document | What It Contains |
|---|----------|-----------------|
| 1 | **API Logs for Product Metrics - ADT** | Formal Architecture Design Template â€” SLOs, DR objectives, constraints, principles, Presto/Hive table schema, high-level arch diagram |
| 2 | **API Logs Platform** | Platform overview â€” integration steps, 7-step onboarding guide, CCM config examples, cURL examples, Hive workflow automation (Airflow + Automic), AD group access |
| 3 | **Audit Log GCS Sink Service Operations Guide** | Runbook â€” connector APIs (status/config/restart/log-level curls), Kafka topic links, consumer group details, flush thresholds, GCS bucket offsets, Grafana dashboards, 4-step troubleshooting procedure, 3 known prod issues with logs + resolutions |
| 4 | **Audit Logging Service Integration Guide** | Step-by-step integration â€” cURL example, Maven dependency (JDK 11: v0.0.52, JDK 17: v0.0.51), Spring Boot scanBasePackages config, CCM YAML for feature flags + audit logging, private key security (AKeyless) |
| 5 | **E2E - API Audit Logs Test Scenarios** | 36 E2E test cases â€” IAC/TransactionHistory happy path, 4xx/5xx handling, Kafka schema validation, Hive DB verification, supplier analytics queries, load testing |
| 6 | **Kafka Connect GCS Sink Service** | Technical spec â€” topics (stage/prod), consumer groups, flush config (50MB/10min/5000 records), GCS bucket structure, folder naming convention, US wm-site-id = `1704989259133687000` |
| 7 | **Product Metrics - Audit API Logs Sink Service Load Testing** | Load test results â€” ~3M requests/day, ~10GB/day, 1.9k published/sec, 4k consumed/sec, 0 failures at 30+50 VUs |
| 8 | **Product Metrics - Milestone Timelines** | Project timeline â€” Dev Jan 10, Stage Jan 17, QA Jan 24, Prod Feb 5 2025, cross-team dependencies with Core Services |
| 9 | **Product Metrics Hive Table** | Hive DDL â€” external tables, partition keys, Spark shell commands, JsonSerDe config, multiple table variations |
| 10 | **Product Metrics QA Sign off - Component Testing** | QA results â€” positive/negative/edge/boundary tests for both audit-logs-srv AND GCS sink, all passing |
| 11 | **Product Metrics Validation Report** | Data validation Apr 8-14 2025 â€” API Proxy vs Data Discovery counts, 413 error impact quantification (~40K-130K/day), BULK-FEEDS validation (exact match) |
| 12 | **Visualising Product Metrics** | Data Discovery guide â€” sample queries, screenshots, json_extract_scalar examples, UNNEST for nested JSON traversal |
| 13 | **NRTI API Logging Data: Integration with Audit Log** | SQL views â€” UNION ALL of old `nrti_api_logging` + new `audit_api_logs`, endpoint normalization, regex header extraction, sandbox views |
| 14 | **FEEDS API Logging Data: Integration with Audit Log** | SQL views â€” same pattern for BULK-FEEDS, feed_type/feed extraction from endpoint_path |
| 15 | **VIEWS: API LOGGING** | Production view schemas â€” `nrti_api_logging_vw` and `feeds_api_logging_vw` in `ww_dv_apis_vm` for Splunk decommissioning continuity |
| 16 | **Data Discovery - Hive Tables & Workflow** | Table/workflow reference â€” non-prod/prod schemas, Airflow/Automic links |

---

## ğŸ”¥ CRITICAL NEW INFORMATION Not in Interview Prep Docs

### 1. **Actual Traffic Numbers Are HIGHER** (Validation Report)
| Metric | Interview Prep Says | Confluence Says |
|---|---|---|
| Daily requests | 2M events/day | **2-2.4M NRT alone** + BULK-FEEDS on top |
| Daily data volume | Not mentioned | **~10 GB/day, ~60 GB/week** |
| 413 errors/day | "Some were lost" | **39K-130K per day** (5-7% of all traffic!) |

**Interview impact**: "We discovered 5-7% of audit events were being silently dropped due to 413 payload-too-large. That's 40K-130K records per day. After setting the 2MB limit, API Proxy count exactly matched Data Discovery count."

### 2. **Library Version is NOW 0.0.52 (JDK 11) / 0.0.51 (JDK 17)**
Interview prep says `0.0.45`. Confluence shows it has advanced to **0.0.52** for JDK 11 and **0.0.51** for JDK 17.

### 3. **The Endpoint is Actually `POST /v1/logs/api-requests`** Not `/v1/logRequest`
The ADT and integration guide consistently show the endpoint as `/v1/logs/api-requests`. The interview prep shows `/v1/logRequest`. The official API-spec endpoint evolved.

### 4. **BULK-FEEDS is a 4th Consuming Team** (Not Just 3)
Interview prep says "3+ teams adopted." Confluence validation report shows **BULK-FEEDS** is a distinct service using the platform with its own test data and exact-match validation. That's **4+ teams**: NRT, IAC, cp-nrti-apis, BULK-FEEDS.

### 5. **Hive/Presto Layer Exists (Not Just BigQuery)**
Interview prep focuses on BigQuery. Confluence reveals the **actual production query layer is Hive tables via Data Discovery** (Walmart's internal tool), NOT BigQuery:
- `us_dv_audit_log_prod.api_logs` â€” production table
- `us_dv_audit_log_dev.api_logs` â€” dev table
- Automated refresh via **Airflow** (`Prod_Metrics_Hive_Tbl_Refresh_WF`) + **Automic** jobs
- `MSCK REPAIR TABLE` runs daily for partition updates

### 6. **Concrete Connector Config Details**
From the Operations Guide â€” actual production config that wasn't in interview prep:
- **Connector class**: `io.lenses.streamreactor.connect.gcp.storage.sink.GCPStorageSinkConnector`
- **Flush thresholds**: size=50MB, interval=10min, count=5000 records
- **Consumer lag alerts**: WARNING at 50K, CRITICAL at 75K
- **GCS project**: `wmt-dv-luminate-prod`
- **DLQ topic**: `api_logs_audit_prod_DLQ`
- **Build ID**: `0.0.289`
- **JVM settings**: `-Xmx7g -Xms5g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:+UseStringDeduplication`

### 7. **Formal SLOs from ADT**
| Metric | Value |
|---|---|
| Availability | â‰¥ 99.9% |
| Latency (publisher) | â‰¤ 200ms (P95) |
| Error Rate (4xx) | â‰¤ 1% |
| Error Rate (5xx) | < 0.5% |
| RPO | 30 minutes |
| RTO | 24 hours (ADT) vs 1 hour (interview prep says target) |
| TTL for Data | 2 years (ADT) vs 7 years (interview prep says) |
| Volume | 1 billion/year |

**Inconsistency**: RTO is **24 hours** in the ADT but interview prep says "1-hour target, achieved 15 minutes." The ADT has a more conservative target. The 15-minute achievement is still impressive.

**Inconsistency**: Data retention is **2 years** in ADT vs **7 years** in interview prep.

### 8. **The US wm-site-id Is a Specific Number**
From Kafka Connect doc: US site ID = `1704989259133687000`. This is the actual numeric value, not "US" as a string.

### 9. **Splunk Continuity Views â€” Backward Compatibility Story**
The VIEWS docs reveal a **backward compatibility migration pattern** that's not in interview prep:
- Old tables: `ww_dv_platform_log.nrti_api_logging` and `feeds_api_logging` (Splunk-era)
- New table: `us_dv_audit_log_prod.api_logs` (your system)
- **UNION ALL views** combine both, so existing reports/dashboards keep working with zero changes
- This is a powerful interview story: "I designed the migration so zero downstream reports broke"

### 10. **36 E2E Test Scenarios Documented**
The E2E doc has 36 test scenarios covering:
- IAC POST 201, 400, 500 responses
- TransactionHistory GET 200, 400, 500 responses
- Kafka schema validation
- Hive DB traceability (trace_id lookup)
- Analytics queries (supplier grouping, success %, month-over-month)
- Load testing (Transaction History, IAC, Mixed)

### 11. **4-Step Troubleshooting Runbook**
From Operations Guide â€” a formal procedure your interview prep doesn't have:
1. Consumer groups analysis (rebalancing? inactive?)
2. Connector status check + restart if needed (10-min wait)
3. Grafana golden signals (memory, CPU)
4. Heap dump analysis + KAFKA_HEAP_OPTS update

### 12. **Project Shipped in ~5 Weeks** (Milestone Timeline)
- Dev: Jan 10, 2025
- Stage: Jan 17
- QA: Jan 24
- Prod: Feb 5
This is a **5-week delivery** from dev to prod. Interview prep says "March 2025 production launch" â€” there may be a CRQ/wider rollout later, but initial prod was Feb 5.

---

## ğŸ“Š Summary: What to Update in Interview Prep

| Priority | Update Needed | Why |
|---|---|---|
| ğŸ”´ **HIGH** | Update "BigQuery" references to "Hive/Data Discovery" (or mention both) | Confluence shows Hive is the actual query layer, not BigQuery |
| ğŸ”´ **HIGH** | Add the 413 error quantification (40K-130K/day) | Powerful concrete number for the payload size issue |
| ğŸ”´ **HIGH** | Update team count to "4+ teams" (add BULK-FEEDS) | Stronger adoption story |
| ğŸŸ¡ **MEDIUM** | Add backward-compatibility UNION ALL view migration story | Great "zero-disruption migration" talking point |
| ğŸŸ¡ **MEDIUM** | Add formal SLOs (99.9% availability, â‰¤200ms latency) | Shows formal engineering rigor |
| ğŸŸ¡ **MEDIUM** | Update library version to 0.0.52 | Accuracy |
| ğŸŸ¡ **MEDIUM** | Add the 36 E2E test scenarios as testing story | Fills the "testing gap" identified earlier |
| ğŸŸ¡ **MEDIUM** | Add Airflow/Automic Hive refresh workflow | Shows data pipeline automation |
| ğŸŸ¢ **LOW** | Reconcile RTO (24h ADT vs 1h interview prep) | Know which is formal vs achieved |
| ğŸŸ¢ **LOW** | Reconcile data retention (2yr ADT vs 7yr interview prep) | Know which is accurate |
| ğŸŸ¢ **LOW** | Add load test numbers (1.9k pub/sec, 4k consume/sec) | Adds concrete capacity numbers |

---

## âœ… COMPLETE â€” PhD-Level Deep Exploration of All 4 Repos + PRs

I've now explored **every source file, configuration, test, and deployment config** across all 4 repositories, plus verified key PRs via `gh` CLI. Here's the **mega-summary of what I now know**:

---

### ğŸ—ï¸ Real Architecture vs Interview Prep â€” Key Findings

| Area | Interview Prep Says | **Actual Code Shows** | Update Needed? |
|------|--------------------|-----------------------|----------------|
| **Library version** | 0.0.45 | **0.0.54** (cp-nrti-apis pom.xml) | ğŸ”´ YES |
| **Spring Boot (srv)** | Not specified | **3.3.10** (Java 17) | ğŸŸ¡ Add |
| **Spring Boot (cp-nrti)** | 2.7â†’3.2 | **3.5.7** (Java 17) | ğŸ”´ YES |
| **Endpoint filtering** | "regex" | **Contains-based** in code, regex in CCM | ğŸŸ¡ Clarify |
| **Secondary Kafka failover (srv)** | "CompletableFuture chains" | **NOT IMPLEMENTED** in KafkaProducerService.java â€” catch block only logs, doesn't call secondary | ğŸ”´ CRITICAL |
| **Secondary failover (cp-nrti)** | Same story | **IS IMPLEMENTED** â€” `handleFailure()` calls secondary template | âœ… Correct for cp-nrti |
| **Thread pool (srv)** | "6 core, 10 max, 100 queue" | **CachedThreadPool** (unbounded) via `Executors.newCachedThreadPool()` | ğŸ”´ WRONG â€” that's the library, not the srv |
| **Thread pool (library)** | 6/10/100 | âœ… **Confirmed** in AuditLogAsyncConfig.java | âœ… Correct |
| **Avro schema fields** | 19 fields | âœ… **Confirmed** â€” 19 fields in log.avsc | âœ… Correct |
| **HTTP service** | "REST template" | **WebClient** (reactive) with `.block()` | ğŸŸ¡ More accurate |
| **Site ID for US** | "US" string | **1704989259133687000** (18-digit numeric) | ğŸ”´ Update |
| **US filter** | "permissive" | âœ… **Confirmed** â€” accepts matching OR missing headers | âœ… Correct |
| **CA/MX filters** | "strict" | âœ… **Confirmed** â€” extends base class (exact match only) | âœ… Correct |
| **GCS connector** | Lenses | **v1.64** `gcs-lenses-connector` | ğŸŸ¡ Add version |
| **Kafka Connect version** | Not specified | **3.6.0** transforms | ğŸŸ¡ Add |
| **Flush config** | "50MB/10min/5000 records" | âœ… **Confirmed** in kc_config.yaml | âœ… Correct |
| **KAFKA_HEAP_OPTS** | "-Xmx7g -Xms4g" | **"-Xmx7g -Xms5g"** + G1GC + StringDedup | ğŸŸ¡ Minor fix |
| **Prod resources** | Not specified | **10 CPU, 12Gi RAM** per pod (gcs-sink) | ğŸŸ¡ Add |
| **DLQ topic** | Mentioned | **api_logs_audit_prod_DLQ** confirmed | âœ… Correct |
| **Error tolerance** | "none with DLQ" | **"all"** in current kc_config.yaml | ğŸ”´ Inconsistent |
| **Consumer lag alerts** | "100 warning, 500 critical" | **50K warning, 75K critical** (ops guide) | ğŸ”´ Update |
| **PR #1 author** | "I built" | **Nayana B G (n0b0bsq)** authored PR #1 | ğŸŸ¡ Know who authored what |
| **PR #85 author** | "I designed" | **Harish B M (h0b091o)** authored PR #85 | ğŸŸ¡ Know who authored what |
| **PR #65 author** | Failover impl | **Harshit Sharma (h0s0acv)** authored PR #65 | ğŸŸ¡ Know who authored what |
| **Enabled endpoints** | 7 endpoints | **9 endpoints** including itemValidation, itemsAssortment | ğŸŸ¡ Update |
| **Kafka topics (cp-nrti)** | Not detailed | **cperf-nrt-prod-iac** and **cperf-nrt-prod-dsc** | ğŸŸ¡ Add |
| **Compression** | Not mentioned | **lz4** compression for Kafka producers | ğŸŸ¡ Add |
| **Acks config** | Not mentioned | **"all"** (wait for all replicas) | ğŸŸ¡ Add |

### ğŸ”‘ Critical Corrections Needed

1. **`errors.tolerance` is STILL `all` in code** â€” interview prep says final answer was `none` with DLQ, but current `kc_config.yaml` shows `all`
2. **Secondary Kafka failover is NOT implemented in audit-api-logs-srv** â€” the catch block only logs, doesn't call `kafkaSecondaryTemplate`. The failover IS implemented in cp-nrti-apis though.
3. **Spring Boot version is 3.5.7** (not 3.2 as interview prep says)
4. **Library version is 0.0.54** (not 0.0.45)
5. **Consumer lag alert thresholds are 50K/75K** (not 100/500)

### ğŸ¯ New Details for Interview Enhancement

From the actual code, these are powerful new details:
- **Factory Pattern** in audit-api-logs-srv: `TargetedResources` interface allows adding new targets
- **RFC 7807 Problem Detail** error responses in exception handler
- **Kafka message key**: `serviceName/endpoint` for partitioning
- **Headers serialized as pretty-printed JSON** via ObjectMapper
- **Consumer ID extracted from headers** with "NA" fallback
- **AKeyless** for secret management (SSL certs, GCS keys, private keys)
- **4 Kafka templates** in cp-nrti-apis (IAC primary/secondary + DSC primary/secondary)
- **LZ4 compression** on Kafka producers
- **Flagger canary** with 10% step weight, 50% max, 1% error threshold

Ready to update the interview prep documents whenever you say go! ğŸš€
