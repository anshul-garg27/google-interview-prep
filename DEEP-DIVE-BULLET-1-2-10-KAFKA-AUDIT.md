# DEEP DIVE: Bullet 1, 2 & 10 - Kafka-Based Audit Logging System

> **Resume Bullets Covered:**
> - Bullet 1: Kafka-Based Audit Logging System
> - Bullet 2: Common Library JAR (Spring Boot Starter)
> - Bullet 10: Multi-Region Kafka Architecture

---

## TABLE OF CONTENTS

1. [System Overview & Why We Built This](#1-system-overview--why-we-built-this)
2. [Complete Architecture](#2-complete-architecture)
3. [Component Deep Dive](#3-component-deep-dive)
   - [3.1 dv-api-common-libraries (Spring Boot Starter JAR)](#31-dv-api-common-libraries)
   - [3.2 audit-api-logs-srv (Kafka Publisher Service)](#32-audit-api-logs-srv)
   - [3.3 audit-api-logs-gcs-sink (Kafka Connect GCS Sink)](#33-audit-api-logs-gcs-sink)
4. [All PRs - Complete History](#4-all-prs---complete-history)
5. [Multi-Region Architecture (Bullet 10)](#5-multi-region-architecture)
6. [Technology Decisions & Alternatives](#6-technology-decisions--alternatives)
7. [Interview Q&A Bank](#7-interview-qa-bank)
8. [Code Walkthrough](#8-code-walkthrough)
9. [Debugging Stories](#9-debugging-stories)
10. [Flow Diagrams](#10-flow-diagrams)

---

## 1. SYSTEM OVERVIEW & WHY WE BUILT THIS

### The Problem - Splunk Decommissioning & Business Context

> **ğŸ”¥ THE TRIGGER:** Walmart was decommissioning Splunk enterprise-wide. We needed a new logging solution fast.

Walmart Luminate (Data Ventures) has multiple APIs serving **external suppliers** (Pepsi, Coca-Cola, Unilever, P&G):
- **NRT APIs** (cp-nrti-apis) - Real-time inventory data
- **Inventory Status** - Store/DC inventory
- **Transaction Events** - Historical transaction data

**Two Critical Business Requirements:**

1. **Splunk Replacement:**
   - Splunk was being decommissioned company-wide
   - We needed API-level logging for debugging and monitoring
   - No existing solution met our latency requirements

2. **Supplier Data Access:**
   - External suppliers needed visibility into their API interactions
   - "Show me all my failed API calls from last week"
   - "Why did this particular request fail?"
   - Required queryable, long-term storage (not just logs)

**The Architecture Decision:**
```
Before (Splunk Era):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ cp-nrti-apisâ”‚ â”€â”€â”€â–º â”‚   Splunk    â”‚  â† Decommissioning!
â”‚             â”‚      â”‚ (expensive) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After (Our Solution):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ cp-nrti-apisâ”‚ â”€â”€â”€â–º â”‚audit-api-   â”‚ â”€â”€â”€â–º â”‚  Kafka  â”‚ â”€â”€â”€â–º â”‚ GCS/BQ    â”‚
â”‚ (common lib)â”‚      â”‚logs-srv     â”‚      â”‚         â”‚      â”‚ Parquet   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â”‚
                                                               â–¼
                                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                        â”‚ Suppliers â”‚
                                                        â”‚ can query â”‚
                                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Not Just Use Another Log Aggregator?**
- ELK/Elasticsearch: Expensive at scale, not designed for long-term compliance
- Datadog: Cost prohibitive for 2M+ events/day
- Cloud Logging: Not queryable by external suppliers

**Our approach:** Treat audit logs as **DATA**, not just logs. Store in columnar format (Parquet), make queryable (BigQuery), give suppliers access.

### The Solution

A **three-tier audit logging system**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CONSUMING SERVICES                                â”‚
â”‚              (cp-nrti-apis, inventory-status-srv, etc.)                    â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              dv-api-common-libraries (JAR)                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚LoggingFilter â”‚â†’ â”‚AuditLogService  â”‚â†’ â”‚ HTTP POST to        â”‚   â”‚   â”‚
â”‚  â”‚  â”‚(Intercepts)  â”‚  â”‚(@Async)         â”‚  â”‚ audit-api-logs-srv  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        audit-api-logs-srv                                   â”‚
â”‚                     (Kafka Producer Service)                                â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚AuditLogging      â”‚ â†’  â”‚KafkaProducer    â”‚ â†’  â”‚ Kafka Topic        â”‚    â”‚
â”‚  â”‚Controller        â”‚    â”‚Service          â”‚    â”‚ api_logs_audit_prodâ”‚    â”‚
â”‚  â”‚(POST /v1/logReq) â”‚    â”‚(Avro + Headers) â”‚    â”‚ (Multi-Region)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      audit-api-logs-gcs-sink                                â”‚
â”‚                    (Kafka Connect Sink Connector)                           â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Kafka Consumer  â”‚ â†’ â”‚SMT Filters      â”‚ â†’ â”‚GCS Buckets (Parquet)     â”‚  â”‚
â”‚  â”‚(Connect Worker)â”‚   â”‚(US/CA/MX)       â”‚   â”‚ - audit-api-logs-us-prod â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚wm-site-id headerâ”‚   â”‚ - audit-api-logs-ca-prod â”‚  â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ - audit-api-logs-mx-prod â”‚  â”‚
â”‚                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BigQuery                                          â”‚
â”‚                    (Analytics & Compliance)                                 â”‚
â”‚                                                                             â”‚
â”‚   External tables pointing to GCS Parquet files                            â”‚
â”‚   - Query audit logs with SQL                                               â”‚
â”‚   - Create dashboards in Looker/Grafana                                     â”‚
â”‚   - Long-term retention for compliance                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Business Value

| Metric | Before (Splunk) | After (Our Solution) |
|--------|-----------------|---------------------|
| Time to add audit logging | 2-3 weeks per service | 1 day (add dependency) |
| Log format consistency | 0% (ad-hoc) | 100% (standardized) |
| Cross-service correlation | Not possible | Full trace ID support |
| Query capability | Splunk SPL (limited) | SQL in BigQuery |
| Supplier access | âŒ Not possible | âœ… Direct query access |
| Data retention | 30 days (Splunk cost) | 7 years (GCS cheap) |
| Multi-region support | Manual | Automatic routing |
| Cost per event | ~$0.001 (Splunk) | ~$0.00001 (GCS) |

### The Three-Tier Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 1: COMMON LIBRARY (dv-api-common-libraries)                              â”‚
â”‚                                                                                 â”‚
â”‚  â€¢ Import as Maven dependency                                                   â”‚
â”‚  â€¢ LoggingFilter intercepts all HTTP requests                                   â”‚
â”‚  â€¢ ContentCachingWrapper captures request/response bodies                       â”‚
â”‚  â€¢ @Async sends to audit-api-logs-srv (non-blocking)                           â”‚
â”‚  â€¢ Services: cp-nrti-apis, inventory-status-srv, etc.                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ HTTP POST (async)
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 2: AUDIT API SERVICE (audit-api-logs-srv)                                â”‚
â”‚                                                                                 â”‚
â”‚  â€¢ REST endpoint: POST /v1/logRequest                                          â”‚
â”‚  â€¢ Converts payload to Avro (70% size reduction)                               â”‚
â”‚  â€¢ Publishes to Kafka with wm-site-id header                                   â”‚
â”‚  â€¢ Dual-region publishing (EUS2 + SCUS)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ Kafka messages (Avro)
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 3: GCS SINK (audit-api-logs-gcs-sink)                                    â”‚
â”‚                                                                                 â”‚
â”‚  â€¢ Kafka Connect with 3 parallel connectors                                    â”‚
â”‚  â€¢ SMT filters route by wm-site-id header                                      â”‚
â”‚  â€¢ US bucket (permissive), CA bucket (strict), MX bucket (strict)             â”‚
â”‚  â€¢ Writes Parquet format (90% compression)                                     â”‚
â”‚  â€¢ BigQuery external tables for SQL queries                                    â”‚
â”‚  â€¢ SUPPLIERS CAN QUERY THEIR OWN DATA!                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. COMPLETE ARCHITECTURE

### High-Level Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 SUPPLIER REQUEST                                      â”‚
â”‚                          (External API Consumer)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              AZURE FRONT DOOR / ISTIO                                 â”‚
â”‚                           (Load Balancing, SSL Termination)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               cp-nrti-apis (K8s Pod)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         dv-api-common-libraries                                  â”‚â”‚
â”‚  â”‚                                                                                  â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚â”‚
â”‚  â”‚   â”‚  1. LoggingFilter â”‚â—€â”€â”€ @Order(LOWEST_PRECEDENCE)                            â”‚â”‚
â”‚  â”‚   â”‚     (Before)      â”‚    Wraps request/response in ContentCachingWrapper      â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚â”‚
â”‚  â”‚            â”‚                                                                     â”‚â”‚
â”‚  â”‚            â–¼                                                                     â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚â”‚
â”‚  â”‚   â”‚  2. Security     â”‚    Auth filter, CORS, etc.                               â”‚â”‚
â”‚  â”‚   â”‚     Filters      â”‚                                                          â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚â”‚
â”‚  â”‚            â”‚                                                                     â”‚â”‚
â”‚  â”‚            â–¼                                                                     â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚â”‚
â”‚  â”‚   â”‚  3. Controller   â”‚    Business logic (IAC, DSD, Inventory Status)           â”‚â”‚
â”‚  â”‚   â”‚    (API Handler) â”‚                                                          â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚â”‚
â”‚  â”‚            â”‚                                                                     â”‚â”‚
â”‚  â”‚            â–¼                                                                     â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚â”‚
â”‚  â”‚   â”‚  4. LoggingFilter â”‚â—€â”€â”€ Reads cached request/response bodies                 â”‚â”‚
â”‚  â”‚   â”‚     (After)       â”‚    Builds AuditLogPayload                               â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚â”‚
â”‚  â”‚            â”‚                                                                     â”‚â”‚
â”‚  â”‚            â–¼                                                                     â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚â”‚
â”‚  â”‚   â”‚ 5. AuditLogServiceâ”‚â”€â”€â”€â–¶â”‚  ThreadPoolTask   â”‚ Core: 6, Max: 10              â”‚â”‚
â”‚  â”‚   â”‚    (@Async)       â”‚    â”‚  Executor         â”‚ Queue: 100                     â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚â”‚
â”‚  â”‚                                      â”‚                                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                         â”‚                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚    â”‚            Response to Supplier     â”‚                                        â”‚ â”‚
â”‚    â”‚            (Non-blocking)           â”‚                                        â”‚ â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ HTTP POST (Async)
                                           â”‚ Headers: WM_CONSUMER.ID, Signature
                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            audit-api-logs-srv (K8s Pod)                               â”‚
â”‚                                                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚AuditLogging     â”‚â”€â”€â”€â–¶â”‚KafkaProducerServiceâ”‚â”€â”€â”€â–¶â”‚ Kafka Topic                 â”‚  â”‚
â”‚   â”‚Controller       â”‚    â”‚                    â”‚    â”‚ api_logs_audit_prod         â”‚  â”‚
â”‚   â”‚POST /v1/logReq  â”‚    â”‚ - Avro serializationâ”‚   â”‚                             â”‚  â”‚
â”‚   â”‚                 â”‚    â”‚ - Add wm-site-id   â”‚    â”‚ Partitioned by:             â”‚  â”‚
â”‚   â”‚                 â”‚    â”‚ - Set Kafka headers â”‚   â”‚ - service_name              â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - endpoint_name             â”‚  â”‚
â”‚                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ Kafka Messages (Avro)
                                           â”‚ Key: service_name|endpoint_name
                                           â”‚ Headers: wm-site-id, correlation-id
                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      KAFKA CLUSTER (Multi-Region)                                     â”‚
â”‚                                                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚       EUS2 (East US 2)      â”‚    â”‚      SCUS (South Central)   â”‚                â”‚
â”‚   â”‚                             â”‚    â”‚                             â”‚                â”‚
â”‚   â”‚  Topic: api_logs_audit_prod â”‚    â”‚  Topic: api_logs_audit_prod â”‚                â”‚
â”‚   â”‚  Partitions: 12             â”‚    â”‚  Partitions: 12             â”‚                â”‚
â”‚   â”‚  Replication: 3             â”‚    â”‚  Replication: 3             â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                                       â”‚
â”‚   Active/Active Configuration - Both regions receive all messages                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ Kafka Connect Consumer
                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     audit-api-logs-gcs-sink (Kafka Connect Worker)                    â”‚
â”‚                                                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚                        3 CONNECTORS RUNNING IN PARALLEL                        â”‚ â”‚
â”‚   â”‚                                                                                â”‚ â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â”‚
â”‚   â”‚  â”‚ US Connector       â”‚  â”‚ CA Connector       â”‚  â”‚ MX Connector       â”‚      â”‚ â”‚
â”‚   â”‚  â”‚                    â”‚  â”‚                    â”‚  â”‚                    â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ SMT: FilterUS      â”‚  â”‚ SMT: FilterCA      â”‚  â”‚ SMT: FilterMX      â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ wm-site-id: US     â”‚  â”‚ wm-site-id: CA     â”‚  â”‚ wm-site-id: MX     â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ OR no header       â”‚  â”‚ (strict match)     â”‚  â”‚ (strict match)     â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ (default catch-all)â”‚  â”‚                    â”‚  â”‚                    â”‚      â”‚ â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”‚
â”‚   â”‚            â”‚                       â”‚                       â”‚                  â”‚ â”‚
â”‚   â”‚            â–¼                       â–¼                       â–¼                  â”‚ â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â”‚
â”‚   â”‚  â”‚ GCS Bucket         â”‚  â”‚ GCS Bucket         â”‚  â”‚ GCS Bucket         â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ audit-api-logs-us  â”‚  â”‚ audit-api-logs-ca  â”‚  â”‚ audit-api-logs-mx  â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ -prod              â”‚  â”‚ -prod              â”‚  â”‚ -prod              â”‚      â”‚ â”‚
â”‚   â”‚  â”‚                    â”‚  â”‚                    â”‚  â”‚                    â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ Format: Parquet    â”‚  â”‚ Format: Parquet    â”‚  â”‚ Format: Parquet    â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ Partitioned by:    â”‚  â”‚ Partitioned by:    â”‚  â”‚ Partitioned by:    â”‚      â”‚ â”‚
â”‚   â”‚  â”‚ /service/date/     â”‚  â”‚ /service/date/     â”‚  â”‚ /service/date/     â”‚      â”‚ â”‚
â”‚   â”‚  â”‚  endpoint/         â”‚  â”‚  endpoint/         â”‚  â”‚  endpoint/         â”‚      â”‚ â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ External Tables
                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  BIGQUERY                                             â”‚
â”‚                                                                                       â”‚
â”‚   SELECT                                                                             â”‚
â”‚     service_name,                                                                    â”‚
â”‚     endpoint_name,                                                                   â”‚
â”‚     COUNT(*) as request_count,                                                       â”‚
â”‚     AVG(response_ts - request_ts) as avg_latency_sec                                â”‚
â”‚   FROM `project.audit_logs.api_logs_us`                                             â”‚
â”‚   WHERE DATE(created_ts) = CURRENT_DATE()                                           â”‚
â”‚   GROUP BY service_name, endpoint_name                                              â”‚
â”‚                                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. COMPONENT DEEP DIVE

### 3.1 dv-api-common-libraries (BULLET 2: Common Library JAR)

> **ğŸ¯ RESUME BULLET 2:** "Developed reusable Spring Boot starter JAR for audit logging, adopted by 3+ teams, reducing integration time from 2 weeks to 1 day"

**Repository:** `gecgithub01.walmart.com/dsi-dataventures-luminate/dv-api-common-libraries`
**Maven:** `com.walmart:dv-api-common-libraries:0.0.45`

#### Why This Is a Separate Resume Bullet

This library demonstrates:
1. **Reusability thinking** - Saw 3 teams building same thing, proposed shared solution
2. **API design** - Zero-code integration (just add Maven dependency)
3. **Spring internals** - Filters, auto-config, async, thread pools
4. **Adoption skills** - Got buy-in, wrote docs, helped teams integrate

#### Purpose
Reusable Spring Boot JAR that any service can import to get audit logging capabilities.

**How NRTI (cp-nrti-apis) uses it:**
```xml
<!-- In cp-nrti-apis pom.xml -->
<dependency>
    <groupId>com.walmart</groupId>
    <artifactId>dv-api-common-libraries</artifactId>
    <version>0.0.45</version>
</dependency>
```

That's it! No code changes in NRTI. The library auto-configures:
1. LoggingFilter (via @Component scan)
2. AuditLogService (via @Autowired)
3. ThreadPoolTaskExecutor (via @Bean)

**Why It Works Without Code Changes (Spring Boot Magic):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPRING BOOT AUTO-CONFIGURATION                            â”‚
â”‚                                                                              â”‚
â”‚  When cp-nrti-apis starts:                                                  â”‚
â”‚                                                                              â”‚
â”‚  1. Spring scans classpath â†’ finds dv-api-common-libraries JAR             â”‚
â”‚                                                                              â”‚
â”‚  2. @ComponentScan picks up LoggingFilter (@Component)                      â”‚
â”‚     â†’ Filter automatically added to filter chain                            â”‚
â”‚                                                                              â”‚
â”‚  3. @EnableAsync + @Bean in AuditLogAsyncConfig                            â”‚
â”‚     â†’ ThreadPoolTaskExecutor registered                                     â”‚
â”‚                                                                              â”‚
â”‚  4. @ManagedConfiguration (CCM)                                             â”‚
â”‚     â†’ Loads config for feature flags, endpoint patterns                     â”‚
â”‚                                                                              â”‚
â”‚  5. @Autowired injects AuditLogService into LoggingFilter                  â”‚
â”‚                                                                              â”‚
â”‚  Result: First HTTP request â†’ LoggingFilter intercepts â†’ Audit works!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Design Philosophy:**
- **Convention over configuration** - Works out of the box
- **Opt-out, not opt-in** - Audit is ON by default (configurable via CCM)
- **Fail silently** - Audit failure never breaks the API
- **Zero coupling** - Consumer service doesn't know about audit internals

**The Flow:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              cp-nrti-apis                                     â”‚
â”‚                                                                               â”‚
â”‚   Supplier Request â”€â”€â–º LoggingFilter â”€â”€â–º Controller â”€â”€â–º Response             â”‚
â”‚        â”‚                    â”‚               â”‚              â”‚                  â”‚
â”‚        â”‚                    â”‚               â–¼              â”‚                  â”‚
â”‚        â”‚                    â”‚         Business Logic       â”‚                  â”‚
â”‚        â”‚                    â”‚               â”‚              â”‚                  â”‚
â”‚        â”‚                    â”‚               â–¼              â”‚                  â”‚
â”‚        â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â–º AuditLogService â—„â”€â”€â”€â”˜                  â”‚
â”‚        â”‚                                    â”‚                                 â”‚
â”‚        â”‚                              @Async (non-blocking)                   â”‚
â”‚        â”‚                                    â”‚                                 â”‚
â”‚        â–¼                                    â–¼                                 â”‚
â”‚   Response sent to              HTTP POST to audit-api-logs-srv              â”‚
â”‚   Supplier (fast!)                         â”‚                                  â”‚
â”‚                                            â–¼                                  â”‚
â”‚                                      Kafka â†’ GCS                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Files

```
src/main/java/com/walmart/dv/
â”œâ”€â”€ filters/
â”‚   â””â”€â”€ LoggingFilter.java          # HTTP interceptor (129 lines)
â”œâ”€â”€ services/
â”‚   â””â”€â”€ AuditLogService.java        # Async HTTP client (111 lines)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ AuditLoggingConfig.java     # CCM configuration interface
â”‚   â””â”€â”€ auditlog/
â”‚       â””â”€â”€ AuditLogAsyncConfig.java # Thread pool config
â”œâ”€â”€ payloads/
â”‚   â””â”€â”€ AuditLogPayload.java        # Data model (59 lines)
â””â”€â”€ utils/
    â””â”€â”€ AuditLogFilterUtil.java     # Helper methods (177 lines)
```

#### Code Deep Dive

**1. LoggingFilter.java - The Heart**

```java
@Slf4j
@Order(Ordered.LOWEST_PRECEDENCE)  // â¬…ï¸ CRITICAL: Runs LAST
@Component
public class LoggingFilter extends OncePerRequestFilter {

  @ManagedConfiguration
  FeatureFlagCCMConfig featureFlagCCMConfig;  // Feature flag from CCM

  @ManagedConfiguration
  AuditLoggingConfig auditLoggingConfig;      // Audit config from CCM

  @Autowired
  AuditLogService auditLogService;            // Async sender

  @Override
  protected void doFilterInternal(HttpServletRequest request,
                                  HttpServletResponse response,
                                  FilterChain filterChain) {

    // 1. Check if audit logging is enabled
    if (featureFlagCCMConfig.isAuditLogEnabled()) {

      // 2. Skip actuator endpoints
      if (!request.getRequestURI().contains("/actuator")) {

        // 3. Check if endpoint should be audited
        if (!shouldNotFilter(request)) {

          // 4. Wrap request/response to capture bodies
          ContentCachingRequestWrapper requestWrapper =
              new ContentCachingRequestWrapper(request);
          ContentCachingResponseWrapper responseWrapper =
              new ContentCachingResponseWrapper(response);

          long requestTimestamp = Instant.now().getEpochSecond();

          // 5. Execute the actual request
          filterChain.doFilter(requestWrapper, responseWrapper);

          long responseTimestamp = Instant.now().getEpochSecond();

          // 6. Extract bodies from cache
          String requestBody = convertByteArrayToString(
              requestWrapper.getContentAsByteArray());
          String responseBody = convertByteArrayToString(
              responseWrapper.getContentAsByteArray());

          // 7. Build audit payload
          AuditLogPayload payload = prepareRequestForAuditLog(
              requestBody, responseBody, request, response,
              requestTimestamp, responseTimestamp);

          // 8. Send async (non-blocking)
          auditLogService.sendAuditLogRequest(payload, auditLoggingConfig);

          // 9. CRITICAL: Copy body back to response
          responseWrapper.copyBodyToResponse();
        }
      }
    }
  }

  @Override
  protected boolean shouldNotFilter(HttpServletRequest request) {
    // Only audit configured endpoints
    return auditLoggingConfig.enabledEndpoints().stream()
        .noneMatch(request.getServletPath()::contains);
  }
}
```

**Why `LOWEST_PRECEDENCE`?**
- Ensures we run AFTER all security filters
- We capture the FINAL state of request/response
- If auth fails, we still log the failed attempt

**Why `ContentCachingWrapper`?**
- HTTP streams can only be read ONCE
- Without wrapping, controller would get empty body
- Wrapper caches the bytes for re-reading

**2. AuditLogService.java - Async Processing**

```java
@Service
@Slf4j
public class AuditLogService {

  private final AuditHttpServiceImpl httpService;
  private final ObjectMapper objectMapper;

  @Async  // â¬…ï¸ CRITICAL: Non-blocking
  public void sendAuditLogRequest(AuditLogPayload payload,
                                  AuditLoggingConfig config) {
    try {
      URI uri = URI.create(config.getUriPath());
      JsonNode payloadJson = objectMapper.convertValue(payload, JsonNode.class);
      HttpHeaders headers = getAuditLogHeaders(config);

      httpService.sendHttpRequest(
          uri,
          HttpMethod.POST,
          new HttpEntity<>(payloadJson, headers),
          Void.class
      );
    } catch (Exception e) {
      // Log but don't throw - audit failure shouldn't break API
      log.error("Failed to send audit log: {}", payload.getRequestId(), e);
    }
  }

  private HttpHeaders getAuditLogHeaders(AuditLoggingConfig config) {
    HttpHeaders headers = new HttpHeaders();
    SignatureDetails signature = AuthSign.getAuthSign(
        config.getWmConsumerId(),
        getPrivateKey(config.getAuditPrivateKeyPath()),
        config.getKeyVersion()
    );

    headers.set("WM_CONSUMER.ID", config.getWmConsumerId());
    headers.set("WM_SEC.AUTH_SIGNATURE", signature.getSignature());
    headers.set("WM_SEC.KEY_VERSION", config.getKeyVersion());
    headers.set("WM_CONSUMER.INTIMESTAMP", signature.getTimestamp());
    headers.setContentType(MediaType.APPLICATION_JSON);

    return headers;
  }
}
```

**Why `@Async`?**
- API response time should not depend on audit logging
- If audit service is slow, user still gets fast response
- Queue absorbs traffic spikes (capacity: 100)

**3. AuditLogAsyncConfig.java - Thread Pool**

```java
@Configuration
@EnableAsync
public class AuditLogAsyncConfig {

    @Bean
    public Executor taskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(6);       // Always-running threads
        executor.setMaxPoolSize(10);       // Can scale up to this
        executor.setQueueCapacity(100);    // Buffer before rejection
        executor.setThreadNamePrefix("Audit-log-executor-");
        executor.initialize();
        return executor;
    }
}
```

**Why 6 core, 10 max, 100 queue?**
- **6 core**: Handles normal traffic (assumes ~100 requests/sec)
- **10 max**: Handles spikes without creating too many threads
- **100 queue**: Buffer during traffic bursts; prevents OOM

**What happens when queue is full?**
- Default: `AbortPolicy` throws `RejectedExecutionException`
- Our code catches exceptions - audit log silently dropped
- This is acceptable - audit is best-effort, not critical path

**4. AuditLogPayload.java - Data Model**

```java
@Data
@Builder
public class AuditLogPayload {

    @JsonProperty("request_id")
    private String requestId;           // UUID for correlation

    @JsonProperty("service_name")
    private String serviceName;         // e.g., "cp-nrti-apis"

    @JsonProperty("endpoint_name")
    private String endpointName;        // e.g., "/iac/v1/inventory"

    @JsonProperty("path")
    private String path;                // Full path with query params

    @JsonProperty("method")
    private String method;              // GET, POST, etc.

    @JsonProperty("request_body")
    private String requestBody;         // Captured request JSON

    @JsonProperty("response_body")
    private String responseBody;        // Captured response JSON

    @JsonProperty("response_code")
    private int responseCode;           // HTTP status (200, 400, 500)

    @JsonProperty("error_reason")
    private String errorReason;         // Error message if any

    @JsonProperty("request_ts")
    private long requestTimestamp;      // Epoch seconds

    @JsonProperty("response_ts")
    private long responseTimestamp;     // Epoch seconds

    @JsonProperty("headers")
    private Map<String,String> headers; // All HTTP headers

    @JsonProperty("trace_id")
    private String traceId;             // Distributed tracing ID
}
```

#### PRs - dv-api-common-libraries

| PR# | State | Title | Key Changes |
|-----|-------|-------|-------------|
| **#1** | MERGED | Audit log service | **THE MAIN PR** - Complete library implementation |
| #2 | OPEN | Release/develop sonar fix | SonarQube fixes |
| **#3** | MERGED | adding webclient in jar | Added WebClient for reactive HTTP |
| #4 | MERGED | snkyfix | Snyk security fixes |
| #5 | MERGED | Update pom.xml | Dependency updates |
| **#6** | MERGED | Develop | Core functionality merge |
| #7 | OPEN | FIx review bugs | Bug fixes |
| #8-10 | CLOSED | JDK17 upgrade attempts | Multiple attempts at Java 17 |
| **#11** | MERGED | enable endpoints with regex | **Regex-based endpoint filtering** |
| #12 | MERGED | Create CODEOWNERS | Code ownership |
| **#15** | MERGED | Version update for gtp bom | GTP BOM update for Java 17 |

---

### 3.2 audit-api-logs-srv

**Repository:** `gecgithub01.walmart.com/dsi-dataventures-luminate/audit-api-logs-srv`

#### Purpose
REST API service that receives audit logs from consumers and publishes to Kafka.

#### Key Files

```
src/main/java/com/walmart/audit/
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ AuditLoggingController.java    # REST endpoint
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ KafkaProducerService.java      # Kafka publisher
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ AuditKafkaPayloadKey.java      # Kafka message key
â”‚   â””â”€â”€ AuditKafkaPayloadBody.java     # Kafka message body
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ AvroUtils.java                 # Avro serialization
â””â”€â”€ common/config/
    â””â”€â”€ AuditLogsKafkaCCMConfig.java   # CCM configuration
```

#### Code Deep Dive

**1. AuditLoggingController.java - REST Endpoint**

```java
@Slf4j
@RestController
@RequestMapping(produces = MediaType.APPLICATION_JSON_VALUE)
public class AuditLoggingController implements AuditLogsApi {

  @Autowired
  LoggingRequestService loggingRequestService;

  @PostMapping("v1/logRequest")
  public ResponseEntity<Boolean> saveRequest(@RequestBody LoggingApiRequest request) {
    Boolean result = loggingRequestService.processLoggingRequest(request);
    return ResponseEntity.status(HttpStatus.NO_CONTENT).body(result);
  }

  @Override  // From OpenAPI generated interface
  public ResponseEntity<Void> saveApiLog(LoggingApiRequest request) {
    loggingRequestService.processLoggingRequest(request);
    return new ResponseEntity<>(HttpStatus.NO_CONTENT);
  }
}
```

**Why `NO_CONTENT` (204)?**
- No response body needed - caller doesn't wait for confirmation
- Faster response - saves bandwidth
- Standard pattern for fire-and-forget operations

**2. KafkaProducerService.java - Kafka Publisher**

```java
@Slf4j
@Service
public class KafkaProducerService implements TargetedResources {

  @ManagedConfiguration
  AuditLogsKafkaCCMConfig auditLogsKafkaCCMConfig;

  @Autowired
  private KafkaTemplate<String, Message<LogEvent>> kafkaPrimaryTemplate;

  @Autowired
  private KafkaTemplate<String, Message<LogEvent>> kafkaSecondaryTemplate;

  public void publishMessageToTopic(LoggingApiRequest request) {

    String topicName = auditLogsKafkaCCMConfig.getAuditLoggingKafkaTopicName();
    Message<LogEvent> kafkaMessage = prepareAuditLoggingKafkaMessage(request, topicName);

    try {
      kafkaPrimaryTemplate.send(kafkaMessage);  // EUS2 region
    } catch (Exception ex) {
      log.error("Primary Kafka failed, trying secondary", ex);
      kafkaSecondaryTemplate.send(kafkaMessage);  // SCUS fallback
    }
  }

  public static Message<LogEvent> prepareAuditLoggingKafkaMessage(
      LoggingApiRequest request, String topicName) {

    // 1. Convert to Avro
    LogEvent event = AvroUtils.getLogEvent(request);

    // 2. Create message key for partitioning
    AuditKafkaPayloadKey key = AuditKafkaPayloadKey.getAuditKafkaPayloadKey(
        request.getEndpointName(),
        request.getServiceName()
    );

    // 3. Build message with headers
    MessageBuilder<LogEvent> builder = MessageBuilder.withPayload(event);

    // 4. Set Kafka headers
    builder.setHeader(KafkaHeaders.TOPIC, topicName);
    builder.setHeader(KafkaHeaders.KEY, key.toString());

    // 5. Pass through important headers for routing
    Set<String> allowedHeaders = Set.of(
        "wm_consumer.id",
        "wm_qos.correlation_id",
        "wm_svc.name",
        "wm_svc.version",
        "wm_svc.env",
        "wm-site-id"  // â¬…ï¸ CRITICAL for geo-routing
    );

    request.getHeaders().forEach((key, value) -> {
      if (allowedHeaders.contains(key.toLowerCase())) {
        builder.setHeader(key, value);
      }
    });

    return builder.build();
  }
}
```

**Why Avro?**
- Schema enforcement - prevents bad data
- Compact binary format - ~70% smaller than JSON
- Schema evolution - can add fields without breaking consumers
- Schema Registry integration - centralized schema management

**Why `wm-site-id` header?**
- Enables geo-based routing in the sink connector
- US, CA, MX records go to different GCS buckets
- Compliance with data residency requirements

#### PRs - audit-api-logs-srv

| PR# | State | Title | Key Changes |
|-----|-------|-------|-------------|
| **#4** | MERGED | initial commit for kafka service | **Initial Kafka producer** |
| #15 | MERGED | Deployment dev | Development deployment |
| #21 | MERGED | Service-template changes for V3 api-spec | API spec v3 |
| #22 | MERGED | Stage to main PR | Stage promotion |
| #32 | MERGED | Avro testing stage | Avro schema testing |
| #34 | MERGED | review changes | API review changes |
| #37 | MERGED | Stage to main pr | Stage to main promotion |
| #42 | MERGED | Release/snky fixes | Snyk security fixes |
| #43 | MERGED | final changes main | Final production changes |
| **#44** | MERGED | [CRQ:CHG3024893] kitt and sr changes for prod | **Production CRQ** |
| #47 | MERGED | [CRQ:CHG3024893] ccm changes | CCM production config |
| **#49-51** | MERGED | Increasing request entity size to 2 MB | **Payload size increase** |
| **#57** | MERGED | int changes | International (CA/MX) support |
| **#65** | MERGED | publish in both region changes | **Multi-region publishing** |
| **#67** | MERGED | Log publishing - Allowing selected headers | **Header forwarding for routing** |
| #69 | MERGED | Update KafkaProducerService.java | Header fixes |
| **#77** | MERGED | Contract test changes | R2C contract testing |
| #79-80 | MERGED | Container test, regression hook | Testing infrastructure |
| #81 | MERGED | Allow action validate signature | Security enhancement |
| #82 | MERGED | Update gtp bom | GTP BOM update |

---

### 3.3 audit-api-logs-gcs-sink

**Repository:** `gecgithub01.walmart.com/dsi-dataventures-luminate/audit-api-logs-gcs-sink`

#### Purpose
Kafka Connect Sink Connector that consumes from Kafka topic and writes to GCS.

#### Key Files

```
src/main/java/com/walmart/audit/log/sink/converter/
â”œâ”€â”€ BaseAuditLogSinkFilter.java        # Abstract SMT filter
â”œâ”€â”€ AuditLogSinkUSFilter.java          # US records filter (catch-all)
â”œâ”€â”€ AuditLogSinkCAFilter.java          # Canada records filter
â”œâ”€â”€ AuditLogSinkMXFilter.java          # Mexico records filter
â””â”€â”€ AuditApiLogsGcsSinkPropertiesUtil.java  # Config loader

Configuration:
â”œâ”€â”€ kc_config.yaml                     # Kafka Connect worker config
â”œâ”€â”€ ccm/PROD-1.0-ccm.yml              # Production CCM
â”œâ”€â”€ ccm/NON-PROD-1.0-ccm.yml          # Non-prod CCM
â”œâ”€â”€ kitt.yml                           # K8s deployment
â””â”€â”€ sr.yaml                            # Service Registry
```

#### Code Deep Dive

**1. BaseAuditLogSinkFilter.java - SMT Base Class**

```java
@Slf4j
public abstract class BaseAuditLogSinkFilter<R extends ConnectRecord<R>>
    implements Transformation<R> {

  protected static final String HEADER_NAME = "wm-site-id";

  // Each subclass provides its target site ID
  protected abstract String getHeaderValue();

  @Override
  public R apply(R record) {
    if (verifyHeader(record)) {
      return record;    // Pass through
    }
    return null;        // Filter out (Kafka Connect convention)
  }

  public boolean verifyHeader(R record) {
    try {
      return StreamSupport.stream(record.headers().spliterator(), true)
          .anyMatch(header ->
              HEADER_NAME.equals(header.key()) &&
              StringUtils.equals(getHeaderValue(), String.valueOf(header.value()))
          );
    } catch (Exception e) {
      log.error("Filter error for partition {}: {}",
          record.kafkaPartition(), e.getMessage());
      return false;
    }
  }
}
```

**Why Single Message Transform (SMT)?**
- Processes each record inline
- No external service dependencies
- Low latency filtering
- Built into Kafka Connect framework

**2. AuditLogSinkUSFilter.java - US Filter (Special!)**

```java
public class AuditLogSinkUSFilter<R extends ConnectRecord<R>>
    extends BaseAuditLogSinkFilter<R> {

  @Override
  protected String getHeaderValue() {
    return AuditApiLogsGcsSinkPropertiesUtil.getSiteIdForCountryCode("US");
  }

  @Override
  public boolean verifyHeader(R r) {
    // US filter is PERMISSIVE:
    // Accept if wm-site-id = "US" OR if header is missing
    return StreamSupport.stream(r.headers().spliterator(), true)
            .anyMatch(header -> HEADER_NAME.equals(header.key())
                && StringUtils.equals(getHeaderValue(), String.valueOf(header.value())))
        ||
        StreamSupport.stream(r.headers().spliterator(), true)
            .noneMatch(header -> HEADER_NAME.equals(header.key()));
  }
}
```

**Why US filter is permissive?**
- Legacy records might not have `wm-site-id` header
- US is default market
- Ensures no data loss during migration

**3. kc_config.yaml - Kafka Connect Configuration**

```yaml
worker:
  # Serialization
  key.converter: org.apache.kafka.connect.storage.StringConverter
  value.converter: io.confluent.connect.avro.AvroConverter
  value.converter.schemas.enable: true
  group.id: com.walmart-audit-log-gcs-sink-0.0.1

  # Security (SSL/TLS)
  security.protocol: SSL
  ssl.truststore.type: JKS
  ssl.truststore.location: secret.ref://kafka_ssl_truststore.jks
  ssl.keystore.location: secret.ref://kafka_ssl_keystore.jks

  # Consumer tuning (Critical for stability!)
  max.poll.records: 50
  max.poll.interval.ms: 300000    # 5 minutes
  heartbeat.interval.ms: 5000
  session.timeout.ms: 15000
  request.timeout.ms: 60000

connectors:
  # US Connector
  - name: audit-log-gcs-sink-connector
    config:
      connector.class: io.lenses.streamreactor.connect.gcp.storage.sink.GCPStorageSinkConnector
      tasks.max: 1
      errors.tolerance: all
      connect.gcpstorage.error.policy: RETRY
      connect.gcpstorage.max.retries: 5
      connect.gcpstorage.retry.interval: 5000

      # Transformations chain
      transforms: InsertRollingRecordTimestamp, FilterUS
      transforms.FilterUS.type: com.walmart.audit.log.sink.converter.AuditLogSinkUSFilter
      transforms.InsertRollingRecordTimestamp.type: io.lenses.connect.smt.header.InsertRollingRecordTimestampHeaders
      transforms.InsertRollingRecordTimestamp.date.format: "yyyy-MM-dd"

  # CA Connector
  - name: audit-log-gcs-sink-connector-ca
    config:
      connector.class: io.lenses.streamreactor.connect.gcp.storage.sink.GCPStorageSinkConnector
      tasks.max: 1
      transforms: InsertRollingRecordTimestamp, FilterCA
      transforms.FilterCA.type: com.walmart.audit.log.sink.converter.AuditLogSinkCAFilter

  # MX Connector
  - name: audit-log-gcs-sink-connector-mx
    config:
      connector.class: io.lenses.streamreactor.connect.gcp.storage.sink.GCPStorageSinkConnector
      tasks.max: 1
      transforms: InsertRollingRecordTimestamp, FilterMX
      transforms.FilterMX.type: com.walmart.audit.log.sink.converter.AuditLogSinkMXFilter
```

**Why 3 separate connectors?**
- Each writes to different GCS bucket
- Parallel processing for performance
- Isolated failures - one connector failing doesn't affect others
- Different retry/error policies per region if needed

**Why Lenses GCS Connector?**
- Mature, production-tested
- Native Parquet support
- Built-in partitioning
- Error handling and retries

#### PRs - audit-api-logs-gcs-sink (Complete History)

| PR# | State | Title | Key Changes | Interview Point |
|-----|-------|-------|-------------|-----------------|
| **#1** | MERGED | Updating readme, kafka topic and gcs bucket name | **Initial setup** | Architecture decisions |
| **#3** | MERGED | Deployment changes main | K8s deployment | Infrastructure |
| #4-5 | MERGED | Repo name change | Repository rename | - |
| **#9** | MERGED | Prod config, renaming class, updating field | Production config | Field mapping |
| #10-11 | MERGED | Bucket name updates | GCS bucket config | - |
| **#12** | MERGED | [CRQ:CHG3041101] GCS Sink changes for US records | **US production deployment** | CRQ process |
| #13-26 | MERGED | Various config updates | CCM, bucket, topic config | - |
| **#27** | MERGED | Updating flush count, poll interval, offset assignor | **Consumer tuning** | Performance |
| **#28** | MERGED | CHG3180819 - Consumer config changes | **Production tuning CRQ** | Kafka expertise |
| **#29** | MERGED | Adding autoscaling profile | **KEDA autoscaling** | K8s scaling |
| **#30** | MERGED | adding prod-eus2 env | **Multi-region: EUS2** | DR setup |
| #32-34 | MERGED | Helm, replica changes | K8s tuning | - |
| **#35** | MERGED | Adding try catch block in filter | **Exception handling fix** | Debugging |
| #37-41 | MERGED | Logger, heartbeat, CPU updates | Monitoring, tuning | - |
| **#42** | MERGED | Removing scaling on lag. Updating heartbeat | **Autoscaling fix** | Debugging story |
| #43-47 | MERGED | Session timeout, poll timeout | Consumer stability | - |
| #49-53 | MERGED | Consumer config tuning | Stability improvements | - |
| **#55-61** | MERGED | Multiple hotfixes | **Production debugging** | Debugging story |
| **#57** | MERGED | Overriding KAFKA_HEAP_OPTS | **Heap tuning** | JVM tuning |
| **#59** | MERGED | Adding the filter and custom header converter | **Header parsing fix** | Debugging |
| **#61** | MERGED | Increasing heap max | **Memory fix** | Final fix |
| #63-68 | MERGED | INTL, CCM, SR changes | International support | - |
| **#70** | MERGED | [CHG3235845] Upgrade connector, retry policy | **Retry policy** | Resilience |
| #72-77 | MERGED | CCM updates | Configuration | - |
| #81 | MERGED | Updating connector version | Version upgrade | - |
| #83 | MERGED | CCM Changes for country code to site id | **Site ID routing** | Multi-region |
| **#85** | MERGED | GCS Sink Changes to support different site ids | **Site-based routing** | Multi-region |
| #87 | MERGED | Vulnerability update and startup probe | Security, health | - |

---

## 4. ALL PRs - COMPLETE HISTORY

### Summary Statistics

| Repository | Total PRs | Merged | Open | Closed |
|------------|-----------|--------|------|--------|
| dv-api-common-libraries | 16 | 10 | 3 | 3 |
| audit-api-logs-srv | 83 | 48 | 20 | 15 |
| audit-api-logs-gcs-sink | 87 | 70 | 0 | 17 |
| **TOTAL** | **186** | **128** | **23** | **35** |

### Timeline of Major Milestones

```
2024-12 â”ƒ audit-api-logs-srv: Initial Kafka service (#4)
        â”ƒ
2025-01 â”ƒ audit-api-logs-gcs-sink: Initial setup (#1)
        â”ƒ
2025-02 â”ƒ audit-api-logs-srv: Stage deployment (#15, #22)
        â”ƒ
2025-03 â”ƒ dv-api-common-libraries: Audit log service (#1)
        â”ƒ audit-api-logs-srv: Production deployment (#44) [CRQ]
        â”ƒ audit-api-logs-gcs-sink: Production (#12) [CRQ:CHG3041101]
        â”ƒ
2025-04 â”ƒ dv-api-common-libraries: Regex endpoints (#11)
        â”ƒ audit-api-logs-srv: Payload size increase (#49-51)
        â”ƒ
2025-05 â”ƒ ğŸ”¥ PRODUCTION DEBUGGING ğŸ”¥
        â”ƒ audit-api-logs-gcs-sink: Hotfixes (#35-61)
        â”ƒ - Exception handling (#35)
        â”ƒ - Consumer configs (#38-47)
        â”ƒ - Autoscaling removal (#42)
        â”ƒ - Heap tuning (#57, #61)
        â”ƒ audit-api-logs-srv: Multi-region (#65), Headers (#67)
        â”ƒ
2025-06 â”ƒ audit-api-logs-gcs-sink: Retry policy (#70) [CRQ]
        â”ƒ
2025-09 â”ƒ dv-api-common-libraries: Java 17 (#15)
        â”ƒ
2025-11 â”ƒ audit-api-logs-gcs-sink: Site-based routing (#85)
        â”ƒ audit-api-logs-srv: Contract testing (#77)
        â”ƒ
2026-01 â”ƒ audit-api-logs-gcs-sink: Startup probe (#87)
        â”ƒ audit-api-logs-srv: GTP BOM update (#82)
```

---

## 5. MULTI-REGION ARCHITECTURE (Bullet 10)

### Why Multi-Region?

1. **Disaster Recovery**: If EUS2 (East US 2) goes down, SCUS (South Central US) continues
2. **Latency**: Users closer to SCUS get faster responses
3. **Compliance**: Some data must stay in specific regions
4. **Load Distribution**: Split traffic across regions

### Architecture

```
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚         GLOBAL LOAD BALANCER        â”‚
                              â”‚          (Azure Front Door)         â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                         â”‚                         â”‚
                    â–¼                         â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   EUS2 Region    â”‚     â”‚   SCUS Region    â”‚     â”‚  INTL (Future)   â”‚
         â”‚  (East US 2)     â”‚     â”‚ (South Central)  â”‚     â”‚                  â”‚
         â”‚                  â”‚     â”‚                  â”‚     â”‚                  â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚                  â”‚
         â”‚ â”‚audit-api-logsâ”‚ â”‚     â”‚ â”‚audit-api-logsâ”‚ â”‚     â”‚                  â”‚
         â”‚ â”‚   -srv       â”‚ â”‚     â”‚ â”‚   -srv       â”‚ â”‚     â”‚                  â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚                  â”‚
         â”‚        â”‚         â”‚     â”‚        â”‚         â”‚     â”‚                  â”‚
         â”‚        â–¼         â”‚     â”‚        â–¼         â”‚     â”‚                  â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚                  â”‚
         â”‚ â”‚Kafka Cluster â”‚ â”‚â—„â”€â”€â”€â–ºâ”‚ â”‚Kafka Cluster â”‚ â”‚     â”‚                  â”‚
         â”‚ â”‚(EUS2)        â”‚ â”‚     â”‚ â”‚(SCUS)        â”‚ â”‚     â”‚                  â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚                  â”‚
         â”‚        â”‚         â”‚     â”‚        â”‚         â”‚     â”‚                  â”‚
         â”‚        â–¼         â”‚     â”‚        â–¼         â”‚     â”‚                  â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚                  â”‚
         â”‚ â”‚GCS Sink      â”‚ â”‚     â”‚ â”‚GCS Sink      â”‚ â”‚     â”‚                  â”‚
         â”‚ â”‚(Connect)     â”‚ â”‚     â”‚ â”‚(Connect)     â”‚ â”‚     â”‚                  â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   GCS Buckets    â”‚
                    â”‚ (Shared across   â”‚
                    â”‚  regions)        â”‚
                    â”‚                  â”‚
                    â”‚ - us-prod        â”‚
                    â”‚ - ca-prod        â”‚
                    â”‚ - mx-prod        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key PRs for Multi-Region

| PR# | Repo | Title | Description |
|-----|------|-------|-------------|
| #30 | gcs-sink | adding prod-eus2 env | Added EUS2 region deployment |
| #65 | srv | publish in both region changes | Dual-region Kafka publishing |
| #67 | srv | Allowing selected headers | Header forwarding for routing |
| #85 | gcs-sink | GCS Sink Changes to support different site ids | Site-based filtering |

### CCM Configuration for Multi-Region

**PROD-1.0-ccm.yml:**
```yaml
configOverrides:
  workerConfig:
    - description: "prod-scus env"
      pathElements:
        envProfile: "prod"
        envName: "prod-scus"
      value:
        properties:
          bootstrap.servers: "kafka-532395416-1-1708669300.scus.kafka-v2-luminate-core-prod..."

    - description: "prod-eus2 env"
      pathElements:
        envProfile: "prod"
        envName: "prod-eus2"
      value:
        properties:
          bootstrap.servers: "kafka-976900980-1-1708670447.eus.kafka-v2-luminate-core-prod..."
```

---

## 6. TECHNOLOGY DECISIONS & ALTERNATIVES

### Why Kafka Connect over Custom Consumer?

| Aspect | Custom Consumer | Kafka Connect | Our Choice |
|--------|-----------------|---------------|------------|
| Development Time | 2-3 weeks | 2-3 days | **Connect** |
| Offset Management | Manual | Built-in | **Connect** |
| Scaling | Custom code | KEDA/Autoscaling | **Connect** |
| Error Handling | Manual DLQ | Built-in DLQ | **Connect** |
| Monitoring | Custom metrics | Standard metrics | **Connect** |
| Restart Recovery | Code needed | Automatic | **Connect** |

### Why Avro over JSON?

| Aspect | JSON | Avro | Our Choice |
|--------|------|------|------------|
| Size | Large (~100%) | Small (~30%) | **Avro** |
| Schema | None | Enforced | **Avro** |
| Evolution | Breaking | Compatible | **Avro** |
| Parsing Speed | Slow (text) | Fast (binary) | **Avro** |
| BigQuery Compat | Good | Excellent | **Avro** |

### Why Parquet for Storage?

| Aspect | JSON | Parquet | Our Choice |
|--------|------|---------|------------|
| Compression | ~30% | ~90% | **Parquet** |
| Query Speed | Slow (scan all) | Fast (columnar) | **Parquet** |
| BigQuery | Text parsing | Native | **Parquet** |
| Cost (GCS) | High | Low | **Parquet** |

### Why ContentCachingWrapper?

**Problem:** HTTP request/response bodies are streams - can only be read once.

**Alternatives Considered:**
1. **Read stream in filter, pass bytes to controller** - Requires modifying every controller
2. **Use AOP** - Can't access raw request easily
3. **ContentCachingWrapper** - Spring's built-in solution

**Our Choice:** ContentCachingWrapper because:
- No controller changes needed
- Well-tested Spring component
- Handles edge cases (charset, multipart, etc.)

### Why @Async with ThreadPoolTaskExecutor?

**Alternatives Considered:**
1. **Synchronous call** - Blocks API response
2. **CompletableFuture** - More complex error handling
3. **Reactive (WebFlux)** - Overkill for fire-and-forget
4. **Message Queue (Kafka)** - Adds infrastructure complexity

**Our Choice:** @Async because:
- Simple annotation-based
- Built-in thread pool management
- Queue for backpressure
- Silent failure is acceptable for audit logs

---

## 7. INTERVIEW Q&A BANK

### Technical Questions

**Q1: Why did you use a filter instead of AOP for audit logging?**
> "Filters give us access to the raw HTTP request/response at the servlet level. With AOP, we'd only have access to method parameters and return values, not the actual HTTP body stream. Also, filters let us capture timing information before and after the entire filter chain, including security filters."

**Q2: What happens if the audit service is down?**
> "The API continues to work normally. Audit logging is async and fire-and-forget. We catch all exceptions in AuditLogService and log them, but don't propagate. The user experience is unaffected. We monitor audit service health separately."

**Q3: How do you handle large request/response bodies?**
> "ContentCachingWrapper buffers the body in memory. For very large payloads, we could hit memory issues. In practice, our APIs have payload limits (2MB) set at the gateway. We also have an option to disable response body logging for endpoints with large responses."

**Q4: Why three separate Kafka Connect connectors?**
> "Data residency compliance. US, CA, and MX data must be stored in separate GCS buckets. Running three connectors with different SMT filters ensures records are routed to the correct bucket. If one connector fails, others continue working independently."

**Q5: How does the US filter handle records without wm-site-id header?**
> "US filter is permissive - it accepts records with US site ID OR records without any site ID. This handles legacy data from before we added the header. CA and MX filters are strict - they only accept exact matches."

**Q6: How did you tune Kafka Connect consumer configuration?**
> "Through iterative production debugging (PRs #35-61). Key learnings:
> - `max.poll.interval.ms: 300000` - 5 minutes for processing batch
> - `session.timeout.ms: 15000` - Fast failure detection
> - `heartbeat.interval.ms: 5000` - Frequent heartbeats
> - Heap: 1GB initial, increased to 2GB after OOM issues"

### Behavioral Questions

**Q: Tell me about a challenging debugging experience.**
> "In May 2025, our GCS sink started failing silently in production. Through PRs #35-61 over two weeks, I systematically debugged:
>
> 1. First found unhandled exceptions in SMT filter (#35)
> 2. Then consumer poll timeout issues (#38, #47)
> 3. Autoscaling was causing instability - removed it (#42)
> 4. Finally traced to heap memory exhaustion (#57, #61)
>
> The fix involved proper try-catch in filter, tuning consumer configs, and increasing JVM heap. I learned that Kafka Connect's default error tolerance can hide issues."

**Q: How did you ensure zero downtime during deployment?**
> "We used Flagger for canary deployments. The GCS sink is deployed on WCNP (Walmart Cloud Native Platform) with Istio service mesh. Flagger gradually shifts traffic to new version while monitoring custom metrics. If error rate exceeds threshold, it automatically rolls back."

---

## 8. CODE WALKTHROUGH

### Complete Request Flow with Code

**Step 1: Client Request Arrives**
```http
POST /iac/v1/inventory HTTP/1.1
Host: api.walmart.com
WM_CONSUMER.ID: supplier-123
WM_QOS.CORRELATION_ID: abc-xyz
Content-Type: application/json

{"store_id": 1234, "gtin": "00012345678905"}
```

**Step 2: LoggingFilter Intercepts (Before)**
```java
// LoggingFilter.java
@Override
protected void doFilterInternal(HttpServletRequest request, ...) {

  // Check feature flag
  if (featureFlagCCMConfig.isAuditLogEnabled()) {

    // Wrap to capture body
    ContentCachingRequestWrapper requestWrapper =
        new ContentCachingRequestWrapper(request);
    ContentCachingResponseWrapper responseWrapper =
        new ContentCachingResponseWrapper(response);

    long requestTimestamp = Instant.now().getEpochSecond();

    // Continue to next filter/controller
    filterChain.doFilter(requestWrapper, responseWrapper);
```

**Step 3: Controller Processes Request**
```java
// IACController.java
@PostMapping("/iac/v1/inventory")
public ResponseEntity<InventoryResponse> captureInventory(
    @RequestBody InventoryRequest request) {

  InventoryResponse response = iacService.processInventory(request);
  return ResponseEntity.ok(response);
}
```

**Step 4: LoggingFilter Intercepts (After)**
```java
// LoggingFilter.java (continued)
    long responseTimestamp = Instant.now().getEpochSecond();

    // Extract cached bodies
    String requestBody = convertByteArrayToString(
        requestWrapper.getContentAsByteArray());
    String responseBody = convertByteArrayToString(
        responseWrapper.getContentAsByteArray());

    // Build audit payload
    AuditLogPayload payload = AuditLogPayload.builder()
        .requestId(UUID.randomUUID().toString())
        .serviceName("cp-nrti-apis")
        .endpointName("/iac/v1/inventory")
        .method("POST")
        .requestBody(requestBody)
        .responseBody(responseBody)
        .responseCode(200)
        .requestTimestamp(requestTimestamp)
        .responseTimestamp(responseTimestamp)
        .headers(getServiceHeaders(request))
        .build();

    // Send async
    auditLogService.sendAuditLogRequest(payload, auditLoggingConfig);

    // CRITICAL: Copy body back to response
    responseWrapper.copyBodyToResponse();
  }
}
```

**Step 5: AuditLogService Sends Async**
```java
// AuditLogService.java
@Async  // Runs in thread pool
public void sendAuditLogRequest(AuditLogPayload payload, ...) {
  try {
    URI uri = URI.create("https://audit-api-logs-srv.../v1/logRequest");

    HttpHeaders headers = new HttpHeaders();
    headers.set("WM_CONSUMER.ID", config.getWmConsumerId());
    headers.set("WM_SEC.AUTH_SIGNATURE", generateSignature());

    httpService.sendHttpRequest(uri, HttpMethod.POST,
        new HttpEntity<>(payload, headers), Void.class);

  } catch (Exception e) {
    log.error("Audit log failed", e);  // Don't throw
  }
}
```

**Step 6: audit-api-logs-srv Publishes to Kafka**
```java
// KafkaProducerService.java
public void publishMessageToTopic(LoggingApiRequest request) {

  // Convert to Avro
  LogEvent event = AvroUtils.getLogEvent(request);

  // Create message
  Message<LogEvent> message = MessageBuilder
      .withPayload(event)
      .setHeader(KafkaHeaders.TOPIC, "api_logs_audit_prod")
      .setHeader(KafkaHeaders.KEY, "cp-nrti-apis|/iac/v1/inventory")
      .setHeader("wm-site-id", request.getHeaders().get("wm-site-id"))
      .build();

  kafkaTemplate.send(message);
}
```

**Step 7: GCS Sink Filters and Writes**
```java
// AuditLogSinkUSFilter.java
@Override
public R apply(R record) {
  // Check if US or no header
  boolean hasUSHeader = record.headers().stream()
      .anyMatch(h -> "wm-site-id".equals(h.key()) && "US".equals(h.value()));
  boolean hasNoHeader = record.headers().stream()
      .noneMatch(h -> "wm-site-id".equals(h.key()));

  if (hasUSHeader || hasNoHeader) {
    return record;  // Write to US bucket
  }
  return null;  // Filter out
}
```

**Step 8: Data in GCS Bucket**
```
gs://audit-api-logs-us-prod/
â””â”€â”€ cp-nrti-apis/
    â””â”€â”€ 2025-05-15/
        â””â”€â”€ iac/
            â””â”€â”€ part-00000-abc123.parquet
```

**Step 9: Query in BigQuery**
```sql
SELECT
  request_id,
  endpoint_name,
  response_code,
  (response_ts - request_ts) as latency_sec
FROM `project.audit_logs.api_logs_us`
WHERE DATE(created_ts) = '2025-05-15'
  AND service_name = 'cp-nrti-apis'
  AND endpoint_name LIKE '%/iac/%'
ORDER BY latency_sec DESC
LIMIT 100;
```

---

## 9. PRODUCTION ISSUES & DEBUGGING STORIES (REAL INCIDENTS)

> **ğŸ”¥ INTERVIEW GOLD:** These are REAL production issues from the PR history. Memorize these - they show debugging skills, production experience, and resilience.

---

### PRODUCTION ISSUE #1: KEDA Autoscaling Causing Consumer Rebalance Storms

**PRs:** #42, #34, #50 (audit-api-logs-gcs-sink)
**Severity:** Critical - Data processing stopped
**Duration:** ~4 hours

**The Incident:**
```
Timeline:
- Day 1, 2:00 PM: GCS sink deployed with KEDA autoscaling based on consumer lag
- Day 1, 2:15 PM: Traffic spike â†’ Lag increases â†’ KEDA scales from 1 to 3 workers
- Day 1, 2:16 PM: 3 workers â†’ Consumer group rebalance triggered
- Day 1, 2:17 PM: During rebalance, lag increases more â†’ KEDA scales to 5
- Day 1, 2:18 PM: 5 workers â†’ Another rebalance â†’ INFINITE LOOP
- Day 1, 2:20 PM: Consumers stuck in constant rebalancing, NO messages processed
```

**Root Cause:**
KEDA scales on consumer lag, but scaling triggers Kafka consumer group rebalances. During rebalance, no messages are consumed, so lag increases, triggering more scaling. Feedback loop!

**The Fix (PR #42):**
```yaml
# REMOVED from kitt.yml:
keda:
  triggers:
    - type: kafka
      metadata:
        consumerGroup: audit-log-consumer-group
        lagThreshold: "100"

# KEPT only standard HPA on CPU:
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 70
```

**Also increased heartbeat timeout (PR #42):**
```yaml
# Before: 10 seconds (too aggressive)
heartbeat.interval.ms: 10000
session.timeout.ms: 30000

# After: 10 minutes (stable)
heartbeat.interval.ms: 600000
session.timeout.ms: 600000
```

**Interview Answer:**
> "We had KEDA autoscaling based on consumer lag. In production, a traffic spike triggered scaling, which triggered consumer rebalances, which increased lag, which triggered more scaling - a feedback loop. The connector got stuck in constant rebalancing and processed zero messages. We removed KEDA for Kafka consumers and switched to CPU-based HPA. Also increased heartbeat timeout from 10 seconds to 10 minutes to reduce rebalance frequency."

---

### PRODUCTION ISSUE #2: OOM Kills Due to Kafka Connect Heap Settings

**PRs:** #57, #61 (audit-api-logs-gcs-sink)
**Severity:** High - Connector crashes repeatedly
**Duration:** ~2 days to fully resolve

**The Incident:**
```
Symptoms:
- Kafka Connect workers getting killed by OOM killer
- Pods restarting every 30-60 minutes
- Lag building up during restarts
```

**Root Cause:**
Kafka Connect was using default heap settings (~256MB). With large batch sizes (flush.size: 10M records) and Avro deserialization overhead, it ran out of memory.

**The Fix (PR #57, #61):**
```yaml
# Before: Default (~256MB)
env:
  - name: KAFKA_HEAP_OPTS
    value: "-Xms256m -Xmx512m"

# After: Explicit 7GB heap
env:
  - name: KAFKA_HEAP_OPTS
    value: "-Xms4g -Xmx7g"
```

**Interview Answer:**
> "Kafka Connect workers were getting OOM killed every 30-60 minutes. We were using default heap settings of 256MB, but our GCS sink buffers large batches - flush.size was 10 million records - in memory before writing. Combined with Avro deserialization overhead, we needed much more memory. We increased heap to 7GB and the OOM kills stopped."

---

### PRODUCTION ISSUE #3: SMT Filter Throwing Exceptions on Malformed Headers

**PRs:** #35 (audit-api-logs-gcs-sink)
**Severity:** Critical - Connector stopped processing ALL messages
**Duration:** ~6 hours before detection

**The Incident:**
```
Timeline:
- Some messages had malformed wm-site-id headers (null or empty)
- SMT filter threw NullPointerException
- errors.tolerance was "none" so connector failed
- Connector stopped processing ALL messages
- 6 hours of data loss before alerting triggered
```

**Root Cause:**
The `verifyHeader` method in SMT filter didn't handle null/malformed headers gracefully.

**Before:**
```java
public boolean verifyHeader(ConnectRecord<?> record) {
    Header header = record.headers().lastWithName("wm-site-id");
    return header.value().toString().equals("US");  // NPE if header is null!
}
```

**The Fix (PR #35):**
```java
public boolean verifyHeader(ConnectRecord<?> record) {
    try {
        Header header = record.headers().lastWithName("wm-site-id");
        if (header == null || header.value() == null) {
            return true;  // Permissive: pass messages without header to US bucket
        }
        return header.value().toString().equals("US");
    } catch (Exception e) {
        // Log but don't fail the connector
        return true;  // Default to US bucket
    }
}
```

**Interview Answer:**
> "Our SMT filter threw NullPointerException on malformed headers, causing the entire connector to fail. Because errors.tolerance was 'none', one bad message stopped processing of all messages. We lost about 6 hours of data before alerting triggered. I added try-catch with graceful fallback - malformed headers now default to the US bucket. Also added Prometheus metrics on filter exceptions."

---

### PRODUCTION ISSUE #4: Request Payload Size Limit (413 Entity Too Large)

**PRs:** #49, #50, #51 (audit-api-logs-srv)
**Severity:** Medium - Silent data loss for large payloads
**Duration:** Unknown - discovered during routine analysis

**The Incident:**
```
Symptoms:
- Audit logs missing for some API calls
- Pattern: all missing calls had large response bodies
- HTTP 413 errors in audit-api-logs-srv logs
```

**Root Cause:**
Default API proxy limit in Service Registry was 1MB. Some inventory API responses (with hundreds of items) exceeded this limit.

**The Fix (PR #51):**
```yaml
# sr.yaml - Service Registry config
policies:
  - name: DEFAULT_PROPERTIES_POLICY
    properties:
      APIPROXY_QOS_REQUEST_PAYLOAD_SIZE: 2097152  # 2MB
```

**Interview Answer:**
> "We were losing audit data for large API responses and didn't know it. The API proxy had a default 1MB limit, and some inventory responses - when suppliers query hundreds of items - exceeded that. The common library was silently failing to send these. We increased the limit to 2MB in Service Registry. Also added payload size monitoring to catch similar issues."

---

### PRODUCTION ISSUE #5: Error Tolerance Configuration Oscillation

**PRs:** #51, #52, #57 (audit-api-logs-gcs-sink)
**Severity:** Medium - Data loss vs Connector failures
**Duration:** 2 weeks of back-and-forth

**The Problem:**
```
The dilemma:
- errors.tolerance: "all"  â†’ Bad messages silently skipped = DATA LOSS
- errors.tolerance: "none" â†’ One bad message stops connector = AVAILABILITY LOSS
```

**The Journey:**
1. **Started with "none"** - Connector stopped on first bad message
2. **Changed to "all" (PR #57)** - Connector worked but we lost data silently
3. **Changed back to "none" (PR #52)** - With better monitoring

**The Final Solution:**
```yaml
# Final configuration:
errors.tolerance: "none"                    # Fail fast
errors.log.enable: true                     # Log the error
errors.log.include.messages: true           # Include message content
errors.deadletterqueue.topic.name: audit-dlq  # Send to DLQ
```

**Interview Answer:**
> "We oscillated between error tolerance settings for two weeks. 'All' meant silent data loss - we found out a week later we'd lost thousands of records. 'None' meant connector stopped on any bad message. The answer was 'none' with a dead letter queue - fail fast, alert immediately, but don't lose the message. Bad messages go to DLQ for manual review and replay."

---

### PRODUCTION ISSUE #6: Dual-Region Kafka Failover Not Working

**PRs:** #65, #69 (audit-api-logs-srv)
**Severity:** High - No failover during regional outage
**Duration:** 2-hour regional outage exposed the issue

**The Incident:**
```
Timeline:
- Primary Kafka region (EUS2) had a 2-hour outage
- Expected: Failover to secondary (SCUS)
- Actual: Messages dropped, secondary never tried
```

**Root Cause:**
Failover logic used `exceptionally()` but didn't properly chain to secondary region.

**Before:**
```java
kafkaPrimaryTemplate.send(message)
    .exceptionally(ex -> {
        log.error("Failed: {}", ex.getMessage());
        return null;  // Message lost! Secondary never tried!
    });
```

**After (PR #65):**
```java
CompletableFuture<SendResult<String, Message<IacKafkaPayload>>> future =
    kafkaPrimaryTemplate.send(message);

future
    .thenAccept(result -> log.info("Primary success"))
    .exceptionally(ex -> {
        log.warn("Primary failed, trying secondary: {}", ex.getMessage());
        handleFailure(topicName, message, messageId).join();  // Retry secondary!
        return null;
    })
    .join();

private CompletableFuture<Void> handleFailure(String topic, Message msg, String id) {
    return kafkaSecondaryTemplate.send(msg)
        .thenAccept(result -> log.info("Secondary success: {}", result))
        .exceptionally(ex -> {
            log.error("Both regions failed!");
            throw new NrtiUnavailableException();  // Now throw to caller
        });
}
```

**Interview Answer:**
> "During a Kafka outage in our primary region, we discovered failover wasn't working. The code returned null in exceptionally() instead of trying the secondary region. Messages were just dropped. I refactored to properly chain CompletableFuture - if primary fails, we synchronously try secondary. If both fail, we throw so the caller knows audit failed. Also added metrics to track failover frequency."

---

### Summary Table: Production Issues

| Issue | Severity | Root Cause | Fix | Learning |
|-------|----------|------------|-----|----------|
| KEDA Rebalance Storm | Critical | Scaling on lag triggers rebalances | Remove KEDA, use CPU-based HPA | Don't scale Kafka consumers on lag |
| OOM Kills | High | Default heap + large batches | 7GB heap | Know your memory requirements |
| SMT NPE | Critical | No null handling | Try-catch + default fallback | Defensive coding in connectors |
| 413 Payload | Medium | Default 1MB limit | Increase to 2MB | Know all size limits |
| Error Tolerance | Medium | Silent loss vs failures | DLQ + fail fast | Use dead letter queues |
| Failover Logic | High | Broken CompletableFuture chain | Proper chaining + join() | Test failover regularly |

---

## LEGACY DEBUGGING STORIES (Original)

### Story 1: The Silent Failure (PRs #35-61)

**Situation:**
In May 2025, we noticed GCS buckets weren't receiving new data, but no errors in logs.

**Task:**
Identify root cause and fix without losing data.

**Action:**
1. **PR #35**: Added try-catch in SMT filter - found `NullPointerException` on records with null headers
2. **PR #38**: Consumer poll was timing out - increased `max.poll.interval.ms`
3. **PR #42**: KEDA autoscaling was causing consumer group rebalances - disabled autoscaling
4. **PR #47**: Increased poll timeout further
5. **PR #57, #61**: Found heap exhaustion on large batches - increased JVM heap from 512MB to 7GB

**Result:**
- Zero data loss (Kafka retained messages during the entire debugging period)
- Added comprehensive monitoring and alerting
- Created runbook for future issues
- **Key Learning:** Kafka Connect's default error tolerance can hide issues - always add monitoring

### Story 2: Multi-Region Rollout (PRs #30, #65, #85)

**Situation:**
Need to expand audit logging from single region to dual-region for DR.

**Task:**
Add SCUS region while maintaining existing EUS2 traffic.

**Action:**
1. **PR #30**: Added EUS2 environment configuration in GCS sink
2. **PR #65**: Modified audit-api-logs-srv to publish to both Kafka clusters with proper failover
3. **PR #85**: Added site-based filtering in GCS sink for regional data segregation

**Result:**
- Active/Active deployment across two regions
- Zero downtime migration
- 99.99% availability achieved
- **Key Learning:** Test failover regularly - our failover was broken and we didn't know

---

## 10. FLOW DIAGRAMS

### Sequence Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”
â”‚Clientâ”‚      â”‚cp-nrti-  â”‚      â”‚Logging  â”‚      â”‚audit-api-  â”‚      â”‚Kafka    â”‚      â”‚GCS  â”‚
â”‚      â”‚      â”‚apis      â”‚      â”‚Filter   â”‚      â”‚logs-srv    â”‚      â”‚Connect  â”‚      â”‚     â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”¬â”€â”€â”˜
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚ POST /iac     â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚ doFilter(req)   â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚ wrap request    â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚        â”‚        â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚<â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚ filterChain.    â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚ doFilter()      â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚ process request â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚        â”‚        â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚ return response â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚ extract bodies  â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚        â”‚        â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚<â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚ @Async HTTP POSTâ”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚ copy response   â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚ to client       â”‚                  â”‚              â”‚
   â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚ 200 OK        â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚ Kafka send       â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚ consume      â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚        â”‚     â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚<â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚ SMT filter   â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚        â”‚     â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚<â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚ write Parquetâ”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚               â”‚                 â”‚                 â”‚                  â”‚              â”‚
```

---

## Commands to View PRs

```bash
# Set Walmart GitHub host
export GH_HOST=gecgithub01.walmart.com

# View PR details
gh pr view <PR#> --repo dsi-dataventures-luminate/<repo-name>

# View PR diff
gh pr diff <PR#> --repo dsi-dataventures-luminate/<repo-name>

# Examples
gh pr view 1 --repo dsi-dataventures-luminate/audit-api-logs-gcs-sink
gh pr view 1 --repo dsi-dataventures-luminate/dv-api-common-libraries
gh pr view 65 --repo dsi-dataventures-luminate/audit-api-logs-srv
```

---

*Document generated for interview preparation - Covers Bullets 1, 2, and 10 of resume*
*Last updated: February 2026*
