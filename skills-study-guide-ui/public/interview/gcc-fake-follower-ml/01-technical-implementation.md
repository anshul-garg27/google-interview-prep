# Technical Implementation - Fake Follower Detection ML System

> **Complete code walkthrough of the 955+ line Python ML system.**
> Covers: 5-feature ensemble model, 10 Indic script transliteration, 13 Unicode normalizations, AWS serverless pipeline.

---

## Table of Contents

1. [System Overview](#1-system-overview)
2. [Core ML Algorithm - fake.py (385 lines)](#2-core-ml-algorithm---fakepy)
3. [Unicode Symbol Normalization (13 Variants)](#3-unicode-symbol-normalization)
4. [Language Detection & Transliteration](#4-language-detection--transliteration)
5. [5-Feature Ensemble Model](#5-five-feature-ensemble-model)
6. [Ensemble Scoring Functions](#6-ensemble-scoring-functions)
7. [AWS Data Pipeline - push.py (154 lines)](#7-aws-data-pipeline---pushpy)
8. [Results Retrieval - pull.py (131 lines)](#8-results-retrieval---pullpy)
9. [Docker Container - Dockerfile (23 lines)](#9-docker-container---dockerfile)
10. [Hindi Transliteration - createDict.py (91 lines)](#10-hindi-transliteration---createdictpy)

---

## 1. System Overview

```
Source Files:
‚îú‚îÄ‚îÄ fake.py          # Core ML detection algorithm (385 lines, 19KB)
‚îú‚îÄ‚îÄ push.py          # Data pipeline: ClickHouse ‚Üí S3 ‚Üí SQS (154 lines)
‚îú‚îÄ‚îÄ pull.py          # Kinesis stream data retrieval (131 lines)
‚îú‚îÄ‚îÄ createDict.py    # Hindi vowel/consonant mapping generator (91 lines)
‚îú‚îÄ‚îÄ Dockerfile       # Lambda Docker image definition (23 lines)
‚îú‚îÄ‚îÄ requirement.txt  # Python dependencies (5 items)
‚îÇ
‚îú‚îÄ‚îÄ baby_names_.csv  # 35,183 Indian baby names database
‚îú‚îÄ‚îÄ svar.csv         # 24 Hindi vowel transliteration mappings
‚îú‚îÄ‚îÄ vyanjan.csv      # 42 Hindi consonant transliteration mappings
‚îÇ
‚îî‚îÄ‚îÄ indic-trans-master/   # HMM-based transliteration library
    ‚îî‚îÄ‚îÄ indictrans/
        ‚îú‚îÄ‚îÄ models/       # 10 pre-trained HMM models
        ‚îÇ   ‚îú‚îÄ‚îÄ hin-eng/  # Hindi ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ ben-eng/  # Bengali ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ guj-eng/  # Gujarati ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ kan-eng/  # Kannada ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ mal-eng/  # Malayalam ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ ori-eng/  # Odia ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ pan-eng/  # Punjabi ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ tam-eng/  # Tamil ‚Üí English
        ‚îÇ   ‚îú‚îÄ‚îÄ tel-eng/  # Telugu ‚Üí English
        ‚îÇ   ‚îî‚îÄ‚îÄ urd-eng/  # Urdu ‚Üí English
        ‚îú‚îÄ‚îÄ _decode/
        ‚îÇ   ‚îú‚îÄ‚îÄ viterbi.pyx     # Viterbi algorithm (Cython)
        ‚îÇ   ‚îî‚îÄ‚îÄ beamsearch.pyx  # Beamsearch decoder (Cython)
        ‚îî‚îÄ‚îÄ _utils/
            ‚îú‚îÄ‚îÄ wx.py             # WX encoding converter
            ‚îú‚îÄ‚îÄ one_hot_encoder.py # Feature encoding
            ‚îî‚îÄ‚îÄ script_normalizer.py
```

---

## 2. Core ML Algorithm - fake.py

### Lambda Handler Entry Point

The AWS Lambda function receives an SQS event containing follower data and runs the full ML pipeline.

```python
# fake.py - Lambda entry point
import json
import pandas as pd
import re
from unidecode import unidecode
from itertools import permutations
import time
from rapidfuzz import fuzz as fuzzz
import numpy as np
import boto3
import csv
from indictrans import Transliterator

session = boto3.Session(
    aws_access_key_id='AKIA...',
    aws_secret_access_key='...',
)

def handler(event, context):
    response = model(event)
    print(response)
    sqs = session.resource('sqs', region_name='eu-north-1')
    queue = sqs.get_queue_by_name(QueueName='output_queue')
    response = queue.send_message(MessageBody=json.dumps(response))
    return {
        "statusCode": 200,
        "body": {"message": response}
    }
```

**Key point:** Each Lambda invocation processes ONE follower record, runs the 5-feature ensemble, and sends results to an SQS output queue. The Kinesis stream is used for downstream aggregation.

### The model() Function - Complete ML Pipeline

This is the heart of the system. It orchestrates the entire detection pipeline for a single follower record.

```python
# fake.py - model() function (lines 316-385)
def model(event):
    follower_data = [event]
    response = {}
    total_time = 0
    for index, temp in enumerate(follower_data, start=1):
        start = time.time()

        # PHASE 1: Text Normalization
        symbolic_name = symbol_name_convert(temp['follower_full_name'])
        fake_real_based_on_lang = check_lang_other_than_indic(symbolic_name)
        transliterated_follower_name = detect_language(symbolic_name)
        decoded_name = uni_decode(transliterated_follower_name)

        # PHASE 2: Feature Extraction
        cleaned_handle = clean_handle(temp['follower_handle'])
        cleaned_name = clean_handle(decoded_name)
        chhitij_logic = process(temp['follower_handle'], cleaned_handle, cleaned_name)
        number_handle = count_numerical_digits(temp['follower_handle'])
        number_more_than_4_handle = fake_real_more_than_4_digit(number_handle)
        similarity_score = generate_similarity_score(cleaned_handle, cleaned_name)
        fake_real_based_on_fuzzy_score_90 = based_on_partial_ratio(similarity_score)
        numeric_handle = only_numeric(temp['follower_handle'])

        # PHASE 3: Ensemble Scoring
        process1_ = process1(fake_real_based_on_lang, number_more_than_4_handle, chhitij_logic)
        final_ = final(fake_real_based_on_lang, similarity_score,
                       number_more_than_4_handle, chhitij_logic)

        # PHASE 4: Indian Name Validation
        indian_name_score = check_indian_names(cleaned_name)
        score_80 = 1 if indian_name_score > 80 else 0

        # 16-field response
        response = {
            "symbolic_name": symbolic_name,
            "fake_real_based_on_lang": fake_real_based_on_lang,
            "transliterated_follower_name": transliterated_follower_name,
            "decoded_name": decoded_name,
            "cleaned_handle": cleaned_handle,
            "cleaned_name": cleaned_name,
            "chhitij_logic": chhitij_logic,
            "number_handle": number_handle,
            "number_more_than_4_handle": number_more_than_4_handle,
            "similarity_score": similarity_score,
            "fake_real_based_on_fuzzy_score_90": fake_real_based_on_fuzzy_score_90,
            "process1_": process1_,
            "final_": final_,
            "numeric_handle": numeric_handle,
            "indian_name_score": indian_name_score,
            "score_80": score_80
        }
    return response
```

---

## 3. Unicode Symbol Normalization

### 13 Unicode Variant Sets

Fake accounts often use "fancy" Unicode characters to evade text-based filters. This function normalizes 13 different Unicode alphabetic variants back to standard ASCII.

```python
# fake.py - symbol_name_convert() (lines 96-121)
def symbol_name_convert(name):
    original = [
        # 1. Circled Letters (negative):     üÖêüÖëüÖíüÖìüÖî...
        # 2. Circled Letters (negative sq):   üÖ∞üÖ±üÖ≤üÖ≥üÖ¥...
        # 3. Parenthesized/Squared:           üÑ∞üÑ±üÑ≤üÑ≥üÑ¥...
        # 4. Circled Latin:                   ‚í∂‚í∑‚í∏‚íπ‚í∫‚ìê‚ìë‚ìí‚ìì‚ìî...
        # 5. Mathematical Bold:               ùêÄùêÅùêÇùêÉùêÑùêöùêõùêúùêùùêû...
        # 6. Mathematical Sans-Serif Bold:    ùóîùóïùóñùóóùóòùóÆùóØùó∞ùó±ùó≤...
        # 7. Mathematical Italic:             ùòàùòâùòäùòãùòåùò¢ùò£ùò§ùò•ùò¶...
        # 8. Mathematical Bold Italic:        ùòºùòΩùòæùòøùôÄùôñùôóùôòùôôùôö...
        # 9. Mathematical Monospace:          ùô∞ùô±ùô≤ùô≥ùô¥ùöäùöãùöåùöçùöé...
        # 10. Mathematical Double-Struck:     ùî∏ùîπ‚ÑÇùîªùîºùïíùïìùïîùïïùïñ...
        # 11. Mathematical Bold Fraktur:      ùï¨ùï≠ùïÆùïØùï∞ùñÜùñáùñàùñâùñä...
        # 12. Mathematical Bold Script:       ùìêùìëùìíùììùìîùì™ùì´ùì¨ùì≠ùìÆ...
        # 13. Full-width:                     Ôº°Ôº¢Ôº£Ôº§Ôº• ÔΩÅÔΩÇÔΩÉÔΩÑÔΩÖ...
        "üÖêüÖëüÖíüÖìüÖîüÖïüÖñüÖóüÖòüÖôüÖöüÖõüÖúüÖùüÖûüÖüüÖ†üÖ°üÖ¢üÖ£üÖ§üÖ•üÖ¶üÖßüÖ®üÖ©...",
        # ... (all 13 sets with uppercase, lowercase, and digits)
    ]
    replaceAlphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789"

    # Build character-to-ASCII mapping from all 13 variant sets
    originalMap = {}
    for alphabet in original:
        originalMap.update(dict(zip(alphabet, replaceAlphabet)))

    # Replace each fancy character with its ASCII equivalent
    result = "".join(originalMap.get(char, char) for char in name)
    return result
```

**Example transformations:**
- `"ùìêùìµùì≤ùì¨ùìÆ"` (Bold Script) -> `"Alice"`
- `"ùôπùöòùöëùöó"` (Monospace) -> `"John"`
- `"Ôº≤Ôº°Ôº®ÔºµÔº¨"` (Full-width) -> `"RAHUL"`

---

## 4. Language Detection & Transliteration

### Character-to-Language Mapping (10 Indic Scripts)

The system builds a reverse lookup table mapping every character from 10 Indic scripts to its language code.

```python
# fake.py - Language character database (lines 44-91)
data = {
    "hin": ["‡§º","‡•¶","‡•ß","‡•®","‡§Ö","‡§Ü","‡§á","‡§à","‡§â","‡§ä","‡§ã","‡§è","‡§ê","‡§ì","‡§î",
            "‡§ï","‡§ñ","‡§ó","‡§ò","‡§ô","‡§ö","‡§õ","‡§ú","‡§ù","‡§û","‡§ü","‡§†","‡§°","‡§¢","‡§£",
            "‡§§","‡§•","‡§¶","‡§ß","‡§®","‡§™","‡§´","‡§¨","‡§≠","‡§Æ","‡§Ø","‡§∞","‡§≤","‡§µ","‡§∂",
            "‡§∑","‡§∏","‡§π","‡§æ","‡§ø","‡•Ä","‡•Å","‡•Ç","‡•É","‡•á","‡•à","‡•ã","‡•å","‡•ç"],  # 77 chars
    "pan": ["‡®Ö","‡®Ü","‡®á","‡®à","‡®â","‡®ä","‡®è","‡®ê","‡®ì","‡®î","‡®ï","‡®ñ","‡®ó","‡®ò",...],  # 61 chars
    "guj": ["‡™Ö","‡™Ü","‡™á","‡™à","‡™â","‡™ä","‡™è","‡™ê","‡™ì","‡™î","‡™ï","‡™ñ","‡™ó","‡™ò",...],  # 82 chars
    "ben": ["‡¶Ö","‡¶Ü","‡¶á","‡¶à","‡¶â","‡¶ä","‡¶è","‡¶ê","‡¶ì","‡¶î","‡¶ï","‡¶ñ","‡¶ó","‡¶ò",...],  # 65 chars
    "urd": ["ÿß","ÿ¢","ÿ®","Ÿæ","ÿ™","Ÿπ","ÿ´","ÿ¨","⁄Ü","ÿ≠","ÿÆ","ÿØ","⁄à","ÿ∞",...],  # 41 chars
    "tam": ["‡ÆÖ","‡ÆÜ","‡Æá","‡Æà","‡Æâ","‡Æä","‡Æé","‡Æè","‡Æê","‡Æí","‡Æì","‡Æî","‡Æï","‡Æô",...],  # 62 chars
    "mal": ["‡¥Ö","‡¥Ü","‡¥á","‡¥à","‡¥â","‡¥ä","‡¥ã","‡¥é","‡¥è","‡¥ê","‡¥í","‡¥ì","‡¥î","‡¥ï",...],  # 43 chars
    "kan": ["‡≤Ö","‡≤Ü","‡≤á","‡≤à","‡≤â","‡≤ä","‡≤é","‡≤è","‡≤ê","‡≤í","‡≤ì","‡≤î","‡≤ï","‡≤ñ",...],  # 65 chars
    "ori": ["‡¨Ö","‡¨Ü","‡¨á","‡¨à","‡¨â","‡¨ä","‡¨ã","‡¨è","‡¨ê","‡¨ì","‡¨î","‡¨ï","‡¨ñ","‡¨ó",...],  # 63 chars
    "tel": ["‡∞Ö","‡∞Ü","‡∞á","‡∞à","‡∞â","‡∞ä","‡∞ã","‡∞é","‡∞è","‡∞ê","‡∞í","‡∞ì","‡∞î","‡∞ï",...],  # 65 chars
}

# Build reverse lookup: character ‚Üí language code
char_to_lang = {}
for lang, chars in data.items():
    for char in chars:
        char_to_lang[char] = lang
```

### Language Detection & Transliteration Router

```python
# fake.py - detect_language() (lines 164-176)
def detect_language(word):
    """
    Character-by-character language identification.
    Routes Hindi to custom process_word(), all others to HMM Transliterator.
    """
    for char in word:
        lang = char_to_lang.get(char)
        if lang is not None:
            if lang == 'hin':
                return process_word(word)  # Custom Hindi transliteration
            else:
                trn = Transliterator(source=lang, target='eng', build_lookup=True)
                return trn.transform(word)  # HMM-based ML transliteration
    return word  # Already in English/Latin script
```

### Custom Hindi Transliteration (svar + vyanjan)

Hindi uses a custom rule-based approach with 24 vowel (svar) and 42 consonant (vyanjan) mappings for higher accuracy than the generic HMM model.

```python
# fake.py - Hindi CSV loading and process_word() (lines 129-162)
def load_dict(filename):
    with open(filename, 'r') as f:
        return dict(csv.reader(f))

vowels = load_dict('svar.csv')       # 24 Hindi vowel mappings
consonants = load_dict('vyanjan.csv') # 42 Hindi consonant mappings

def process_word(word):
    """
    Custom Hindi (Devanagari) ‚Üí English transliteration.
    Handles nukta diacritics, matra combinations, and consonant clusters.

    Example: "‡§∞‡§æ‡§π‡•Å‡§≤" ‚Üí "raahul"
    """
    str1 = ""
    i = 0
    while i < len(word):
        # Check for nukta (‡§º) diacritics - two-character combinations
        if (i+1 < len(word) and word[i+1].strip() == '‡§º'.strip()):
            c = word[i] + word[i+1]
            i += 2
        else:
            c = word[i]
            i += 1

        if c in vowels:
            str1 += vowels[c]
        elif c in consonants:
            # Handle consonant + next consonant (cluster) vs consonant + vowel
            if i < len(word) and word[i] in consonants:
                if (c == '‡§ù' and i != 0) or \
                   (i != 0 and i+1 < len(word) and word[i+1] in vowels):
                    str1 += consonants[c]
                else:
                    str1 += consonants[c] + 'a'  # Add inherent 'a' vowel
            else:
                str1 += consonants[c]
        elif c in ['\n','\t',' ','!',',','‡•§','-',':','\\','_','?'] or c.isalnum():
            str1 += c.replace('‡•§', '.')
    return str1
```

### Hindi Vowel Mappings (svar.csv - 24 entries)

```python
# createDict.py - vowels (lines 4-29)
vowels = OrderedDict([
    ('‡§Å','n'),    # Chandrabindu (nasal)
    ('‡§Ç','n'),    # Anusvara
    ('‡§É','a'),    # Visarga
    ('‡§Ö','a'),    # A
    ('‡§Ü','aa'),   # Aa
    ('‡§á','i'),    # I
    ('‡§à','ee'),   # Ii
    ('‡§â','u'),    # U
    ('‡§ä','oo'),   # Uu
    ('‡§ã','ri'),   # Vocalic R
    ('‡§è','e'),    # E
    ('‡§ê','ae'),   # Ai
    ('‡§ì','o'),    # O
    ('‡§î','au'),   # Au
    ('‡§æ','a'),    # Aa matra
    ('‡§ø','i'),    # I matra
    ('‡•Ä','i'),    # Ii matra
    ('‡•Å','u'),    # U matra
    ('‡•Ç','oo'),   # Uu matra
    ('‡•É','ri'),   # Ri matra
    ('‡•á','e'),    # E matra
    ('‡•à','ai'),   # Ai matra
    ('‡•ã','o'),    # O matra
    ('‡•å','au'),   # Au matra
])
```

### Hindi Consonant Mappings (vyanjan.csv - 42 entries)

```python
# createDict.py - consonants (lines 31-80)
consonants = OrderedDict([
    # Velar
    ('‡§ï','k'), ('‡§ñ','kh'), ('‡§ó','g'), ('‡§ò','gh'), ('‡§ô','ng'),
    # Palatal
    ('‡§ö','ch'), ('‡§õ','chh'), ('‡§ú','j'), ('‡§ú‡§º','z'), ('‡§ù','jh'), ('‡§û','nj'),
    # Retroflex
    ('‡§ü','t'), ('‡§†','th'), ('‡§°','d'), ('‡§°‡§º','r'), ('‡§¢','dh'), ('‡§£','n'),
    # Dental
    ('‡§§','t'), ('‡§•','th'), ('‡§¶','d'), ('‡§ß','dh'), ('‡§®','n'),
    # Labial
    ('‡§™','p'), ('‡§´','ph'), ('‡§´‡§º','f'), ('‡§¨','b'), ('‡§≠','bh'), ('‡§Æ','m'),
    # Semi-vowels
    ('‡§Ø','y'), ('‡§∞','r'), ('‡§≤','l'), ('‡§µ','v'),
    # Sibilants
    ('‡§∂','sh'), ('‡§∑','sh'), ('‡§∏','s'),
    # Glottal
    ('‡§π','h'),
    # Complex conjuncts
    ('‡§ï‡•ç‡§∑','ksh'), ('‡§§‡•ç‡§∞','tr'), ('‡§ú‡•ç‡§û','gy'),
])
# Note: Nukta variants (‡§ï‡§º,‡§ñ‡§º,‡§ó‡§º,‡§ú‡§º,‡§°‡§º,‡§¢‡§º,‡§´‡§º) have duplicate
# entries for different Unicode representations of the same character
```

### HMM-Based ML Transliteration (indictrans)

For non-Hindi Indic scripts (Bengali, Tamil, Telugu, etc.), the system uses pre-trained HMM models.

```
Each language model contains:
‚îú‚îÄ‚îÄ coef_.npy           # HMM coefficient matrix
‚îú‚îÄ‚îÄ classes.npy         # Output character mapping
‚îú‚îÄ‚îÄ intercept_init_.npy # Initial state probabilities
‚îú‚îÄ‚îÄ intercept_trans_.npy# Transition probabilities
‚îú‚îÄ‚îÄ intercept_final_.npy# Final state probabilities
‚îî‚îÄ‚îÄ sparse.vec          # Feature vocabulary

ML Pipeline:
1. UTF-8 ‚Üí WX notation (ISO 15919 encoding)
2. Feature extraction: character n-gram context windows
3. HMM prediction: Linear classifier + Viterbi decoder
4. WX ‚Üí UTF-8 (target English script)
```

### Non-Indic Language Detection

Detects scripts that should not appear in names of Indian Instagram users, indicating bot accounts.

```python
# fake.py - check_lang_other_than_indic() (lines 123-126)
def check_lang_other_than_indic(symbolic_name):
    """
    Detects non-Indic scripts via Unicode regex.
    Pattern: r'[Œë-Œ©Œ±-œâ‘±-’ñ·Éê-·É∞‰∏Ä-ÈøøÍ∞Ä-Ìû£]+'

    Detects:
    - Greek:    Œë-Œ© (uppercase), Œ±-œâ (lowercase)
    - Armenian: ‘±-’ñ
    - Georgian: ·Éê-·É∞
    - Chinese:  ‰∏Ä-Èøø (CJK Unified Ideographs)
    - Korean:   Í∞Ä-Ìû£ (Hangul Syllables)

    Returns: 1 (FAKE) if non-Indic detected, 0 (REAL) otherwise
    """
    if not symbolic_name:
        print(symbolic_name)
    return 1 if re.search(r'[Œë-Œ©Œ±-œâ‘±-’ñ·Éê-·É∞‰∏Ä-ÈøøÍ∞Ä-Ìû£]+', symbolic_name, re.UNICODE) else 0
```

### Unicode Decode (Final ASCII Normalization)

```python
# fake.py - uni_decode() (lines 181-182)
def uni_decode(row):
    """Final pass: convert any remaining Unicode to ASCII."""
    return unidecode(row, errors='preserve')
```

---

## 5. Five-Feature Ensemble Model

### Feature 1: Non-Indic Language Detection (0 or 1)

Already shown above in `check_lang_other_than_indic()`. Returns 1 if Greek, Armenian, Georgian, Chinese, or Korean characters found.

### Feature 2: Digit Count in Handle (0 or 1)

```python
# fake.py (lines 213-220)
def count_numerical_digits(text):
    """Count digits in handle. 'rahul_12345' ‚Üí 5"""
    if not isinstance(text, str):
        text = str(text)
    return sum(c.isdigit() for c in text)

def fake_real_more_than_4_digit(number):
    """Threshold: >4 digits = FAKE (1), otherwise REAL (0)"""
    return 1 if number > 4 else 0
```

### Feature 3: Handle-Name Special Character Correlation (0, 1, or 2)

```python
# fake.py - process() (lines 188-200)
def process(follower_handle, cleaned_handle, cleaned_name):
    """
    Decision tree based on special character presence:
    - Has _ or - or . AND single-word name AND similarity > 80 ‚Üí 0 (REAL)
    - Has _ or - or . AND single-word name AND similarity <= 80 ‚Üí 1 (FAKE)
    - Has _ or - or . AND multi-word name ‚Üí 0 (REAL)
    - No special chars ‚Üí 2 (INCONCLUSIVE)
    """
    SPECIAL_CHARS = ('_', '-', '.')

    if any(char in follower_handle for char in SPECIAL_CHARS):
        if not ' ' in cleaned_name:
            if generate_similarity_score(cleaned_handle, cleaned_name) > 80:
                return 0  # REAL
            else:
                return 1  # FAKE
        else:
            return 0  # Multi-word name with separators = REAL
    else:
        return 2  # No special chars = INCONCLUSIVE
```

### Feature 4: RapidFuzz Weighted Similarity Scoring (0-100)

```python
# fake.py - generate_similarity_score() (lines 225-244)
def generate_similarity_score(handle, name):
    """
    Weighted ensemble of 3 RapidFuzz metrics across all name permutations.
    Formula: (2 * partial_ratio + token_sort_ratio + token_set_ratio) / 4

    partial_ratio gets 2x weight because substring matching catches
    handles that are abbreviations of names (e.g., "rahul" in "rahul_kumar").
    """
    name = name.split()
    if len(name) <= 4:
        name_permutations = [' '.join(p) for p in permutations(name)]
        # Max 4 words = 4! = 24 permutations
    else:
        name_permutations = name  # Too many permutations, skip

    similarity_score = -1
    cleaned_handle = handle.replace(' ', '')

    for name in name_permutations:
        cleaned_name = name.replace(' ', '')
        partial_ratio = fuzzz.partial_ratio(cleaned_handle.lower(), cleaned_name.lower())
        token_sort_ratio = fuzzz.token_sort_ratio(cleaned_handle.lower(), cleaned_name.lower())
        token_set_ratio = fuzzz.token_set_ratio(cleaned_handle.lower(), cleaned_name.lower())

        # Weighted average: partial gets 2x weight
        similarity_score = max(similarity_score,
                              (2 * partial_ratio + token_sort_ratio + token_set_ratio) / 4)

    return similarity_score

def based_on_partial_ratio(similarity_score):
    """Threshold at 90: above = REAL (0), below = FAKE (1)"""
    if similarity_score > 90:
        return 0
    return 1
```

### Feature 5: Indian Name Database Matching (0-100)

```python
# fake.py - check_indian_names() (lines 286-312)
# Global: loaded once at module import
name_data = pd.read_csv('baby_names.csv')
namess = name_data['Baby Names'].str.lower()  # 35,183 names

def score(i, first_name):
    """Same weighted fuzzy formula for name matching."""
    i = i.lower()
    first_name = first_name.lower()
    ratio = fuzzz.ratio(i, first_name)
    token_sort_ratio = fuzzz.token_sort_ratio(i, first_name)
    token_set_ratio = fuzzz.token_set_ratio(i, first_name)
    return (2 * ratio + token_sort_ratio + token_set_ratio) / 4

def check_indian_names(name):
    """
    Fuzzy match against 35,183 Indian baby names.
    Checks both first name and last name independently.
    Returns max similarity score (0-100).
    """
    if len(name) < 2:
        return 1  # Too short to be a real name

    similarity_score = 0
    name = name.split()
    first_name = name[0]
    last_name = name[1] if len(name) >= 2 else None

    # Match first name against entire database
    for i in namess:
        similarity_score = max(similarity_score, score(i, first_name))

    # Match last name if present
    if last_name:
        if len(last_name) < 2:
            similarity_score = 1  # Short last name = suspicious
        else:
            for i in namess:
                similarity_score = max(similarity_score, score(i, last_name))

    return similarity_score
```

### Handle Cleaning (Shared Normalization)

```python
# fake.py - clean_handle() (lines 203-210)
def clean_handle(handle):
    """
    Multi-stage normalization for both handles and names.
    Steps:
    1. [_\-.] ‚Üí space (separators become spaces)
    2. [^\w\s] ‚Üí removed (non-word, non-space chars)
    3. \d ‚Üí removed (all digits)
    4. [^a-zA-Z\s] ‚Üí removed (non-Latin chars)
    5. lowercase + strip
    """
    if not handle or isinstance(handle, float):
        return ''
    cleaned_handle = re.sub(r'[_\-.]', ' ', handle)
    cleaned_handle = re.sub(r'[^\w\s]', '', cleaned_handle)
    cleaned_handle = re.sub(r'\d', '', cleaned_handle).lower()
    cleaned_handle = re.sub(r'[^a-zA-Z\s]', '', cleaned_handle).strip()
    return cleaned_handle

def only_numeric(name):
    """Check if handle is purely numeric (no letters at all)."""
    if re.search(r'[a-zA-Z]', name):
        return 0
    else:
        return 1
```

---

## 6. Ensemble Scoring Functions

### Binary Feature Combination - process1()

```python
# fake.py - process1() (lines 252-261)
def process1(fake_real_based_on_lang, number_more_than_4_handle, chhitij_logic):
    """
    Binary classifier using 3 features.
    Priority order: language ‚Üí digits ‚Üí special chars
    Returns: 0 (REAL), 1 (FAKE), 2 (INCONCLUSIVE)
    """
    if fake_real_based_on_lang:
        return 1  # Non-Indic language = FAKE
    if number_more_than_4_handle:
        return 1  # >4 digits = FAKE
    if chhitij_logic == 1:
        return 1  # Special char mismatch = FAKE
    elif chhitij_logic == 2:
        return 2  # No special chars = INCONCLUSIVE
    return 0      # Default = REAL
```

### Weighted Final Score - final()

```python
# fake.py - final() (lines 263-277)
def final(fake_real_based_on_lang, similarity_score,
          number_more_than_4_handle, chhitij_logic):
    """
    Produces the final confidence score: 0.0, 0.33, or 1.0

    Decision priority:
    1. Non-Indic language ‚Üí 1.0 (100% FAKE)
    2. Similarity 0-40   ‚Üí 0.33 (33% FAKE, weak signal)
    3. >4 digits         ‚Üí 1.0 (100% FAKE)
    4. Special char mismatch ‚Üí 1.0 (100% FAKE)
    5. No special chars   ‚Üí 0.0 (REAL)
    6. Default            ‚Üí 0.0 (REAL)
    """
    if fake_real_based_on_lang:
        return 1      # 100% FAKE
    if 0 < similarity_score <= 40:
        return 0.33   # Weak FAKE signal
    if number_more_than_4_handle:
        return 1      # 100% FAKE
    if chhitij_logic == 1:
        return 1      # 100% FAKE
    elif chhitij_logic == 2:
        return 0      # REAL
    return 0          # Default: REAL
```

---

## 7. AWS Data Pipeline - push.py

### ClickHouse Data Extraction with CTE Queries

```python
# push.py (lines 1-64) - Data extraction
import itertools
import json
import multiprocessing
from datetime import datetime
import boto3
import clickhouse_connect

# AWS and ClickHouse connections
sqs = boto3.resource('sqs', region_name='ap-south-1')
s3 = boto3.resource('s3', ...)
kinesis_client = boto3.client('kinesis', region_name='ap-south-1')
client = clickhouse_connect.get_client(
    host="ec2-52-66-200-31.ap-south-1.compute.amazonaws.com",
    user="cube", password="..."
)

current_date = datetime.today()
file_name = f"{current_date.year}_{current_date.month}_{current_date.day}_creator_followers.json"

# Complex CTE query: extract follower data from ClickHouse ‚Üí S3
query = f"""
    INSERT INTO FUNCTION s3(
        'https://gcc-social-data.s3.ap-south-1.amazonaws.com/temp/{file_name}',
        'AKIA...', '...', 'JSONEachRow'
    )
    WITH
        handles AS (
            SELECT Names as handle
            FROM s3('...creators_handles.csv', '...', 'CSVWithNames')
        ),
        profile_ids AS (
            SELECT ig_id FROM dbt.mart_instagram_account mia
            WHERE handle IN (SELECT handle FROM handles)
        ),
        follower_data AS (
            SELECT log.target_profile_id,
                   JSONExtractString(source_dimensions, 'handle') AS follower_handle,
                   JSONExtractString(source_dimensions, 'full_name') AS follower_full_name
            FROM dbt.stg_beat_profile_relationship_log log
            WHERE target_profile_id IN profile_ids
              AND follower_handle IS NOT NULL AND follower_handle != ''
              AND follower_full_name IS NOT NULL AND follower_full_name != ''
        ),
        follower_events_data AS (
            SELECT log.target_profile_id,
                   JSONExtractString(source_dimensions, 'handle') AS follower_handle,
                   JSONExtractString(source_dimensions, 'full_name') AS follower_full_name
            FROM _e.profile_relationship_log_events log
            WHERE target_profile_id IN profile_ids
              AND follower_handle IS NOT NULL AND follower_handle != ''
        ),
        data AS (
            SELECT * FROM follower_data
            UNION ALL
            SELECT * FROM follower_events_data
        )
    SELECT mia.handle, follower_handle, follower_full_name
    FROM data
    INNER JOIN dbt.mart_instagram_account mia ON mia.ig_id = target_profile_id
    GROUP BY handle, follower_handle, follower_full_name
    SETTINGS s3_truncate_on_insert=1
"""
result = client.query(query)
```

### 8-Worker Multiprocessing Batch Distribution

```python
# push.py (lines 66-143) - SQS message distribution

# Download S3 file locally
s3.Bucket('gcc-social-data').download_file(f"temp/{file_name}", file_name)

# Create SQS queue
queue = sqs.create_queue(QueueName='creator_follower_in',
                         Attributes={
                             'MaximumMessageSize': '262144',      # 256 KB
                             'MessageRetentionPeriod': '345600',  # 4 days
                             'VisibilityTimeout': '30'            # 30 seconds
                         })

# Create Kinesis stream (ON_DEMAND auto-scaling)
try:
    kinesis_client.create_stream(StreamName='creator_out',
                                 StreamModeDetails={'StreamMode': 'ON_DEMAND'})
except Exception as e:
    if e.__class__.__name__ == 'ResourceInUseException':
        print("Stream already created")

def final(messages):
    """Send batch of messages to SQS (max 10 per API call)."""
    response = queue.send_messages(Entries=messages)
    failed = response.get('Failed')
    if failed:
        print(failed)

def divide_into_buckets(lst, num_buckets):
    """Round-robin distribution across N buckets."""
    buckets = [[] for _ in range(num_buckets)]
    for index, item in enumerate(lst):
        bucket_index = index % num_buckets
        buckets[bucket_index].append(item)
    return buckets

# Main processing loop: 10K lines at a time, 8 parallel workers
with open(file_name, 'r') as f:
    while True:
        batch_lines = list(itertools.islice(f, 10000))
        if not batch_lines:
            break
        messages = [{'Id': str(i), 'MessageBody': row}
                    for i, row in enumerate(batch_lines)]

        num_buckets = 8
        buckets = divide_into_buckets(messages, num_buckets)

        pool = multiprocessing.Pool(processes=num_buckets)
        pool.map(final, buckets)
        pool.close()
        pool.join()
```

---

## 8. Results Retrieval - pull.py

### Multi-Shard Parallel Kinesis Reader

```python
# pull.py (lines 69-131) - Kinesis stream reader
from datetime import datetime
import json
import time
import boto3
from multiprocessing import Pool

column_name = ['handle', 'follower_handle', 'follower_full_name',
               'symbolic_name', 'fake_real_based_on_lang',
               'transliterated_follower_name', 'decoded_name',
               'cleaned_handle', 'cleaned_name', 'chhitij_logic',
               'number_handle', 'number_more_than_4_handle',
               'similarity_score', 'fake_real_based_on_fuzzy_score_90',
               'process1_', 'final_', 'numeric_handle',
               'indian_name_score', 'score_80']

client = boto3.client('kinesis')
stream_name = 'creator_out'

current_date = datetime.today()
file_name = f"{current_date.year}_{current_date.month}_{current_date.day}" \
            f"_creator_followers_final_fake_analysis.json"

def process_shard(shard_id, starting_sequence_number):
    """Process a single Kinesis shard - reads all records sequentially."""
    total_length = 0
    shard_iterator = client.get_shard_iterator(
        StreamName=stream_name,
        ShardId=shard_id,
        ShardIteratorType='AFTER_SEQUENCE_NUMBER',
        StartingSequenceNumber=starting_sequence_number
    )['ShardIterator']

    while True:
        messages = []
        response = client.get_records(ShardIterator=shard_iterator, Limit=10000)
        if len(response['Records']) < 1:
            break
        shard_iterator = response['NextShardIterator']
        response = response['Records']
        total_length += len(response)

        for record in response:
            response = json.loads(record['Data'])
            response = list(response.values())
            row_dict = dict(zip(column_name, response))
            messages.append(row_dict)

        with open(file_name, 'a') as f:
            for message in messages:
                f.write(json.dumps(message) + '\n')

if __name__ == '__main__':
    pool = Pool()
    shards = client.list_shards(StreamName=stream_name)['Shards']
    args = [(shard['ShardId'],
             shard['SequenceNumberRange']['StartingSequenceNumber'])
            for shard in shards]
    pool.starmap(process_shard, args)
    pool.close()
    pool.join()
```

---

## 9. Docker Container - Dockerfile

```dockerfile
# Dockerfile (23 lines) - Lambda ECR container
FROM public.ecr.aws/lambda/python:3.10

# System dependencies for indictrans Cython compilation
RUN yum install -y gcc-c++ pkgconfig poppler-cpp-devel

# Install Python dependencies and indictrans from source
COPY requirement.txt ./
COPY indic-trans-master ./
RUN pip install -r requirements.txt
RUN pip install .  # Installs indictrans from setup.py

# Copy pre-trained HMM models to site-packages
RUN cp -r indictrans/models /var/lang/lib/python3.10/site-packages/indictrans/
RUN cp -r indictrans/mappings /var/lang/lib/python3.10/site-packages/indictrans/

# Copy Hindi transliteration mappings
COPY svar.csv ./      # 24 vowel mappings
COPY vyanjan.csv ./   # 42 consonant mappings
RUN pip install --upgrade pip && \
    pip install -r requirement.txt && \
    pip install --upgrade numpy

# Copy application code and data
COPY fake.py ./
COPY baby_names_.csv ./baby_names.csv
RUN rm -r indictrans

# Lambda entry point
CMD [ "fake.handler" ]
```

**Key design decisions:**
- Uses `public.ecr.aws/lambda/python:3.10` base image for Lambda compatibility
- Compiles indictrans from source (requires gcc-c++ for Cython .pyx files)
- Copies HMM models into site-packages path so indictrans can find them
- Renames `baby_names_.csv` to `baby_names.csv` during build

---

## 10. Hindi Transliteration - createDict.py

```python
# createDict.py (91 lines) - Generates svar.csv and vyanjan.csv
import csv
from collections import OrderedDict

vowels = OrderedDict([
    ('‡§Å','n'), ('‡§Ç','n'), ('‡§É','a'),
    ('‡§Ö','a'), ('‡§Ü','aa'), ('‡§á','i'), ('‡§à','ee'),
    ('‡§â','u'), ('‡§ä','oo'), ('‡§ã','ri'),
    ('‡§è','e'), ('‡§ê','ae'), ('‡§ì','o'), ('‡§î','au'),
    ('‡§æ','a'), ('‡§ø','i'), ('‡•Ä','i'), ('‡•Å','u'), ('‡•Ç','oo'),
    ('‡•É','ri'), ('‡•á','e'), ('‡•à','ai'), ('‡•ã','o'), ('‡•å','au')
])

consonants = OrderedDict([
    ('‡§ï','k'), ('‡§ñ','kh'), ('‡§ó','g'), ('‡§ò','gh'), ('‡§ô','ng'),
    ('‡§ö','ch'), ('‡§õ','chh'), ('‡§ú','j'), ('‡§ú‡§º','z'), ('‡§ù','jh'), ('‡§û','nj'),
    ('‡§ü','t'), ('‡§†','th'), ('‡§°','d'), ('‡§°‡§º','r'), ('‡§¢','dh'), ('‡§£','n'),
    ('‡§§','t'), ('‡§•','th'), ('‡§¶','d'), ('‡§ß','dh'), ('‡§®','n'),
    ('‡§™','p'), ('‡§´','ph'), ('‡§´‡§º','f'), ('‡§¨','b'), ('‡§≠','bh'), ('‡§Æ','m'),
    ('‡§Ø','y'), ('‡§∞','r'), ('‡§≤','l'), ('‡§µ','v'),
    ('‡§∂','sh'), ('‡§∑','sh'), ('‡§∏','s'), ('‡§π','h'),
    ('‡§ï‡•ç‡§∑','ksh'), ('‡§§‡•ç‡§∞','tr'), ('‡§ú‡•ç‡§û','gy')
])

# Note: Nukta variants (‡§ú‡§º, ‡§°‡§º, ‡§´‡§º) have duplicate entries for different
# Unicode representations - one with combining nukta, one with precomposed form.
# Comment from code: "these two are very different, see them in unicode"

with open('svar.csv', 'w') as f:
    csvwriter = csv.writer(f)
    for k, v in zip(vowels.keys(), vowels.values()):
        csvwriter.writerow([k, v])

with open('vyanjan.csv', 'w') as f:
    csvwriter = csv.writer(f)
    for k, v in zip(consonants.keys(), consonants.values()):
        csvwriter.writerow([k, v])
```

---

## Output Schema (16 Fields)

| # | Field | Type | Meaning |
|---|-------|------|---------|
| 1 | `symbolic_name` | str | Name after Unicode symbol normalization |
| 2 | `fake_real_based_on_lang` | 0/1 | Non-Indic language detected |
| 3 | `transliterated_follower_name` | str | Name after Indic ‚Üí English transliteration |
| 4 | `decoded_name` | str | Final ASCII-normalized name |
| 5 | `cleaned_handle` | str | Handle after multi-stage cleaning |
| 6 | `cleaned_name` | str | Name after multi-stage cleaning |
| 7 | `chhitij_logic` | 0/1/2 | Special character correlation result |
| 8 | `number_handle` | int | Count of digits in handle |
| 9 | `number_more_than_4_handle` | 0/1 | Digit count exceeds threshold |
| 10 | `similarity_score` | 0-100 | Weighted fuzzy similarity |
| 11 | `fake_real_based_on_fuzzy_score_90` | 0/1 | Similarity above 90 threshold |
| 12 | `process1_` | 0/1/2 | Binary ensemble result |
| 13 | `final_` | 0.0/0.33/1.0 | **Final fake probability** |
| 14 | `numeric_handle` | 0/1 | Purely numeric handle flag |
| 15 | `indian_name_score` | 0-100 | Name database match score |
| 16 | `score_80` | 0/1 | Name score above 80 threshold |

---

## Performance Characteristics

```
Per-record timing:
  Symbol conversion:     1-5ms
  Language detection:    1-2ms
  Transliteration:       5-10ms (ML inference for non-Hindi)
  Fuzzy scoring:         5-15ms (up to 24 permutations)
  Indian name check:     10-50ms (linear scan of 35,183 names)
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL:                 50-100ms per follower

Throughput:
  Single Lambda:         10-20 records/second
  8 parallel workers:    80-160 records/second
  Daily batch (100K):    ~10-20 minutes
  Monthly (3M):          ~5-10 hours
```

---

*This document covers all 955+ lines of source code across fake.py (385 lines), push.py (154 lines), pull.py (131 lines), createDict.py (91 lines), and Dockerfile (23 lines).*
