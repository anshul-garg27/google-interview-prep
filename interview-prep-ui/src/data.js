// Auto-generated from markdown files
export const documents = [
  {
    "id": "GOOGLEYNESS_ALL_QUESTIONS",
    "title": "60+ Googleyness Questions",
    "category": "interview",
    "badge": "Must Read",
    "content": "# GOOGLEYNESS - ALL 70+ QUESTIONS WITH ANSWERS\n## LeetCode + Discord + Real Interview Experiences Se Curated\n\n---\n\n# SECTION 1: GENERAL BEHAVIORAL QUESTIONS (11 Questions)\n\n---\n\n## Q1: \"Tell me about a time when your manager set reasonable demands. Follow up: Describe a situation with unreasonable demands.\"\n\n### REASONABLE DEMANDS - Answer:\n\n> \"My manager at Good Creator Co set a reasonable demand when he asked me to build the data platform using Airflow and dbt within 3 months.\n>\n> **Why it was reasonable:**\n> - He gave me time to learn the technologies (I was new to Airflow/dbt)\n> - He broke it into milestones: Month 1 - POC, Month 2 - Core DAGs, Month 3 - Full rollout\n> - He provided resources - access to ClickHouse documentation, budget for a dbt course\n> - He was available for design reviews and unblocking\n>\n> **Result:** I delivered 76 DAGs and 112 dbt models. The phased approach let me learn while delivering.\"\n\n### UNREASONABLE DEMANDS - Follow-up Answer:\n\n> \"Once, there was a push to integrate with a new social media API in just 2 days, including testing and deployment.\n>\n> **Why it was unreasonable:**\n> - New API meant unknown rate limits, response formats, error handling\n> - 2 days wasn't enough for proper testing\n> - No buffer for production issues\n>\n> **How I handled it:**\n> I didn't just say 'no'. I said: 'I can deliver a basic integration in 2 days, but it won't be production-ready. Here's what we'd be risking...'\n>\n> I proposed: 'Give me 4 days for production-quality, or 2 days for a beta version behind a feature flag.'\n>\n> We went with the feature flag approach - delivered fast, validated, then hardened.\"\n\n---\n\n## Q2: \"Tell me about one of the biggest accomplishments in your career so far.\"\n\n> \"Building the event-driven architecture that transformed how we handle time-series data at Good Creator Co.\n>\n> **The Challenge:**\n> We were storing 10 million daily data points directly in PostgreSQL. Query times were increasing, we were losing historical granularity, and the database was becoming a bottleneck.\n>\n> **What I Built:**\n> I designed and implemented an event-driven pipeline:\n> - beat publishes events to RabbitMQ instead of direct DB writes\n> - event-grpc (Go service) consumes with buffered sinkers - 1000 events/batch, 5-sec flush\n> - ClickHouse stores time-series data\n> - stir (Airflow + dbt) transforms and syncs back to PostgreSQL\n>\n> **Why It's My Biggest Accomplishment:**\n> 1. **Technical Depth**: I designed the architecture, wrote Go code (new language for me), built the dbt models\n> 2. **Cross-Service Impact**: Changed how 4 different services work together\n> 3. **Measurable Results**: 2.5x faster log retrieval, 10,000+ events/sec, zero data loss\n> 4. **Lasting Impact**: Running 15+ months in production\n>\n> This wasn't just a feature - it was a foundational change to our data infrastructure.\"\n\n---\n\n## Q3: \"Tell me about a time when you faced a challenging situation at work.\"\n\n> \"Building fake follower detection with no training data and 10 Indian languages to support.\n>\n> **The Challenge:**\n> Brands wanted to know which influencer followers were fake. But:\n> - No labeled dataset of fake vs real followers\n> - No clear definition of 'fake'\n> - Followers had names in Hindi, Bengali, Tamil, Telugu... 10 scripts\n>\n> **How I Approached It:**\n>\n> First, I broke down 'fake' into observable signals:\n> - Greek/Chinese text in Indian influencer's followers = suspicious\n> - Username like 'user12345678' = suspicious\n> - Display name doesn't match username = suspicious\n>\n> For multi-language, I built a transliteration pipeline using HMM models - convert Hindi script to English phonetically, then compare.\n>\n> I designed 3-level scoring (0.0, 0.33, 1.0) instead of binary - acknowledging uncertainty.\n>\n> **Result:**\n> - 85% accuracy on manually validated accounts\n> - Processes millions of followers\n> - Runs cost-effectively on AWS Lambda\n> - Brands now make data-driven influencer decisions\"\n\n---\n\n## Q4: \"How do you manage multiple priorities? Do you prefer working in a dynamic environment with changing priorities or doing the same type of work repeatedly?\"\n\n> \"I use a simple framework for managing priorities:\n>\n> **My Prioritization System:**\n> 1. **What unblocks others?** - If my delay blocks the team, that's #1\n> 2. **What has customer impact?** - External commitments over internal work\n> 3. **What's the impact/effort ratio?** - Quick wins with big impact first\n>\n> **Example:**\n> At Good Creator Co, I often had competing priorities - new feature requests, production bugs, technical debt.\n>\n> One week I had: new leaderboard feature (product wanted urgently), DAG reliability fixes (internal), and API rate limit issues (production).\n>\n> I chose: Rate limit fix first (production impact), then DAG reliability (unblocked team), then leaderboard (new feature can wait a few days).\n>\n> **Dynamic vs Repetitive:**\n> Honestly, I prefer dynamic environments. At Good Creator Co, one day I'm writing Python scrapers, next day Go consumers, next day dbt SQL.\n>\n> The variety keeps me learning. Repetitive work I try to automate - that's why I built configurable worker pools with 73 different flows, rather than writing separate code for each.\"\n\n---\n\n## Q5: \"Tell me about a time you set a goal for yourself and how you approached achieving it.\"\n\n> \"I set a goal to learn the modern data stack - Airflow, dbt, ClickHouse - in 3 months while delivering production code.\n>\n> **Why This Goal:**\n> We needed a data platform, and I wanted to build it with modern tools rather than legacy approaches. But I had zero experience with these technologies.\n>\n> **My Approach:**\n>\n> **Month 1 - Foundation:**\n> - Took an online dbt course (2 hours/day after work)\n> - Built toy Airflow DAGs locally\n> - Read ClickHouse documentation, especially ReplacingMergeTree\n>\n> **Month 2 - Application:**\n> - Built first 10 production DAGs\n> - Wrote 20 dbt models (staging layer)\n> - Failed a lot, learned from each failure\n>\n> **Month 3 - Scale:**\n> - Expanded to 76 DAGs, 112 dbt models\n> - Taught team members what I learned\n> - Wrote internal documentation\n>\n> **Result:**\n> - Delivered production data platform on time\n> - Became the team's go-to person for data engineering\n> - The goal forced structured learning with immediate application\"\n\n---\n\n## Q6: \"Describe a positive leadership or managerial style you liked from one of your previous managers. How did it influence your work style?\"\n\n> \"My best manager had a 'context, not control' style.\n>\n> **What He Did:**\n> - Gave me full context on WHY we were building something\n> - Set clear outcomes but let me choose HOW to achieve them\n> - Regular 1:1s focused on unblocking, not micromanaging\n> - Celebrated failures as learning opportunities\n>\n> **Specific Example:**\n> When building beat, he said: 'We need to scrape 10M data points daily without getting banned by APIs. Figure out how.'\n>\n> He didn't prescribe the architecture. I designed worker pools, rate limiting, credential rotation. When I made mistakes (like the GPT timeout issue), he asked 'What did you learn?' not 'Why did you fail?'\n>\n> **How It Influenced Me:**\n> Now when I work with junior engineers, I do the same:\n> - Explain the 'why' thoroughly\n> - Give ownership of the 'how'\n> - Be available for questions but don't hover\n> - Treat failures as learning, not blame\n>\n> When I helped a junior engineer debug Airflow, I walked through the process WITH them rather than fixing it myself.\"\n\n---\n\n## Q7: \"Tell me about a time when you received critical feedback from your manager. How did you respond, and what actions did you take to improve?\"\n\n> \"My manager once told me my technical documentation was 'too brief and assumes too much knowledge.'\n>\n> **The Feedback:**\n> I had written a design doc for the event-grpc buffered sinker. I thought it was clear. He said: 'A new team member couldn't implement this from your doc. You've skipped too many details.'\n>\n> **My Initial Reaction:**\n> Honestly, I was a bit defensive internally. I thought 'I know this system deeply, the doc makes sense to me.'\n>\n> **What I Did:**\n> 1. I asked him to show me specifically which parts were unclear\n> 2. I asked a junior engineer to read the doc and tell me what confused them\n> 3. I rewrote the doc with:\n>    - Architecture diagrams\n>    - Step-by-step data flow\n>    - Code snippets with comments\n>    - FAQ section for common questions\n>\n> **Result:**\n> The new doc became a template for other system docs. Junior engineers could onboard faster.\n>\n> **What I Learned:**\n> Documentation isn't for me - it's for the reader. Now I always ask: 'Could someone new understand this?' I also get peer review on docs before finalizing.\"\n\n---\n\n## Q8: \"Describe a situation where you had a disagreement with a colleague or manager. How did you resolve the conflict, and what was the outcome?\"\n\n> \"I disagreed with a senior engineer about using MongoDB vs ClickHouse for our analytics platform.\n>\n> **The Disagreement:**\n> He strongly advocated for MongoDB - he had expertise, it's flexible, good for documents. I believed ClickHouse was better for our OLAP queries - aggregations over billions of rows.\n>\n> **How I Resolved It:**\n>\n> **Step 1 - Understand their perspective:**\n> I asked: 'Help me understand why MongoDB fits here?' He explained: familiarity, schema flexibility, faster development.\n>\n> **Step 2 - Propose data-driven resolution:**\n> Instead of arguing opinions, I said: 'What if we benchmark both with our actual queries?'\n>\n> **Step 3 - Run fair experiment:**\n> We tested with 100M rows:\n> - MongoDB: 45 seconds for aggregation\n> - ClickHouse: 0.8 seconds for same query\n>\n> **Step 4 - Present objectively:**\n> I acknowledged his valid points: 'You're right about flexibility. But for analytics, performance difference is 50x.'\n>\n> **Outcome:**\n> We went with ClickHouse. He actually became an advocate after seeing production performance. Our relationship improved because I respected his expertise and let data decide.\"\n\n---\n\n## Q9: \"How do you prioritize and manage multiple tasks or projects? Provide an example of a time when you successfully juggled several tasks at once.\"\n\n> \"I was simultaneously working on three major projects at Good Creator Co:\n>\n> 1. **beat rate limiting improvements** - Production issues with API bans\n> 2. **stir DAG development** - New analytics requirements\n> 3. **fake_follower_analysis** - Entire new system to build\n>\n> **How I Managed:**\n>\n> **Time Boxing:**\n> - Mornings (9-12): Deep work on fake_follower (new system, needed focus)\n> - After lunch (1-3): stir DAGs (incremental work, less focus needed)\n> - Afternoons (3-5): beat issues (reactive, meetings, reviews)\n>\n> **Communication:**\n> - Weekly updates to stakeholders on each project\n> - Clear about what was possible: 'fake_follower will be done in 4 weeks, not 2'\n>\n> **Ruthless Prioritization:**\n> When beat had a production incident, everything else paused. Production > new features.\n>\n> **Result:**\n> - beat rate limiting deployed, zero API bans since\n> - stir expanded to 76 DAGs on schedule\n> - fake_follower shipped in 5 weeks (1 week over, but fully tested)\n>\n> The key was accepting that I couldn't do everything at once - just the most important thing right now.\"\n\n---\n\n## Q10: \"Tell me about a time you had to manage a critical project under tight deadlines. How did you ensure completion on time?\"\n\n> \"Building the ClickHouse → PostgreSQL sync pipeline in 2 weeks for a major client demo.\n>\n> **The Situation:**\n> Product team had committed to showing real-time analytics in a client demo. We needed data flowing from ClickHouse (analytics) to PostgreSQL (application) in 2 weeks.\n>\n> **How I Ensured Completion:**\n>\n> **Day 1-2: Scope Ruthlessly**\n> I identified MVP: sync just the leaderboard table, not all 15 tables. That's what the demo needed.\n>\n> **Day 3-7: Build Core**\n> Built the three-layer sync:\n> - ClickHouse → S3 export\n> - S3 → PostgreSQL server download\n> - PostgreSQL atomic table swap\n>\n> **Day 8-10: Handle Edge Cases**\n> What if S3 upload fails? What if swap fails mid-way? Added retry logic and rollback capability.\n>\n> **Day 11-12: Testing**\n> Tested with production-like data. Found a bug - large JSON files causing memory issues. Fixed with streaming.\n>\n> **Day 13-14: Buffer**\n> Had 2 days buffer. Used it for documentation and team walkthrough.\n>\n> **Result:**\n> Demo happened on time. Client signed. Pipeline has been running reliably since.\n>\n> **Key Lesson:**\n> Tight deadlines require ruthless scoping. Don't try to do everything - do the most important thing well.\"\n\n---\n\n## Q11: \"How do you handle situations where work assigned to you keeps getting de-prioritized and changed repeatedly? How would you feel about it?\"\n\n> \"This happened with a feature I was building - Instagram Stories analytics.\n>\n> **The Situation:**\n> I started building Stories analytics. One week in, priorities shifted to YouTube Shorts. Two weeks later, back to Instagram but different feature. Then paused entirely.\n>\n> **How I Felt:**\n> Honestly, frustrated. I'd invested time in understanding Stories API, wrote initial code, then it got shelved.\n>\n> **How I Handled It:**\n>\n> **1. Understand the 'why':**\n> I asked my manager: 'What's driving these changes?' Turns out, a major client's needs kept shifting. That's business reality.\n>\n> **2. Extract value from incomplete work:**\n> The Stories code wasn't wasted - I refactored it into reusable components. When we eventually built Stories analytics, I had a head start.\n>\n> **3. Communicate impact:**\n> I said: 'Frequent changes have a cost - context switching, incomplete code, team morale. Can we batch priority changes to weekly instead of daily?'\n>\n> **4. Build for flexibility:**\n> I started designing systems more modularly. My worker pool has 73 configurable flows - easy to add/remove without major refactoring.\n>\n> **Result:**\n> We moved to weekly priority reviews instead of ad-hoc changes. My modular design meant future changes were less disruptive.\"\n\n---\n\n# SECTION 2: PROJECT AND AMBIGUITY QUESTIONS (5 Questions)\n\n---\n\n## Q12: \"Tell me about a time when you faced ambiguity in the requirements of a project.\"\n\n> \"The fake follower detection project was entirely ambiguous.\n>\n> **The Ambiguous Requirements:**\n> - 'Detect fake followers' - but no definition of 'fake'\n> - 'High accuracy' - but no target percentage\n> - 'Support Indian languages' - but which ones? All 22?\n> - 'Scalable' - for how many followers?\n>\n> **How I Navigated:**\n>\n> **1. Define concrete sub-problems:**\n> Instead of 'detect fake', I defined:\n> - Identify non-Indian scripts\n> - Identify bot-like usernames\n> - Compare username vs display name\n>\n> **2. Propose and validate assumptions:**\n> I proposed: 'Let's support top 10 Indic scripts by user population. Good enough?'\n> Stakeholder agreed.\n>\n> **3. Create measurable targets:**\n> I said: '85% precision on a manually validated 500-account dataset?'\n> That became our success metric.\n>\n> **4. Build iteratively:**\n> Shipped v1 with 3 features. Got feedback. Added 2 more. Each iteration reduced ambiguity.\n>\n> **Result:**\n> Delivered working system. The process of resolving ambiguity became more valuable than the initial unclear requirement.\"\n\n---\n\n## Q13: \"Tell me about a time you had to get people on the same page about a decision.\"\n\n> \"Getting team alignment on using dbt over commercial ETL tools.\n>\n> **The Situation:**\n> Half the team wanted Fivetran (easy, commercial). Other half wanted custom scripts (familiar). I proposed dbt (open source, best of both).\n>\n> **How I Got Alignment:**\n>\n> **1. Understand each perspective:**\n> - Fivetran fans: 'We don't want to maintain ETL code'\n> - Custom script fans: 'We need flexibility for our use cases'\n>\n> **2. Build a POC addressing both concerns:**\n> Created 5 dbt models showing:\n> - Minimal maintenance (just SQL, no Python orchestration)\n> - Full flexibility (custom SQL, any transformation)\n>\n> **3. Present trade-offs objectively:**\n>\n> | Option | Cost | Flexibility | Maintenance |\n> |--------|------|-------------|-------------|\n> | Fivetran | $500/mo | Low | Zero |\n> | Custom | $0 | High | High |\n> | dbt | $0 | High | Low |\n>\n> **4. Offer risk mitigation:**\n> 'Let's try dbt for 2 weeks. If it doesn't work, we switch to Fivetran. I'll own the learning curve.'\n>\n> **Result:**\n> Team agreed to try dbt. It worked. Now we have 112 production models. Both camps are happy.\"\n\n---\n\n## Q14: \"How would you handle people who disagree with the majority decision on a non-work-related matter?\"\n\n> \"This is about inclusion and psychological safety.\n>\n> **My Approach:**\n>\n> **1. Acknowledge their view:**\n> 'I hear that you prefer X. That's a valid perspective.'\n>\n> **2. Ensure they feel heard:**\n> 'Before we move forward, is there anything about your concern we should consider?'\n>\n> **3. Separate disagreement from exclusion:**\n> If it's team lunch venue: 'We're going to Y this time, but let's do X next week.'\n> If it's team event: 'Not everyone has to participate in everything.'\n>\n> **4. Follow up privately:**\n> Check in later: 'Hey, I noticed you weren't thrilled about the decision. Everything okay?'\n>\n> **Real Example:**\n> Team wanted Friday evening team dinner. One colleague had family commitments. Instead of pressuring them, we:\n> - Did dinner for those who could come\n> - Did a team lunch the following week (everyone could attend)\n> - Made it clear: 'No pressure, family first'\n>\n> **Key Principle:**\n> Majority rules for logistics, but minority shouldn't feel excluded. Rotate who gets their preference.\"\n\n---\n\n## Q15: \"Tell me about a time you had to deal with last-minute changes in a project.\"\n\n> \"Two days before a major demo, product changed the leaderboard sorting logic.\n>\n> **The Change:**\n> Original: Sort by follower count\n> New requirement: Sort by engagement rate (likes + comments / followers)\n>\n> **The Challenge:**\n> - engagement_rate wasn't in our mart table\n> - Recalculating for 1M+ profiles would take hours\n> - Demo was in 48 hours\n>\n> **How I Handled:**\n>\n> **Hour 1-2: Assess impact**\n> - Identified which dbt models needed change\n> - Estimated: 4 hours to modify, 6 hours to backfill\n>\n> **Hour 3-4: Communicate clearly**\n> Told product: 'I can do this, but here are the risks:\n> - No time for thorough testing\n> - If backfill fails, we might miss demo\n> - Alternative: Show follower count now, engagement rate next week'\n>\n> **Decision:** They wanted engagement rate for demo. Okay, let's do it.\n>\n> **Hour 5-12: Execute**\n> - Modified dbt model to calculate engagement_rate\n> - Optimized query to run faster (used approximate percentiles)\n> - Started backfill overnight\n>\n> **Hour 13-20: Monitor and fix**\n> - Woke up at 3 AM to check backfill\n> - Found and fixed a division-by-zero edge case\n>\n> **Result:**\n> Demo happened with engagement rate sorting. Client was impressed. But I documented: 'Last-minute changes have hidden costs - let's build buffer into future timelines.'\"\n\n---\n\n## Q16: \"How would you prioritize tasks when facing multiple critical deadlines?\"\n\n> \"I use the Eisenhower matrix adapted for engineering:\n>\n> **My Framework:**\n>\n> | | URGENT | NOT URGENT |\n> |---|--------|------------|\n> | **IMPORTANT** | Production issues, Client commitments | Technical debt, Architecture improvements |\n> | **NOT IMPORTANT** | Meetings, Minor requests | Nice-to-haves |\n>\n> **Real Example - Three Critical Deadlines:**\n>\n> 1. **Production bug**: API returning 500 errors (due: NOW)\n> 2. **Client demo**: New feature (due: 3 days)\n> 3. **Sprint commitment**: Refactoring task (due: 5 days)\n>\n> **How I Prioritized:**\n>\n> **Day 1**: Production bug ONLY\n> - 500 errors affect all users\n> - Everything else can wait\n> - Fixed by EOD\n>\n> **Day 2-3**: Client demo\n> - External commitment, reputation at stake\n> - Communicated to team: 'I'm heads-down on demo'\n>\n> **Day 4-5**: Sprint commitment\n> - Internal deadline, can negotiate if needed\n> - Finished with half a day to spare\n>\n> **Communication Throughout:**\n> - Daily standup: 'Status on each deadline'\n> - Proactive escalation: 'If demo takes longer, sprint task might slip'\n>\n> **Key Principle:**\n> Production > External commitments > Internal commitments > Nice-to-haves\"\n\n---\n\n# SECTION 3: TECHNICAL AND ROLE-RELATED QUESTIONS (7 Questions)\n\n---\n\n## Q17: \"Imagine you're part of the Google Photos team, and your feature detects smiling faces in photos. How will you identify false positives and what actions will you take?\"\n\n> \"I'd approach this systematically:\n>\n> **Identifying False Positives:**\n>\n> **1. Define 'false positive' clearly:**\n> - FP Type 1: Non-smiling face marked as smiling\n> - FP Type 2: Non-face marked as smiling face\n> - FP Type 3: Smiling face but wrong person tagged\n>\n> **2. Build feedback loops:**\n> - User reports: 'This isn't a smile' button\n> - Implicit signals: User deletes auto-generated 'smiling moments' album\n> - Quality sampling: Human review of random predictions\n>\n> **3. Analyze patterns:**\n> - Which lighting conditions cause FPs?\n> - Which ethnicities have higher FP rates?\n> - Do certain expressions (grimace, squint) get misclassified?\n>\n> **Actions I'd Take:**\n>\n> **1. Immediate - Reduce user impact:**\n> - Add confidence threshold - only show high-confidence smiles\n> - Easy correction mechanism for users\n>\n> **2. Short-term - Improve model:**\n> - Collect FP examples as training data\n> - Retrain with hard negatives (grimaces, squints)\n> - Test across diverse demographics\n>\n> **3. Long-term - Prevent future FPs:**\n> - Build automated FP detection pipeline\n> - A/B test model changes before full rollout\n> - Monitor FP rate as key metric\n>\n> **My Experience:**\n> In fake follower detection, I faced similar issues. I built manual validation of 500 accounts to identify false positives, then adjusted scoring thresholds.\"\n\n---\n\n## Q18: \"Tell me about a time when you had to work on multiple projects simultaneously.\"\n\n> \"At Good Creator Co, I simultaneously worked on:\n>\n> 1. **beat** - Data scraping service (Python)\n> 2. **event-grpc** - Event consumer (Go)\n> 3. **stir** - Data platform (Airflow/dbt)\n>\n> **Challenge:**\n> Different tech stacks, different stakeholders, different timelines.\n>\n> **How I Managed:**\n>\n> **Context Switching Strategy:**\n> - Dedicated days for deep work: Monday/Wednesday for beat, Tuesday/Thursday for stir\n> - Fridays for event-grpc (smaller scope, less context needed)\n>\n> **Documentation:**\n> - Kept detailed notes on 'where I left off' for each project\n> - Before switching, wrote 'next steps' for future-me\n>\n> **Stakeholder Management:**\n> - Weekly updates to each project's stakeholders\n> - Clear about capacity: 'I'm 50% beat, 30% stir, 20% event-grpc this sprint'\n>\n> **Technical Synergies:**\n> - Beat and event-grpc are connected - fixing one often helped the other\n> - Used learnings from dbt to improve beat's data quality\n>\n> **Result:**\n> All three projects delivered successfully. beat handles 10M daily data points, event-grpc processes 10K events/sec, stir runs 76 DAGs.\"\n\n---\n\n## Q19: \"Give an example of a challenging technical problem you faced recently. How did you solve it, and what was the result?\"\n\n> \"Instagram API suddenly started returning incomplete audience demographics data.\n>\n> **The Problem:**\n> Instagram Audience Insights API returns age-gender breakdown. The percentages should add up to 100%, but we started seeing sums like 95% or 105%. We couldn't serve inconsistent data to customers.\n>\n> **Investigation:**\n> 1. Checked our parsing code - correct\n> 2. Compared raw API responses - the API itself returned inconsistent data\n> 3. Facebook documentation - no mention of this behavior\n>\n> **Solution Options:**\n>\n> | Option | Pros | Cons |\n> |--------|------|------|\n> | Simple scaling | Easy | Distorts proportions |\n> | Drop inconsistent data | Clean | Lose data |\n> | Mathematical normalization | Preserves proportions | Complex |\n>\n> **What I Built:**\n> Implemented gradient descent normalization:\n>\n> \\`\\`\\`python\n> def gradient_descent(values, target=100, lr=0.01, epochs=1000):\n>     for _ in range(epochs):\n>         error = target - sum(values)\n>         values = [v + lr * error / len(values) for v in values]\n>     return values\n> \\`\\`\\`\n>\n> This adjusts each value proportionally to converge to exactly 100%.\n>\n> **Result:**\n> - All audience demographics now sum to exactly 100%\n> - Relative proportions preserved\n> - No data loss\n> - Customers get consistent, accurate data\"\n\n---\n\n## Q20: \"How do you ensure the quality of your code and that of your team? Can you provide an example where your focus on quality made a difference?\"\n\n> \"I ensure quality through process, not just review.\n>\n> **My Quality Practices:**\n>\n> **1. Write tests first (when possible):**\n> For fake_follower, I wrote test cases before implementation:\n> - \\`test_greek_text_detected_as_fake()\\`\n> - \\`test_hindi_name_transliteration()\\`\n>\n> **2. Code review with context:**\n> When reviewing others' code, I don't just check syntax. I ask:\n> - 'What happens if this API fails?'\n> - 'How does this perform with 1M records?'\n>\n> **3. Design docs before code:**\n> For major features, I write a 1-page design doc. Gets reviewed before coding starts.\n>\n> **4. Monitoring as quality signal:**\n> If something passes review but fails in production, our process failed.\n>\n> **Example Where Quality Made a Difference:**\n>\n> When building the ClickHouse sync pipeline, I insisted on:\n> - Atomic table swap (not delete + insert)\n> - Retry logic at each step\n> - Rollback capability\n>\n> Team thought I was over-engineering. 'Just do simple COPY,' they said.\n>\n> Two months later, PostgreSQL had a disk issue during sync. My rollback logic kicked in, previous data was preserved. Without it, we'd have lost the leaderboard table.\n>\n> **Result:**\n> Zero data loss incidents in 15 months. The 'over-engineering' paid off.\"\n\n---\n\n## Q21: \"Describe a time when you encountered a significant bug or issue in production. How did you handle it?\"\n\n> \"Our Instagram scraping suddenly dropped by 80% - API returning 429 (rate limit) errors everywhere.\n>\n> **Discovery:**\n> 6 AM - Monitoring alert: 'Scraping success rate below 20%'\n> I checked immediately - thousands of 429 errors.\n>\n> **Immediate Actions (First 30 minutes):**\n>\n> 1. **Assess scope:** All credentials affected, not just one\n> 2. **Reduce bleeding:** Scaled workers from 50 to 10 immediately\n> 3. **Communicate:** Notified team: 'Investigating API rate limit issue'\n>\n> **Investigation (30 min - 2 hours):**\n>\n> - Checked our rate limiting code - working correctly\n> - Compared error patterns - started exactly at midnight UTC\n> - Hypothesis: Facebook silently reduced rate limits\n>\n> **Fix (2 hours - 4 hours):**\n>\n> Short-term:\n> - Reduced workers further to 5\n> - Implemented credential rotation more aggressively\n>\n> Medium-term:\n> - Added adaptive rate limiting that backs off on 429s\n> - Created dashboards for real-time rate limit monitoring\n>\n> **Communication Throughout:**\n> - 8 AM: 'Identified issue, implementing fix'\n> - 10 AM: 'Fix deployed, monitoring recovery'\n> - 2 PM: 'Scraping at 70% of normal, will be 100% by evening'\n>\n> **Post-Incident:**\n> - Wrote incident report\n> - Created runbook for similar issues\n> - Built alerting for rate limit changes\n>\n> **Result:**\n> Restored within 4 hours. Runbook has been used twice since for similar issues.\"\n\n---\n\n## Q22: \"Explain a situation where you misunderstood project requirements. How did you rectify it, and what did you learn?\"\n\n> \"I misunderstood the scope of 'follower analysis' feature.\n>\n> **The Misunderstanding:**\n> Requirement: 'Analyze follower quality for influencers'\n>\n> I understood: Analyze ALL followers of every influencer\n> Actual intent: Analyze a SAMPLE of followers (1000 per influencer)\n>\n> **How I Discovered:**\n> After building for 2 weeks, I showed progress: 'Pipeline can process 100K followers per influencer, takes 6 hours each.'\n>\n> Product manager: 'Why 6 hours? We just need a representative sample.'\n>\n> **Impact of Misunderstanding:**\n> - 2 weeks of over-engineered code\n> - Complex pagination logic that wasn't needed\n> - AWS costs for processing 100K vs 1000\n>\n> **How I Rectified:**\n>\n> 1. **Accepted responsibility:** 'I should have clarified scope upfront.'\n> 2. **Salvaged what I could:** Pagination logic was reusable for other features\n> 3. **Simplified:** Rewrote for 1000-follower sample (2 days)\n> 4. **Documented:** Added 'sample size' parameter for future flexibility\n>\n> **What I Learned:**\n>\n> 1. **Ask 'how much' not just 'what':** Requirements about scale matter\n> 2. **Show progress early:** If I'd shown at 1 week, caught earlier\n> 3. **Design for flexibility:** Now I always parameterize quantities\n>\n> **Process Change:**\n> Before starting any feature, I now write a 'scope checklist':\n> - What data?\n> - How much data?\n> - For how many entities?\n> - How fresh must it be?\"\n\n---\n\n## Q23: \"Give an example of a time you had to quickly learn a new technology or tool. How did you approach it, and what was the outcome?\"\n\n> \"Learning Go to build event-grpc in 3 weeks.\n>\n> **The Context:**\n> Our event consumer needed to handle 10,000+ events/sec. Python wasn't cutting it. Team decided: build in Go.\n>\n> Problem: I'd never written Go before.\n>\n> **My Learning Approach:**\n>\n> **Week 1 - Fundamentals:**\n> - Completed 'Tour of Go' (official tutorial)\n> - Read 'Go by Example' for practical patterns\n> - Built toy projects: HTTP server, JSON parsing\n> - Key concepts: goroutines, channels, defer\n>\n> **Week 2 - Applied Learning:**\n> - Read existing Go code in our codebase\n> - Built first consumer: simple message → log\n> - Made mistakes: forgot to close channels, goroutine leaks\n> - Each bug taught me something\n>\n> **Week 3 - Production Code:**\n> - Built buffered sinker pattern\n> - Implemented connection auto-recovery\n> - Code review from Go-experienced colleague\n> - Deployed to production (with careful monitoring)\n>\n> **Key Learning Strategies:**\n> 1. **Learn by doing:** Not courses, but actual code\n> 2. **Read good code:** Our existing Go services were my teachers\n> 3. **Fail fast:** Made mistakes in dev, not production\n> 4. **Teach back:** Documented what I learned for future team members\n>\n> **Outcome:**\n> - event-grpc handles 10K+ events/sec reliably\n> - Go became one of my working languages\n> - I now onboard new engineers on Go basics\"\n\n---\n\n# SECTION 4: MENTORING AND LEADERSHIP QUESTIONS (5 Questions)\n\n---\n\n## Q24: \"Tell me about a time you advocated for yourself or someone on your team.\"\n\n> \"I advocated for a junior engineer who was being assigned only bug fixes.\n>\n> **The Situation:**\n> A junior engineer had been with us 6 months. He was only getting bug fix tickets, never feature work. He was frustrated and mentioned considering other opportunities.\n>\n> **What I Did:**\n>\n> **1. Gathered Data:**\n> - Looked at his last 3 months of tickets - 90% bugs, 10% small features\n> - Compared with other juniors - they had more feature work\n>\n> **2. Talked to Him First:**\n> 'I noticed you're getting mostly bug fixes. Is that something you want to change?'\n> He said yes, he wanted to grow but felt stuck.\n>\n> **3. Advocated to Manager:**\n> In sprint planning, I said: 'Can we assign the new API endpoint to [junior engineer]? He's shown good debugging skills, and this would stretch him.'\n>\n> Manager's concern: 'Will he deliver on time?'\n>\n> My response: 'I'll pair with him on design. If he gets stuck, I'll unblock him. The risk is manageable.'\n>\n> **4. Supported Him:**\n> - Reviewed his design doc\n> - Answered questions without taking over\n> - Celebrated his successful delivery\n>\n> **Result:**\n> He delivered the feature. Got more feature work after that. He's now one of our stronger mid-level engineers.\n>\n> **Why I Advocated:**\n> I remembered being a junior who wanted more responsibility. Someone advocated for me once. Paying it forward.\"\n\n---\n\n## Q25: \"Describe a situation where you helped an underperforming team member improve.\"\n\n> \"A team member was consistently missing sprint commitments and writing buggy code.\n>\n> **The Situation:**\n> Over 3 sprints, this engineer delivered late twice and had multiple bugs caught in review. Team was getting frustrated.\n>\n> **What I Did:**\n>\n> **1. Understood the Root Cause:**\n> Had a 1:1 coffee chat (not formal meeting). Asked: 'How are things going? Anything blocking you?'\n>\n> Turned out: He was overwhelmed. New to async Python (we use it heavily), didn't want to ask 'stupid questions.'\n>\n> **2. Created Safe Learning Environment:**\n> - Told him: 'Asking questions is how you learn. I asked tons of questions when I started.'\n> - Set up daily 15-min sync: 'Show me what you're working on, any blockers?'\n>\n> **3. Paired on Complex Tasks:**\n> - For async code, I wrote first version while explaining\n> - He wrote second version while I watched\n> - Third version: he wrote, I reviewed\n>\n> **4. Adjusted Expectations Temporarily:**\n> - Talked to manager: 'He needs 2-3 weeks of ramping. Let's give smaller tasks.'\n> - Manager agreed to lighter sprint load temporarily\n>\n> **Result:**\n> After 4 weeks:\n> - His code quality improved significantly\n> - He started asking questions proactively\n> - Delivered on time\n> - He later helped another new engineer with async Python\n>\n> **Key Learning:**\n> Underperformance often has a reason. Find it before judging.\"\n\n---\n\n## Q26: \"How do you mentor junior team members? Can you share a successful mentoring experience?\"\n\n> \"My mentoring philosophy: Teach process, not just solutions.\n>\n> **My Approach:**\n>\n> **1. Don't just fix their code:**\n> When a junior shows me buggy code, I don't fix it. I ask:\n> - 'Walk me through what this code does'\n> - 'What happens when this input is null?'\n> - 'How would you test this?'\n>\n> **2. Make them write the fix:**\n> After discussing, THEY implement the fix. I watch. They remember better.\n>\n> **3. Share context, not just answers:**\n> Instead of 'use asyncio.gather', I explain: 'We want concurrent execution because these API calls are independent...'\n>\n> **Successful Experience:**\n>\n> A junior engineer was stuck on Airflow DAG debugging for 2 days.\n>\n> **What I Didn't Do:**\n> - Jump in and fix it\n> - Take over the task\n>\n> **What I Did:**\n> Sat with them for 1 hour. Walked through systematic debugging:\n>\n> 1. 'Let's check scheduler logs first'\n> 2. 'Which specific task failed?'\n> 3. 'What's in that task's code?'\n> 4. 'Let's trace the database connection'\n>\n> We found it together: ClickHouse timeout on heavy query.\n>\n> **The Lasting Impact:**\n> - They wrote a 'DAG Debugging Guide' based on our session\n> - They now debug independently\n> - They've taught the same process to newer engineers\n>\n> **Result:**\n> Team escalations reduced by 40% because juniors could self-serve.\"\n\n---\n\n## Q27: \"What challenges have you faced when mentoring junior colleagues?\"\n\n> \"Two main challenges:\n>\n> **Challenge 1: Different Learning Speeds**\n>\n> I mentored two juniors simultaneously. One picked up concepts quickly, asked sharp questions. Another needed more time, repeated explanations.\n>\n> **How I Handled:**\n> - Adapted my style: Quick explanations for fast learner, step-by-step walkthroughs for slower one\n> - Realized: 'Slow' isn't 'bad'. The slower learner wrote more careful, bug-free code\n> - Set different expectations: Fast learner got complex tasks, other got foundational ones\n>\n> **Challenge 2: Knowing When to Step Back**\n>\n> I have a tendency to over-explain. Sometimes juniors need to struggle a bit to learn.\n>\n> **Situation:**\n> A junior was implementing rate limiting. I knew exactly how to do it. Wanted to just tell them.\n>\n> **What I Did Instead:**\n> - Gave them 2 hours to try on their own\n> - They came back with a working (but inefficient) solution\n> - We discussed trade-offs together\n> - They refactored based on discussion\n>\n> **Result:**\n> They understood rate limiting deeply because they discovered the pitfalls themselves.\n>\n> **Key Lesson:**\n> Productive struggle > spoon-feeding. My job is to guide, not do.\"\n\n---\n\n## Q28: \"What would you do if a junior team member was not working properly and delaying tasks?\"\n\n> \"I'd approach with curiosity before judgment.\n>\n> **Step 1: Gather Facts**\n> - Which tasks are delayed? By how much?\n> - Is this new or ongoing?\n> - What's their workload like?\n>\n> **Step 2: Private Conversation**\n> Not a confrontation. A genuine check-in:\n> - 'I noticed task X is behind schedule. What's going on?'\n> - Listen actively. Maybe they're struggling with something\n> - Maybe personal issues, maybe unclear requirements, maybe skill gap\n>\n> **Step 3: Identify Root Cause**\n>\n> | Root Cause | My Action |\n> |------------|-----------|\n> | Skill gap | Training, pairing, simpler tasks |\n> | Unclear requirements | Clarify together, document |\n> | Personal issues | Empathy, flexible deadlines |\n> | Motivation | Understand why, find engaging work |\n> | Just not working | Clear expectations, consequences |\n>\n> **Step 4: Create Action Plan**\n> - Specific milestones with check-ins\n> - 'Let's sync daily for 15 mins until this is on track'\n> - Not micromanaging, but supporting\n>\n> **Step 5: Escalate If Needed**\n> If no improvement after 2-3 weeks of support, involve manager. But document what you tried.\n>\n> **Real Example:**\n> I had a junior who was delaying tasks. Turned out: he didn't understand async Python and was embarrassed to ask. Once we identified that, we solved it together.\"\n\n---\n\n# SECTION 5: TEAM DYNAMICS AND CONFLICT RESOLUTION (7 Questions)\n\n---\n\n## Q29: \"Describe a time when you had to work with someone outside your team.\"\n\n> \"I worked closely with the Identity team to build credential management for beat.\n>\n> **The Context:**\n> Beat needed to manage Instagram/YouTube API credentials. Identity team owned authentication. I needed their tokens, they needed our credential rotation feedback.\n>\n> **Challenges:**\n> - Different priorities: They had their roadmap, I had mine\n> - Different timelines: I needed the feature in 2 weeks, they had 4-week sprint\n> - Different tech stacks: They used Node.js, I used Python\n>\n> **How I Made It Work:**\n>\n> **1. Found Common Ground:**\n> Met with their tech lead. Understood their constraints. Found overlap: they also wanted better token refresh handling.\n>\n> **2. Proposed Win-Win:**\n> 'What if I build the credential rotation logic, and you provide the token refresh API? We both get what we need.'\n>\n> **3. Clear Interface:**\n> Documented the API contract:\n> - They call \\`/credentials/disable\\` when token expires\n> - I call \\`/tokens/refresh\\` when needed\n>\n> **4. Regular Syncs:**\n> 15-min sync twice a week until integration was done.\n>\n> **5. Celebrated Together:**\n> When it worked, acknowledged their contribution: 'Thanks to Identity team for the token API.'\n>\n> **Result:**\n> Credential management system working smoothly. We've collaborated on 3 more integrations since. Built a good cross-team relationship.\"\n\n---\n\n## Q30: \"Tell me about a situation where you had a conflict with a colleague and how you resolved it.\"\n\n> *(Same as Q8 - MongoDB vs ClickHouse story)*\n>\n> \"Technical disagreement with a senior engineer about database choice. Used data-driven benchmarking to resolve. Let the data decide, not opinions.\"\n\n---\n\n## Q31: \"What would you do if your team was not bonding well?\"\n\n> \"I'd diagnose before prescribing.\n>\n> **Diagnosis:**\n>\n> 1. **Observe:** Is it everyone or specific people? All the time or certain situations?\n> 2. **Talk:** 1:1 chats to understand individual perspectives\n> 3. **Identify patterns:** Is it remote vs office? New vs old members?\n>\n> **Common Causes and Actions:**\n>\n> | Cause | Action |\n> |-------|--------|\n> | Remote isolation | Regular video calls, virtual coffee |\n> | New members feel excluded | Buddy system, structured onboarding |\n> | Conflict between people | Mediate, clear the air |\n> | All work, no fun | Team activities, non-work chat |\n>\n> **What I'd Actually Do:**\n>\n> **1. Create low-pressure interaction opportunities:**\n> - Start standups with 'One good thing from yesterday' (2 mins)\n> - Monthly team lunch (in-person or virtual)\n> - Slack channel for non-work chat\n>\n> **2. Pair people on tasks:**\n> Collaboration builds relationships. Assign cross-functional pairs.\n>\n> **3. Celebrate together:**\n> Launch celebration, birthday acknowledgments, sprint completion\n>\n> **4. Address conflicts directly:**\n> If two people don't get along, talk to each separately, then together.\n>\n> **Follow-up: If I were team lead?**\n> Same actions, but also:\n> - Model behavior: I'd initiate casual conversations\n> - Make bonding part of culture, not extra\n> - Address toxic behavior immediately\"\n\n---\n\n## Q32: \"Tell me about a situation where you proposed an idea, but your team disagreed. How did you handle it?\"\n\n> \"I proposed using Redis Streams for our task queue, but the team wanted to stick with PostgreSQL.\n>\n> **My Proposal:**\n> Replace our SQL-based task queue with Redis Streams for better throughput.\n>\n> **Team's Disagreement:**\n> - 'PostgreSQL works fine, why add Redis?'\n> - 'More infrastructure to maintain'\n> - 'Learning curve for the team'\n>\n> **How I Handled:**\n>\n> **1. Listened to understand, not to respond:**\n> Asked: 'What are your main concerns?' Genuinely understood their points.\n>\n> **2. Validated their concerns:**\n> They were right - more infrastructure IS more complexity.\n>\n> **3. Presented data:**\n> Our PostgreSQL queue handled 1000 tasks/sec. That was sufficient for current load.\n>\n> **4. Accepted the decision:**\n> Said: 'You're right that it adds complexity without immediate need. Let's revisit when we hit scaling issues.'\n>\n> **5. Documented for future:**\n> Wrote a design doc: 'Redis Streams Migration Plan' for when we need it.\n>\n> **Result:**\n> We stayed with PostgreSQL. Six months later, still working fine. If we need to scale, the plan is ready.\n>\n> **Key Learning:**\n> Not every good idea needs to be implemented now. Timing matters. Team alignment matters more than being right.\"\n\n---\n\n## Q33: \"Have you worked with cross-team members? Can you describe your experience and how it went?\"\n\n> *(Expand on Q29)*\n>\n> \"Yes, extensively. At Good Creator Co, beat interacted with:\n>\n> **1. Identity Team (Node.js)**\n> - Credential management integration\n> - Weekly syncs, clear API contracts\n> - Result: Smooth token refresh system\n>\n> **2. Coffee Team (Go)**\n> - beat provides scraping API to coffee\n> - Documented endpoints, SLA agreements\n> - Result: Reliable real-time profile lookups\n>\n> **3. Data Science Team**\n> - They consume our ClickHouse data\n> - Bi-weekly meetings on data quality\n> - Result: Clean data, happy data scientists\n>\n> **What Made It Work:**\n>\n> 1. **Clear interfaces:** Document what you provide, what you expect\n> 2. **Regular communication:** Short syncs > long meetings\n> 3. **Empathy:** Understand their priorities, not just yours\n> 4. **Escalation path:** Know who to talk to when stuck\n>\n> **Challenge:**\n> Different sprint cycles. Identity was 4-week sprints, we were 2-week. Coordination required flexibility.\"\n\n---\n\n## Q34: \"Describe a time when you handled a colleague who was difficult to work with.\"\n\n> \"A colleague was dismissive of my suggestions in code reviews.\n>\n> **The Situation:**\n> Whenever I commented on his PRs, he'd respond with 'It's fine' or 'That's not important' without engaging. Made me feel my input wasn't valued.\n>\n> **How I Handled:**\n>\n> **1. Didn't react publicly:**\n> Didn't argue in PR comments. That would escalate.\n>\n> **2. Had private conversation:**\n> 'Hey, I've noticed my review comments often get dismissed. Is there something I'm missing about the context?'\n>\n> **3. Listened:**\n> Turned out: He felt I was being nitpicky on things that didn't matter. He was under pressure to ship.\n>\n> **4. Found middle ground:**\n> I said: 'I'll focus my reviews on critical issues only. For style stuff, I'll just note it, not block on it.'\n> He said: 'That would help. And I'll be more open to discussion on critical stuff.'\n>\n> **5. Rebuilt relationship:**\n> Started having coffee chats. Understood his work better. My reviews became more relevant.\n>\n> **Result:**\n> Our code review interactions improved. He actually started asking for my input on design decisions.\n>\n> **Key Learning:**\n> 'Difficult' often has context. Understand before judging.\"\n\n---\n\n# SECTION 6: GOAL SETTING AND MANAGER EXPECTATIONS (5 Questions)\n\n---\n\n## Q35: \"What is your idea of a perfect manager? Would you be the type of manager you described?\"\n\n> \"My ideal manager: **Context giver, not controller.**\n>\n> **Qualities I Value:**\n>\n> | Quality | What It Looks Like |\n> |---------|-------------------|\n> | Gives context | Explains WHY, not just WHAT |\n> | Trusts | Lets me choose HOW to achieve |\n> | Available | Unblocks when needed, doesn't hover |\n> | Direct feedback | Tells me what to improve, specifically |\n> | Celebrates team | Gives credit to team, takes blame themselves |\n>\n> **Would I Be This Manager?**\n>\n> I try to be, even without the title:\n>\n> - When I mentor juniors, I explain 'why' before 'what'\n> - I give them ownership of implementation\n> - I'm available for questions but don't check in hourly\n> - I give specific, actionable feedback\n> - When our team succeeds, I highlight individual contributions\n>\n> **Where I'd Need to Grow:**\n>\n> - I tend to over-explain. As a manager, I'd need to know when to step back.\n> - I'd need to improve at handling performance issues. Currently I'm better at helping people succeed than addressing underperformance.\n>\n> **Yes, I aspire to be the manager I described.** But I'm self-aware about gaps.\"\n\n---\n\n## Q36: \"Tell me about a situation where you worked outside your role definition or responsibilities.\"\n\n> \"I took ownership of production monitoring even though it wasn't my job.\n>\n> **The Situation:**\n> Our team didn't have dedicated DevOps. Developers deployed, but nobody owned monitoring. When things broke at night, we'd find out from customers.\n>\n> **What I Did:**\n>\n> **1. Identified the gap:**\n> 'We have no alerting. We learn about outages from complaints.'\n>\n> **2. Stepped up:**\n> Even though I was hired as a backend engineer, I:\n> - Set up Grafana dashboards for beat\n> - Created PagerDuty alerts for critical metrics\n> - Wrote runbooks for common issues\n>\n> **3. Made it sustainable:**\n> Documented everything so others could maintain it\n> Trained team on alert response\n>\n> **Why I Did It:**\n> - The problem was hurting our customers\n> - Waiting for a 'DevOps hire' could take months\n> - I had enough knowledge to do it\n>\n> **Result:**\n> - MTTR (mean time to recovery) improved from hours to minutes\n> - We caught issues before customers did\n> - Eventually, monitoring became everyone's responsibility\n>\n> **Key Learning:**\n> Role definitions are starting points, not boundaries. If you see a gap and can fill it, do it.\"\n\n---\n\n## Q37: \"Tell me about a situation where you learned something valuable from a colleague.\"\n\n> \"I learned about 'boring technology' philosophy from a senior engineer.\n>\n> **The Context:**\n> I wanted to use Kafka for our event processing. It was the 'cool' choice. The senior suggested RabbitMQ instead.\n>\n> **What He Taught Me:**\n>\n> 'Choose boring technology when possible. Kafka is powerful, but:\n> - Do we need its scale? (No, RabbitMQ handles our load)\n> - Do we have Kafka expertise? (No)\n> - Is Kafka's complexity worth it for our use case? (No)\n>\n> RabbitMQ is boring, well-understood, and sufficient. Save complexity budget for where you need it.'\n>\n> **How It Changed My Thinking:**\n>\n> Before: 'What's the most powerful tool?'\n> After: 'What's the simplest tool that solves the problem?'\n>\n> I've since applied this:\n> - Used PostgreSQL's FOR UPDATE SKIP LOCKED instead of adding Redis for task queue\n> - Used simple Python multiprocessing instead of Celery\n> - Used dbt (SQL) instead of custom Python transformations\n>\n> **The Lesson:**\n> Every technology has complexity cost. Only pay it when benefits outweigh costs.\"\n\n---\n\n## Q38: \"Tell me about a time when your work was deprioritized mid-way through a project. How did you handle the situation?\"\n\n> \"Instagram Stories analytics was deprioritized after 2 weeks of work.\n>\n> **The Situation:**\n> I was building Stories analytics - parsing story data, storing metrics, showing trends. Two weeks in, priorities shifted to YouTube Shorts for a major client.\n>\n> **My Initial Reaction:**\n> Frustrated. I'd invested time understanding Stories API, written initial code, got excited about the feature.\n>\n> **How I Handled:**\n>\n> **1. Understood the business reason:**\n> Asked: 'Why the shift?' Major client specifically wanted YouTube. Business decision made sense.\n>\n> **2. Documented my progress:**\n> Wrote detailed notes on Stories implementation so far. If we return to it, no knowledge lost.\n>\n> **3. Extracted reusable work:**\n> Some code was reusable - parsing logic, storage patterns. Refactored into shared components.\n>\n> **4. Committed fully to new priority:**\n> Didn't half-heartedly work on YouTube while mourning Stories. Full focus on new priority.\n>\n> **5. Communicated impact:**\n> Told manager: 'I understand the shift. FYI, 2 weeks of work is paused. Let's try to batch priority changes in future.'\n>\n> **Result:**\n> YouTube Shorts shipped successfully. Stories was eventually built 6 months later, using my notes and reusable code.\n>\n> **Follow-up: If I were team lead?**\n> I'd also:\n> - Shield team from too-frequent shifts\n> - Push back on changes unless truly necessary\n> - Create 'cool down' periods between priority changes\"\n\n---\n\n## Q39: \"What generally excites you? What areas of work would you like to explore?\"\n\n> \"I get excited about **systems that process data at scale**.\n>\n> **What Excites Me:**\n>\n> **1. Distributed Systems:**\n> How do you process 10M events reliably? How do you handle node failures? How do you maintain consistency?\n>\n> I built this with event-grpc - buffered sinkers, connection auto-recovery, zero data loss.\n>\n> **2. Data Pipelines:**\n> Taking messy real-world data and transforming it into insights. The puzzle of handling edge cases, late-arriving data, schema evolution.\n>\n> I explored this with stir - 112 dbt models, incremental processing, cross-database sync.\n>\n> **3. ML in Production:**\n> Not just training models, but deploying them reliably. How do you handle inference at scale?\n>\n> I touched this with fake_follower - ML models on Lambda, batch processing, cost optimization.\n>\n> **What I'd Like to Explore at Google:**\n>\n> **1. Larger scale:**\n> I've done 10M data points/day. Google does billions. I want to learn what changes at that scale.\n>\n> **2. Internal tooling:**\n> Tools like Spanner, BigQuery, Borg - how they're built, not just used.\n>\n> **3. Cross-functional impact:**\n> Working on infrastructure that many teams depend on. Multiplier effect.\n>\n> **What keeps me learning:**\n> Every scale brings new problems. I haven't stopped learning in 3 years, don't expect to stop at Google.\"\n\n---\n\n# SECTION 7: CLIENT, DEADLINE, AND PROCESS IMPROVEMENT (5 Questions)\n\n---\n\n## Q40: \"What would you do if you were going to miss a project deadline?\"\n\n> \"Three principles: **Communicate early, explain clearly, propose alternatives.**\n>\n> **What I'd Do:**\n>\n> **1. Communicate EARLY:**\n> The moment I see risk - not on deadline day. If deadline is Friday and I see trouble on Tuesday, I say something Tuesday.\n>\n> **2. Explain the 'why':**\n> Not 'it's taking longer' but specific: 'I found edge cases that could cause data corruption if not handled. Fixing them properly needs 3 more days.'\n>\n> **3. Propose options:**\n> 'Option A: Ship on time with known risk. Option B: 3 more days for complete solution. Option C: Ship partial feature on time, complete later.'\n>\n> **Real Example:**\n>\n> Building ClickHouse sync pipeline. Committed to 2 weeks. At day 10, found edge cases in atomic table swap.\n>\n> **What I did:**\n> - Day 10: Told manager 'I might need 1 more week'\n> - Explained: 'If swap fails mid-way, we could lose data. I need to build rollback.'\n> - Proposed: 'I can ship risky version Friday, or safe version next Friday'\n> - Decision: Safe version chosen\n>\n> **Result:**\n> Delivered 1 week late, but pipeline has had zero data loss incidents in 15 months.\n>\n> **Key Learning:**\n> Deadlines are important, but data integrity is more important. Communicate trade-offs, let stakeholders decide.\"\n\n---\n\n## Q41: \"Suppose you are a product manager. After receiving all necessary approvals, a friend suggests a helpful change to your project. What do you do?\"\n\n> \"I'd evaluate the change, not automatically reject it.\n>\n> **My Thought Process:**\n>\n> **1. Assess the change:**\n> - Is it truly helpful or just 'nice to have'?\n> - What's the scope? Minor tweak or major shift?\n> - What's the risk of including it?\n>\n> **2. Consider the cost of change:**\n> - We have approvals already. Reopening means delay.\n> - Stakeholders might lose trust if scope keeps changing.\n> - Team might feel frustrated with moving goalposts.\n>\n> **3. Decision Matrix:**\n>\n> | Change Type | Scope | Action |\n> |-------------|-------|--------|\n> | Critical bug/risk | Any | Include, communicate |\n> | High value, small scope | <1 day work | Evaluate including |\n> | Nice to have | Any | Defer to v2 |\n> | Large change | >2 days | Definitely defer |\n>\n> **What I'd Actually Do:**\n>\n> 1. Thank my friend for the suggestion\n> 2. Evaluate objectively (not just because friend suggested)\n> 3. If small and valuable: Include, inform stakeholders of minor scope change\n> 4. If large: 'Great idea, let's include in v2'\n> 5. Document for future reference\n>\n> **Key Principle:**\n> Approvals aren't sacred, but process matters. Small, valuable changes can be accommodated. Large changes need re-approval cycle.\"\n\n---\n\n## Q42: \"Describe a scenario where you improved a process or system within your team. What impact did it have?\"\n\n> \"I improved our code review process which was causing delays.\n>\n> **The Problem:**\n> PRs were sitting in review for 3-4 days. Developers were blocked, frustration was high. Code was going stale.\n>\n> **Root Cause Analysis:**\n> - No clear ownership of reviews\n> - Big PRs taking too long\n> - No SLA on review turnaround\n>\n> **What I Did:**\n>\n> **1. Created review guidelines:**\n> - PRs should be <400 lines\n> - Larger changes need design doc first\n> - Break big features into smaller PRs\n>\n> **2. Established SLA:**\n> - First review within 24 hours\n> - If can't review, comment 'will review by [time]'\n>\n> **3. Rotated reviewer assignment:**\n> - Each day, one person is 'primary reviewer'\n> - They prioritize reviews over their own coding\n>\n> **4. Led by example:**\n> - I started reviewing within hours\n> - Gave specific, actionable feedback\n> - Approved with comments (not blocking on minor stuff)\n>\n> **Impact:**\n>\n> | Metric | Before | After |\n> |--------|--------|-------|\n> | Avg review time | 3.5 days | 1 day |\n> | PRs merged/week | 8 | 15 |\n> | Developer frustration | High | Low |\n>\n> **Key Learning:**\n> Process improvements often need one person to champion and model the behavior.\"\n\n---\n\n## Q43: \"Imagine working on a project with a strict deadline. How would you approach the situation?\"\n\n> \"I'd focus on **scope, communication, and execution**.\n>\n> **My Approach:**\n>\n> **1. Ruthless Scoping:**\n> What's the MVP? What can be cut?\n>\n> Example: For a client demo, I needed leaderboard in 2 weeks. Original scope: 15 tables synced. MVP scope: Just leaderboard table. Cut to what demo actually needs.\n>\n> **2. Break Into Milestones:**\n> Day 1-3: Core functionality\n> Day 4-7: Edge cases\n> Day 8-10: Testing\n> Day 11-14: Buffer\n>\n> **3. Daily Progress Checks:**\n> Am I on track? If not, escalate early.\n>\n> **4. Protect Focus Time:**\n> Block calendar for deep work. Decline non-essential meetings. 'I'm heads-down on deadline.'\n>\n> **5. Have a Plan B:**\n> If things go wrong, what's the fallback? Partial feature? Extended demo date?\n>\n> **Real Example:**\n>\n> Two-week deadline for sync pipeline:\n> - Day 1: Scoped to 1 table instead of 15\n> - Day 3-8: Built core with proper error handling\n> - Day 9-12: Testing, found and fixed edge cases\n> - Day 13-14: Buffer (used for documentation)\n>\n> **Result:**\n> Delivered on time, production-quality.\n>\n> **Key Principle:**\n> Strict deadlines require strict scoping. Don't try to do everything - do the most important thing well.\"\n\n---\n\n## Q44: \"What is the most challenging project you've worked on?\"\n\n> \"Building the complete event-driven architecture across beat, event-grpc, stir, and coffee.\n>\n> **Why It Was Challenging:**\n>\n> **1. Scope:**\n> Not one service - FOUR services with different tech stacks (Python, Go, Airflow/dbt, Go).\n>\n> **2. Technical Complexity:**\n> - Event publishing from Python\n> - High-throughput consumption in Go\n> - Transformation in SQL/dbt\n> - Sync to different databases\n>\n> **3. No Rollback:**\n> Changing from direct DB writes to event-driven. Couldn't easily undo if it failed.\n>\n> **4. Learning Curve:**\n> Had to learn Go and dbt specifically for this project.\n>\n> **5. Cross-Team Coordination:**\n> coffee team depended on my changes. Identity team's events flowed through my system.\n>\n> **How I Managed:**\n>\n> **1. Phased approach:**\n> - Phase 1: Non-critical events only\n> - Phase 2: Main profile events\n> - Phase 3: All events\n>\n> **2. Feature flags:**\n> Could switch between old (direct DB) and new (event-driven) per event type.\n>\n> **3. Monitoring:**\n> Built dashboards to compare old vs new data. Any discrepancy = alert.\n>\n> **4. Documentation:**\n> 50-page system design doc for future maintainers.\n>\n> **Result:**\n> - 2.5x faster log retrieval\n> - 10K events/sec capacity\n> - Zero data loss\n> - Running 15+ months\"\n\n---\n\n# SECTION 8: MISCELLANEOUS AND LIFE EXPERIENCE (6 Questions)\n\n---\n\n## Q45: \"Describe a time when you solved a customer pain point.\"\n\n> \"Brands couldn't trust influencer follower counts - solved with fake follower detection.\n>\n> **The Pain Point:**\n> Brands were spending money on influencer marketing but getting poor ROI. Reason: many influencers had fake followers. Brands had no way to verify.\n>\n> **What I Did:**\n>\n> **1. Understood the real problem:**\n> Talked to sales team. Brands weren't just asking 'how many followers' but 'how many REAL followers.'\n>\n> **2. Built the solution:**\n> Fake follower detection system:\n> - Analyzes follower characteristics\n> - Supports 10 Indian languages\n> - Returns confidence score\n>\n> **3. Made it actionable:**\n> Didn't just say 'fake score = 0.7'. Provided:\n> - Clear labels: 'Low Quality', 'Medium Quality', 'High Quality'\n> - Breakdown of why: '20% have bot-like usernames'\n> - Comparison with similar influencers\n>\n> **4. Integrated into workflow:**\n> Brands could filter influencer search by follower quality. Built into their decision process.\n>\n> **Result:**\n> - Brands reported better campaign ROI\n> - Sales team had unique selling point\n> - Influencers with real followers got more deals (good for ecosystem)\n>\n> **Key Learning:**\n> Customer pain point → technical solution → actionable output. Don't just build, make it useful.\"\n\n---\n\n## Q46: \"What is the biggest hurdle you have faced in life? Why was it significant, and how did it affect you?\"\n\n> \"Transitioning from non-tech background to software engineering.\n>\n> **The Hurdle:**\n> I didn't have a traditional CS background. When I started, I didn't know data structures, algorithms, or system design. Felt imposter syndrome constantly.\n>\n> **Why It Was Significant:**\n> - Everyone around me seemed to 'just know' things I struggled with\n> - Had to learn while delivering at work\n> - Constant fear of being 'found out'\n>\n> **How I Overcame It:**\n>\n> **1. Accepted the gap:**\n> Instead of pretending, I acknowledged: 'I don't know this, teach me.'\n>\n> **2. Structured learning:**\n> - 2 hours daily on fundamentals\n> - Applied immediately at work\n> - Asked questions without shame\n>\n> **3. Reframed the narrative:**\n> 'I'm behind' became 'I'm learning fast.' Everyone has gaps.\n>\n> **4. Proved through work:**\n> Best antidote to imposter syndrome is shipping real code that works.\n>\n> **How It Affected Me:**\n>\n> **Positively:**\n> - I'm empathetic to people who are learning\n> - I don't assume knowledge in others\n> - I document extensively (because I remember needing it)\n>\n> **The Learning:**\n> Background doesn't define capability. Consistent effort does.\"\n\n---\n\n## Q47: \"Why are you leaving your current organization?\"\n\n> \"I'm looking for scale and learning opportunities.\n>\n> **What Good Creator Co Gave Me:**\n> - Ownership: Built complete systems from scratch\n> - Breadth: Python, Go, Airflow, dbt, ML\n> - Impact: 10M+ data points daily\n>\n> **Why I'm Looking to Move:**\n>\n> **1. Scale:**\n> I've built systems for millions. Google operates at billions. I want to learn what changes at that scale.\n>\n> **2. Learning from the best:**\n> At a startup, I'm often the most experienced on a technology. At Google, I'll learn from engineers who've built Spanner, BigQuery, YouTube.\n>\n> **3. Infrastructure focus:**\n> I enjoyed building beat and event-grpc most - foundational systems. Google has incredible infrastructure teams.\n>\n> **What I'm NOT saying:**\n> - I'm not running away from problems\n> - Current team is great\n> - Just ready for next challenge\n>\n> **I'm leaving because I'm ready to grow, not because something is wrong.**\"\n\n---\n\n## Q48: \"Have you encountered unreasonable tasks from your manager? How did you handle them?\"\n\n> \"Once asked to integrate a new API in 2 days including production deployment.\n>\n> **Why It Was Unreasonable:**\n> - New API = unknown rate limits, error handling\n> - Production deployment needs testing\n> - 2 days was for building, not testing and deploying\n>\n> **How I Handled:**\n>\n> **1. Didn't just say 'no':**\n> That's not constructive.\n>\n> **2. Clarified the ask:**\n> 'Do you need it production-ready in 2 days, or a working demo?'\n>\n> **3. Explained trade-offs:**\n> 'In 2 days, I can:\n> - Option A: Production-ready integration (not possible)\n> - Option B: Working integration behind feature flag (possible)\n> - Option C: Demo with hardcoded data (definitely possible)'\n>\n> **4. Proposed alternatives:**\n> 'For production-ready, I need 5 days. Would 5 days work, or is 2 days hard deadline?'\n>\n> **5. Committed to what I agreed:**\n> We chose Option B. I delivered working integration behind feature flag in 2 days. Hardened it over the next 3 days.\n>\n> **Key Learning:**\n> 'Unreasonable' often means unclear expectations. Clarify scope, propose options, deliver what you commit.\"\n\n---\n\n## Q49: \"Describe a time when you had to make last-minute changes to your code. How did you feel about it?\"\n\n> \"Two days before demo, product changed leaderboard sorting from followers to engagement rate.\n>\n> **The Change:**\n> Significant - engagement_rate wasn't even in our data model. Had to calculate from likes, comments, followers.\n>\n> **How I Felt:**\n> - Initially: Stressed. This was risky.\n> - After thinking: Challenge accepted. Let's make it work.\n>\n> **What I Did:**\n>\n> **1. Assessed quickly:**\n> 4 hours to modify dbt model, 6 hours to backfill, 2 hours buffer. Tight but doable.\n>\n> **2. Communicated risks:**\n> 'I can do this. But limited testing time. If backfill fails, we might not have data for demo.'\n>\n> **3. Executed carefully:**\n> - Modified dbt model with engagement calculation\n> - Optimized query for faster backfill\n> - Started backfill at night, monitored at 3 AM\n>\n> **4. Had backup plan:**\n> If engagement rate failed, fallback to follower count (original sorting).\n>\n> **Result:**\n> Demo happened successfully with engagement rate. Client was impressed.\n>\n> **How I Feel About It:**\n> - Last-minute changes are reality in startups\n> - They're not fun but can be managed\n> - Proper risk communication is essential\n> - Buffer time in estimates helps absorb such changes\"\n\n---\n\n## Q50: \"How do you stay updated with the latest industry trends? Can you give an example of applying a new trend or technology in your work?\"\n\n> \"Multiple sources, but always with application in mind.\n>\n> **My Learning Sources:**\n>\n> 1. **Hacker News / Tech Twitter:** Daily scan for interesting posts\n> 2. **Engineering blogs:** Stripe, Uber, Airbnb engineering blogs\n> 3. **Conference talks:** YouTube for QCon, Strange Loop talks\n> 4. **Hands-on:** Side projects to try new things\n>\n> **Example - Applying dbt:**\n>\n> **How I learned about it:**\n> Read about the 'Modern Data Stack' trend on a blog. Companies like GitLab, Shopify were using dbt for transformations.\n>\n> **How I evaluated:**\n> - Read dbt documentation\n> - Did their tutorial project\n> - Compared with our current approach (raw SQL, no versioning)\n>\n> **How I applied:**\n> Proposed dbt for our data platform. Built POC. Got team buy-in. Now we have 112 dbt models.\n>\n> **Result:**\n> - Version-controlled SQL\n> - Incremental processing\n> - Built-in documentation\n> - 50% faster development\n>\n> **Key Principle:**\n> Don't learn trends for resume. Learn what solves your problems, then apply.\"\n\n---\n\n# SECTION 9: ADDITIONAL QUESTIONS FROM COMMENTS\n\n---\n\n## Q51: \"If you were asked to work on an entirely new tech stack, with a new team, how would you approach it?\"\n\n> \"I've done this - learning Go for event-grpc with team I hadn't worked with.\n>\n> **My Approach:**\n>\n> **Week 1 - Foundation:**\n> - Official tutorials (Tour of Go)\n> - Read team's existing code\n> - Ask questions without shame\n>\n> **Week 2 - Pair and Learn:**\n> - Pair with experienced team member\n> - Work on small tasks first\n> - Make mistakes, learn from them\n>\n> **Week 3+ - Contribute:**\n> - Take ownership of small feature\n> - Get code reviewed heavily\n> - Document what I learn for next person\n>\n> **With New Team:**\n> - Understand their culture (code review style, meeting cadence)\n> - Find a buddy/mentor\n> - Over-communicate initially\n> - Build trust through reliable delivery\n>\n> **Key:** Humility. I don't know this stack. I'm here to learn.\"\n\n---\n\n## Q52: \"Let's say you are unable to find any relevant resources and documentation to help you ramp up, what do you do?\"\n\n> \"This happens often with internal systems. No docs, no tutorials.\n>\n> **What I Do:**\n>\n> **1. Read the code:**\n> Code is the ultimate documentation. Start with entry point, trace the flow.\n>\n> **2. Run it locally:**\n> Best way to understand is to use it. Set up, play with it, break it intentionally.\n>\n> **3. Find the expert:**\n> Someone built this. Find them. Buy them coffee. Ask questions.\n>\n> **4. Create the documentation:**\n> As I learn, I document. Future people will thank me.\n>\n> **Real Example:**\n> Our Airflow setup had zero documentation. I:\n> - Read DAG code to understand patterns\n> - Traced failures to understand error handling\n> - Asked the original developer 5 key questions\n> - Wrote a 'DAG Debugging Guide'\n>\n> Now there IS documentation - because I created it.\"\n\n---\n\n## Q53: \"How do you ensure accessibility in the work you do?\"\n\n> \"Accessibility in data/backend context:\n>\n> **API Accessibility:**\n> - Clear error messages, not just status codes\n> - Consistent response formats\n> - Good documentation with examples\n>\n> **Data Accessibility:**\n> - Data dictionaries for every table\n> - Column descriptions in dbt models\n> - Example queries for common use cases\n>\n> **Code Accessibility:**\n> - Comments explaining 'why', not 'what'\n> - Readme files for every service\n> - Onboarding guides for new team members\n>\n> **Process Accessibility:**\n> - Decisions documented in design docs\n> - Meeting notes shared\n> - Knowledge not siloed in one person\n>\n> **Example:**\n> For stir dbt models, I added:\n> - Description for every model\n> - Column descriptions\n> - Example queries\n> - Business context\n>\n> New analyst can understand our data without asking me every question.\"\n\n---\n\n## Q54: \"How do you handle different perspectives around you and how do you make sure that you are being inclusive of everyone's perspectives?\"\n\n> \"Actively seek perspectives, don't just wait for them.\n>\n> **What I Do:**\n>\n> **1. In meetings:**\n> - Ask quiet people directly: 'Alex, what do you think?'\n> - Don't let loud voices dominate\n> - Create space for async input: 'Send thoughts after if you need time'\n>\n> **2. In design decisions:**\n> - Share design doc before meeting\n> - Ask for written feedback (introverts prefer this)\n> - Consider perspectives from different roles (PM, QA, ops)\n>\n> **3. In code reviews:**\n> - Don't dismiss junior perspectives\n> - Ask 'why do you think this approach?' not 'that's wrong'\n> - Consider their context\n>\n> **Example:**\n> During beat architecture discussion, a junior suggested simpler approach I'd dismissed. I asked them to explain. Their reasoning was valid for simpler cases. We ended up with configurable complexity - simple by default, complex when needed.\n>\n> **Key:** My perspective isn't the only valid one. Actively include others.\"\n\n---\n\n## Q55: \"What would your ideal team look like?\"\n\n> \"Mix of skills, shared values.\n>\n> **Skills Mix:**\n> - Senior engineers for technical leadership\n> - Mid-level for execution capacity\n> - Juniors for fresh perspectives and growth\n>\n> **Values:**\n> - Ownership: People who care about outcomes, not just tasks\n> - Curiosity: Always learning, asking why\n> - Collaboration: Help each other, not compete\n> - Directness: Say what you think, respectfully\n>\n> **Working Style:**\n> - Clear goals, autonomy on execution\n> - Regular but not excessive meetings\n> - Written communication for async work\n> - Celebration of wins, blameless analysis of failures\n>\n> **Size:**\n> 5-8 people. Small enough to move fast, large enough for diversity of thought.\n>\n> **What I'd Contribute:**\n> - Technical depth in data systems\n> - Mentorship for juniors\n> - Documentation culture\n> - Ownership mindset\"\n\n---\n\n## Q56: \"As a manager building a team of 10, how many SDE1/SDE2/SDE3 would you hire—and why?\"\n\n> \"My composition: **2 SDE3, 5 SDE2, 3 SDE1**\n>\n> **Rationale:**\n>\n> **SDE3 (2 people - 20%):**\n> - Technical leadership and architecture\n> - Mentorship capacity\n> - Complex problem ownership\n> - One focused on system design, one on execution\n>\n> **SDE2 (5 people - 50%):**\n> - Execution backbone\n> - Independent feature ownership\n> - Can mentor SDE1s\n> - Most of the actual building\n>\n> **SDE1 (3 people - 30%):**\n> - Fresh perspectives\n> - Growth opportunity (pipeline to SDE2)\n> - Learn from seniors\n> - Good for well-defined tasks\n>\n> **Why This Balance:**\n>\n> - **Too many seniors:** Expensive, everyone wants to architect, less execution\n> - **Too many juniors:** High mentorship overhead, slower delivery\n> - **Sweet spot:** Seniors lead, mid-levels build, juniors learn and contribute\n>\n> **Hiring Order:**\n> 1. First SDE3 (establish technical direction)\n> 2. 2-3 SDE2s (start building)\n> 3. Second SDE3 + remaining SDE2s (scale execution)\n> 4. SDE1s last (need seniors to mentor them)\"\n\n---\n\n## Q57: \"Talk about a time when you missed a personal goal.\"\n\n> \"I set a goal to become proficient in Rust in 2023. Didn't achieve it.\n>\n> **The Goal:**\n> Learn Rust well enough to build a side project. Timeline: 6 months.\n>\n> **What Happened:**\n> - Started strong: 2 hours/day for first month\n> - Work got busy: beat scaling issues consumed my time\n> - Rust learning dropped to weekends only\n> - Weekends got busy too\n> - By month 6, I hadn't completed even the basics\n>\n> **Why I Missed:**\n> - Overestimated available time\n> - Underestimated work unpredictability\n> - Didn't adjust goal when circumstances changed\n>\n> **What I'd Do Differently:**\n>\n> 1. **Smaller goal:** 'Complete Rust basics' not 'become proficient'\n> 2. **Protected time:** Calendar blocks that don't get moved\n> 3. **Adjust early:** When work got busy, should have revised timeline\n> 4. **Accountability:** Learning alone is easy to deprioritize\n>\n> **What I Actually Did:**\n> Pivoted to learning Go instead (directly applicable to work). Achieved that goal because it was aligned with job needs.\"\n\n---\n\n## Q58: \"Describe an incident that changed your perception of someone.\"\n\n> \"A colleague I thought was 'difficult' turned out to be under immense pressure.\n>\n> **Initial Perception:**\n> He was dismissive in code reviews, short in messages, seemed uninterested in collaboration.\n>\n> **The Incident:**\n> During a production outage, we were both on call. Working together intensely for 4 hours, I saw a different person:\n> - Deeply knowledgeable about the system\n> - Calm under pressure\n> - Actually helpful when stakes were high\n>\n> After fixing the issue, we chatted. He mentioned he was dealing with a family health situation and was trying to minimize time at work.\n>\n> **Perception Change:**\n> - 'Difficult' → 'Dealing with personal stress'\n> - 'Dismissive' → 'Conserving energy'\n> - 'Uninterested' → 'Focused on essentials'\n>\n> **What I Learned:**\n> Everyone has context I don't see. 'Difficult' behavior often has reasons.\n>\n> **How It Changed Me:**\n> Now when someone seems difficult, I assume positive intent first. Ask 'Is everything okay?' before judging.\"\n\n---\n\n## Q59: \"Share an initiative you took for a customer that made an impact.\"\n\n> \"Proactively built data quality dashboard for customers.\n>\n> **The Context:**\n> Customers (brands) were using our influencer data but had no way to verify its quality. They just had to trust us.\n>\n> **My Initiative (Not Requested):**\n>\n> I noticed customers asking: 'How fresh is this data?' 'How accurate?'\n>\n> Built a data quality dashboard showing:\n> - Data freshness: When was this profile last updated?\n> - Data completeness: Which fields are available?\n> - Update frequency: How often do we refresh?\n>\n> **How I Did It:**\n> - Added metadata tracking to beat\n> - Created dbt models for quality metrics\n> - Built simple dashboard in Metabase\n> - Showed it to product team, they loved it\n>\n> **Impact:**\n> - Customers had transparency into our data\n> - Sales team had proof of quality\n> - Differentiated us from competitors\n> - Reduced support questions about data freshness\n>\n> **Key Learning:**\n> Listen to customer questions. They reveal unmet needs.\"\n\n---\n\n## Q60: \"If asked to organize a non-work team event, what would be your considerations?\"\n\n> \"Inclusivity is the top priority.\n>\n> **My Considerations:**\n>\n> **1. Inclusivity:**\n> - Not everyone drinks → don't center event around bar\n> - Not everyone is athletic → avoid sports-only activities\n> - Different dietary restrictions → ensure food options\n> - Introverts exist → have quieter spaces/activities\n> - Family commitments → reasonable timing\n>\n> **2. Participation:**\n> - Make attendance voluntary, not pressured\n> - Offer alternatives for those who can't attend\n> - Don't make it mandatory for 'team bonding points'\n>\n> **3. Accessibility:**\n> - Physical accessibility of venue\n> - Cost (don't make people pay for expensive things)\n> - Location (easy to reach)\n>\n> **4. Variety:**\n> - Rotate activity types over time\n> - Mix of social and activity-based\n> - Consider remote team members\n>\n> **Example Event I'd Organize:**\n>\n> **Option 1:** Team lunch (noon, everyone can attend, food for all diets)\n> **Option 2:** Game afternoon (board games, video games, chatting option)\n> **Option 3:** Virtual coffee chat for remote folks\n>\n> **Key Principle:**\n> The goal is connection, not a specific activity. Design for maximum inclusion.\"\n\n---\n\n# SUMMARY: KEY STORIES TO REMEMBER\n\n| Story | Use For Questions About |\n|-------|------------------------|\n| Event-driven architecture | Leadership, Technical decision, Biggest accomplishment |\n| GPT integration timeout | Failure, Learning from mistakes |\n| MongoDB vs ClickHouse | Conflict, Data-driven decisions |\n| Fake follower detection | Ambiguity, Innovation, Customer impact |\n| dbt adoption | Influencing without authority, Challenging status quo |\n| Junior engineer Airflow | Mentoring, Helping teammates |\n| Rate limiting code review | Receiving feedback, Improvement |\n| Stories analytics deprioritized | Handling change, Frustration management |\n| Last-minute leaderboard change | Deadline pressure, Last-minute changes |\n| Cross-team Identity integration | Working with other teams |\n\n---\n\n**YE 60 QUESTIONS COVER LAGBHAG 95% OF WHAT GOOGLE ASKS. PRACTICE THESE!**\n"
  },
  {
    "id": "GOOGLE_L4_FINAL_PREP",
    "title": "L4 Final Prep Guide",
    "category": "interview",
    "badge": null,
    "content": "# GOOGLE L4 INTERVIEW - FINAL PREPARATION GUIDE\n## Googleyness & Hiring Manager Rounds - Kal Ke Liye Ready Ho Jao!\n\n---\n\n# PART 1: INTERVIEW STRUCTURE SAMJHO\n\n## Google L4 Interview Format (2025-2026)\n\n\\`\\`\\`\nTotal Rounds: 4 rounds (45 minutes each)\n├── Round 1-3: Coding/DSA (Technical)\n└── Round 4: Googleyness & Leadership (Behavioral) ← YE TUMHARA HAI\n\\`\\`\\`\n\n**Important**: L4 me System Design NAHI hota, sirf coding + behavioral.\n\n### Googleyness Round Breakdown\n\n| Time | What Happens |\n|------|--------------|\n| **0-3 min** | Introduction - \"Tell me about yourself\" |\n| **3-40 min** | 4-5 Behavioral Questions (STAR format) |\n| **40-45 min** | Your questions for interviewer |\n\n---\n\n# PART 2: 6 GOOGLEYNESS ATTRIBUTES - YE YAAD KARO\n\nGoogle evaluate karta hai tumhe **6 core attributes** par:\n\n| # | Attribute | Kya Matlab Hai | Tumhara Example |\n|---|-----------|----------------|-----------------|\n| 1 | **Thriving in Ambiguity** | Jab clear requirements na ho, tab bhi kaam kar sako | Instagram API rate limits suddenly change hue, tune handle kiya |\n| 2 | **Valuing Feedback** | Feedback sunna aur uspe act karna | Senior engineer ne MongoDB suggest kiya, tune ClickHouse prove kiya data se |\n| 3 | **Challenging Status Quo** | Galat cheez ko respectfully challenge karna | dbt recommend kiya Fivetran ki jagah |\n| 4 | **Putting User First** | User needs ko priority dena | Brands ko fake follower detection chahiye tha, tune bana diya |\n| 5 | **Doing the Right Thing** | Ethical decisions lena | System reliability choose kiya over new features |\n| 6 | **Caring About Team** | Team members ki help karna | Junior engineer ko Airflow debugging sikhaya |\n\n---\n\n# PART 3: TOP 25 QUESTIONS + EXACT ANSWERS\n\n## Category 1: LEADERSHIP & INFLUENCE\n\n### Q1: \"Tell me about a time you led a team through a difficult situation\"\n\n**Kab Puchenge**: Almost always - sabse common question\n\n**Tumhara Answer (Word by Word):**\n\n> \"Let me tell you about building the real-time event processing pipeline at Good Creator Co.\n>\n> **Situation**: We were storing influencer data directly in PostgreSQL, but as we scaled to 10 million daily data points, the database was becoming a bottleneck. Query times were increasing, and we were losing time-series granularity.\n>\n> **Task**: I had to design and lead the implementation of a new architecture that could handle this scale while preserving historical data for analytics.\n>\n> **Action**:\n> First, I proposed an event-driven architecture where instead of direct database writes, we'd publish events to RabbitMQ. I then built the event-grpc consumer in Go that batches 1000 events and flushes to ClickHouse every 5 seconds.\n>\n> The challenging part was getting buy-in. Some team members were comfortable with the existing approach. I created a proof-of-concept showing 50x faster analytics queries with ClickHouse. I also documented the migration path to minimize risk.\n>\n> I led the implementation across three services - beat for publishing, event-grpc for consuming, and stir for transformation. We did a phased rollout, starting with non-critical events.\n>\n> **Result**: We achieved 2.5x faster log retrieval times. The system now handles 10,000+ events per second with zero data loss. Most importantly, we enabled time-series analytics that weren't possible before, like tracking follower growth over time.\"\n\n**Time**: 2-3 minutes\n\n---\n\n### Q2: \"Tell me about a time you influenced others without direct authority\"\n\n**Tumhara Answer:**\n\n> \"When building our data platform at Good Creator Co., I advocated for using dbt over a commercial ETL tool.\n>\n> **Situation**: The team wanted to use Fivetran for data transformations. I believed dbt would be better for our use case - we needed version control, custom SQL, and the cost savings mattered for our startup.\n>\n> **Task**: Convince the team without being the decision-maker.\n>\n> **Action**:\n> Rather than just arguing my point, I built a working proof-of-concept. I created 5 dbt models showing the workflow - from raw data to analytics-ready tables. I presented the trade-offs objectively:\n> - Fivetran: Easier setup, but $500+/month, limited customization\n> - dbt: Learning curve, but free, full control, version-controlled\n>\n> I addressed concerns directly: 'Learning curve? I'll create documentation and train the team.' I also offered a compromise: 'Let's try dbt for 2 weeks. If it doesn't work, we switch to Fivetran.'\n>\n> **Result**: dbt was adopted as our primary transformation tool. We saved $6,000+ per year on licensing. The team got upskilled in modern data stack. We now have 112 dbt models powering all our analytics.\"\n\n---\n\n### Q3: \"Describe a time when you had to make a decision with incomplete information\"\n\n**Tumhara Answer:**\n\n> \"One morning, our Instagram data collection suddenly dropped by 80%.\n>\n> **Situation**: Instagram Graph API started returning 429 errors at 10x the normal rate. No warning from Facebook, no documentation about changes. Our customers were asking why their data wasn't updating.\n>\n> **Task**: Diagnose and fix the issue quickly while maintaining data freshness.\n>\n> **Action**:\n> With incomplete information, I had to make quick decisions:\n>\n> First, I analyzed the patterns - the errors weren't random, they correlated with specific credential types. This suggested Facebook had silently reduced rate limits.\n>\n> I made an immediate decision to reduce concurrent workers from 50 to 20 - I didn't have confirmation, but the downside of this decision was acceptable (slower data, not lost data).\n>\n> For medium-term, I implemented credential rotation across multiple Facebook accounts - spreading the load.\n>\n> For long-term, I built adaptive rate limiting that learns from 429 responses and automatically backs off.\n>\n> **Result**: Restored data collection within 2 hours. Built a resilient system that now handles rate limit changes automatically. Created a runbook for similar incidents.\"\n\n---\n\n## Category 2: AMBIGUITY & PROBLEM-SOLVING\n\n### Q4: \"Tell me about a time you navigated ambiguity in a project\"\n\n**Tumhara Answer:**\n\n> \"Building the fake follower detection system was full of ambiguity.\n>\n> **Situation**: Brands wanted to know which influencer followers were fake, but there was no clear definition of 'fake' and no labeled training data. Plus, followers could have names in 10 different Indian languages.\n>\n> **Task**: Build an ML system to detect fake followers with high accuracy, despite no ground truth.\n>\n> **Action**:\n> First, I decomposed the ambiguous problem into concrete signals. Instead of trying to define 'fake', I identified observable patterns:\n> - Non-Indic scripts (Greek, Chinese) in Indian influencer followers = suspicious\n> - More than 4 digits in username = suspicious\n> - Username doesn't match display name = suspicious\n>\n> For the multi-language challenge, I built a transliteration pipeline supporting 10 Indic scripts using HMM models.\n>\n> I designed a scoring system with 3 confidence levels (0.0, 0.33, 1.0) instead of binary fake/real - this acknowledged the inherent uncertainty.\n>\n> I validated by manually checking 500 accounts where I could verify authenticity.\n>\n> **Result**: Achieved ~85% accuracy on validated accounts. The system now processes millions of followers and gives brands actionable insights.\"\n\n---\n\n### Q5: \"How do you approach a project when requirements are unclear?\"\n\n**Tumhara Answer:**\n\n> \"I follow a structured approach that I used when building beat, our data aggregation service.\n>\n> **First**, I identify what IS known. For beat, I knew we needed to scrape Instagram and YouTube data. The unclear part was: how many profiles? What data points? How fresh?\n>\n> **Second**, I build for flexibility. I designed the worker pool system with configurable parameters - each of our 73 flows has adjustable worker count and concurrency. This meant we could tune based on actual requirements.\n>\n> **Third**, I get early feedback. I built a minimal version first - just profile scraping. Deployed it, got feedback: 'We also need posts.' Added that. 'We need engagement metrics.' Added that.\n>\n> **Fourth**, I document decisions and their reasoning. When requirements later became clear, we could evaluate if our assumptions were correct.\n>\n> The result? beat now handles 15+ API integrations with 150+ workers. The flexible architecture meant we could adapt as requirements evolved.\"\n\n---\n\n### Q6: \"Tell me about a time you had to make a trade-off between speed and quality\"\n\n**Tumhara Answer:**\n\n> \"When adding GPT integration to beat for profile enrichment.\n>\n> **Situation**: Marketing wanted AI-powered demographic inference urgently for a major client pitch. They wanted it in one week.\n>\n> **Task**: Deliver GPT integration quickly without compromising system reliability.\n>\n> **Trade-off Decision**:\n>\n> I could have:\n> - Option A: Build a full solution with retry logic, fallbacks, monitoring (3 weeks)\n> - Option B: Quick integration, accept some failure cases (1 week)\n>\n> I chose a **middle path**: Build the core integration quickly, but make the feature degradable.\n>\n> **Action**:\n> - Week 1: Shipped basic GPT integration that worked for 70% of cases\n> - Made it async - if GPT fails, the profile still processes, enrichment happens later\n> - Used temperature=0 for consistent outputs\n> - Added simple timeout (30 seconds)\n>\n> **Result**: Delivered for the client pitch on time. 70% of profiles got enriched immediately. Later, I added proper retry logic and improved accuracy to 85%. The key insight: **deliver value early, iterate on quality**.\"\n\n---\n\n## Category 3: FAILURE & LEARNING\n\n### Q7: \"Tell me about a time you failed\" ← YE 100% PUCHENGE\n\n**Google kya chahta hai sunna:**\n- Real failure (fake mat bolo)\n- Ownership (blame mat karo)\n- What you learned\n- How you improved\n\n**Tumhara Answer:**\n\n> \"I'll tell you about a production incident I caused with GPT integration.\n>\n> **Situation**: I had added OpenAI GPT integration to beat for inferring creator demographics. In testing, it worked perfectly.\n>\n> **What went wrong**: In production, 30% of requests started timing out. GPT API had variable latency - sometimes 2 seconds, sometimes 45 seconds. I hadn't accounted for this variability.\n>\n> **The failure**: I should have load-tested with realistic conditions. I was excited about the feature and rushed it to production.\n>\n> **What I learned**:\n> 1. External APIs have unpredictable behavior - always test with realistic load\n> 2. Features should be degradable - system should work without optional components\n> 3. Set explicit timeouts for every external call\n>\n> **What I did**:\n> 1. Added 30-second timeout with circuit breaker\n> 2. Made GPT enrichment asynchronous - separate worker queue\n> 3. System now works without GPT data, enriches later in background\n> 4. Added monitoring dashboard for GPT latency\n>\n> **Now**: The integration runs reliably with 95%+ success rate. More importantly, I apply these learnings to every external integration.\"\n\n---\n\n### Q8: \"Tell me about a time you received critical feedback. How did you handle it?\"\n\n**Tumhara Answer:**\n\n> \"Early in building beat, a senior engineer reviewed my rate limiting code and said it was 'too complex and would be hard to maintain.'\n>\n> **My initial reaction**: Honestly, I felt defensive. I had worked hard on it.\n>\n> **What I did**:\n> First, I took a day before responding. I re-read my code with fresh eyes.\n>\n> I realized they were right. My rate limiting had 5 different classes, complex inheritance, and was hard to follow.\n>\n> I asked them to pair with me and refactored it. The new version used simple stacked context managers:\n>\n> \\`\\`\\`python\n> async with RateLimiter(global_limit):\n>     async with RateLimiter(per_minute_limit):\n>         async with RateLimiter(per_handle_limit):\n>             result = await make_api_call()\n> \\`\\`\\`\n>\n> **Result**: Code became much simpler, easier to test, and easier for others to understand.\n>\n> **What I learned**: 'Complex' isn't impressive. Simple is hard and valuable. Now I actively seek code review feedback and specifically ask 'Is this too complex?'\"\n\n---\n\n### Q9: \"Describe a time when you missed a deadline\"\n\n**Tumhara Answer:**\n\n> \"When building the ClickHouse → PostgreSQL sync pipeline in stir.\n>\n> **Situation**: I committed to delivering the full sync pipeline in 2 weeks. It was my first time working with Airflow's SSHOperator and cross-database sync patterns.\n>\n> **What happened**: At 1.5 weeks, I realized the atomic table swap pattern I'd designed had edge cases I hadn't considered. What if S3 upload fails? What if PostgreSQL table rename fails mid-way?\n>\n> **The miss**: I had to push the deadline by 1 week.\n>\n> **How I handled it**:\n> 1. **Communicated early**: As soon as I saw the risk, I told the team - not on day 14, but day 10\n> 2. **Explained why**: Not 'it's taking longer' but 'I found edge cases that could cause data loss'\n> 3. **Proposed plan**: 'I need 1 more week to build proper error handling and retry logic'\n>\n> **What I delivered**: A robust pipeline with:\n> - Atomic table swap for zero-downtime updates\n> - Retry logic at each step\n> - Rollback capability if sync fails\n>\n> **Lesson**: Now when I estimate, I add 30% buffer for unknowns, especially with new technologies.\"\n\n---\n\n## Category 4: COLLABORATION & TEAMWORK\n\n### Q10: \"Tell me about a time you had a conflict with a colleague\"\n\n**Google yahan check karta hai**:\n- Kya tum respectfully disagree kar sakte ho\n- Kya tum data-driven decisions lete ho\n- Kya tum relationships maintain karte ho\n\n**Tumhara Answer:**\n\n> \"I had a technical disagreement with a senior engineer about database choice for our analytics platform.\n>\n> **Situation**: They strongly advocated for MongoDB because of their expertise with it. I believed ClickHouse was better for our OLAP workload.\n>\n> **How conflict started**: In a design review, they said 'MongoDB can handle this easily.' I said 'I think we need columnar storage.' The discussion became a bit heated.\n>\n> **How I handled it**:\n>\n> First, I asked to understand their perspective: 'Help me understand why MongoDB fits here. What advantages do you see?' They explained familiarity, document flexibility, and easier development.\n>\n> Then I proposed an experiment: 'What if we benchmark both with our actual queries? Let the data decide.'\n>\n> We tested with 100 million rows and typical analytics queries:\n> - MongoDB: 45 seconds for aggregation query\n> - ClickHouse: 0.8 seconds for same query\n>\n> I presented results objectively, acknowledging their valid points: 'You're right that MongoDB is more flexible for schema changes. But for our analytics use case, performance difference is 50x.'\n>\n> **Result**: We went with ClickHouse. The senior engineer became an advocate after seeing performance in production. Our relationship actually improved because I respected their input and let data decide.\"\n\n---\n\n### Q11: \"Tell me about a time you helped a struggling teammate\"\n\n**Tumhara Answer:**\n\n> \"A junior engineer was stuck on an Airflow DAG failure for 2 days.\n>\n> **Situation**: The DAG kept failing with cryptic timeout errors. They had tried various fixes but nothing worked. They were stressed and considering escalating.\n>\n> **What I did**:\n>\n> First, I didn't just take over and fix it. I sat with them and walked through systematic debugging:\n>\n> 1. 'Let's check Airflow scheduler logs first'\n> 2. 'Which specific task is failing?'\n> 3. 'What does that task's code do?'\n> 4. 'Let's check database connection settings'\n>\n> We found it together: ClickHouse connection timeout was too short for a heavy aggregation query.\n>\n> I explained WHY it happened, not just the fix: 'ClickHouse is processing billions of rows. Default 30-second timeout isn't enough for this query.'\n>\n> We implemented the fix together: connection retry with exponential backoff.\n>\n> Then I asked them to document it: 'Can you write a DAG Debugging Guide based on what we learned?'\n>\n> **Result**:\n> - Junior engineer solved future issues independently\n> - The debugging guide became team documentation\n> - Team escalations reduced by 40%\n>\n> **Key**: I invested time in teaching, not just doing.\"\n\n---\n\n### Q12: \"How do you work with people who have different working styles?\"\n\n**Tumhara Answer:**\n\n> \"In our team, I worked with developers who had very different styles.\n>\n> One senior engineer was very detail-oriented - he'd spend days perfecting code before committing. Another moved fast and iterated quickly.\n>\n> **How I adapted**:\n>\n> With the detail-oriented engineer, I learned to have early design discussions. Rather than showing him finished code, I'd share my approach first: 'I'm thinking of using buffered channels in Go for the sinker. What do you think?' This way, he felt involved and his perfectionism became an asset in design phase.\n>\n> With the fast-moving developer, I focused on establishing clear interfaces. 'You handle the API integration, I'll handle the processing pipeline. Let's agree on this data contract.' This gave him freedom to move fast within boundaries.\n>\n> For code reviews, I calibrated my feedback. For the perfectionist, I'd say 'This looks great, ship it.' For the fast mover, I'd say 'Let's add error handling for this edge case.'\n>\n> **Result**: Our team shipped beat with all 73 flows on time. Both engineers felt their style was respected.\"\n\n---\n\n## Category 5: ETHICS & DOING THE RIGHT THING\n\n### Q13: \"Tell me about a time you had to make an unpopular decision\"\n\n**Tumhara Answer:**\n\n> \"I chose to delay a new feature to fix system reliability.\n>\n> **Situation**: Product team wanted a new leaderboard feature urgently. But our Airflow DAGs were failing 2-3 times per week, causing data delays.\n>\n> **The unpopular decision**: I advocated for fixing DAG reliability first, delaying the leaderboard by 2 weeks.\n>\n> **How I made the case**:\n>\n> I showed the data: 'In the last month, we had 12 DAG failures. Each failure delays data by 2-4 hours. Users are complaining about stale data.'\n>\n> I explained the trade-off: 'If we add more features on an unstable foundation, we'll have more failures, not fewer.'\n>\n> I proposed a compromise: 'Give me 2 weeks for reliability. Then I'll deliver leaderboard with confidence.'\n>\n> **Result**: After reliability fixes, DAG failures dropped to near-zero. Leaderboard shipped 2 weeks later and worked flawlessly. Product team later acknowledged this was the right call.\"\n\n---\n\n### Q14: \"Give an example of doing the right thing even when it was difficult\"\n\n**Tumhara Answer:**\n\n> \"I discovered that one of our data sources was providing partially fabricated data.\n>\n> **Situation**: A RapidAPI provider we used for Instagram data was returning suspicious metrics. Engagement rates were impossibly high for some profiles.\n>\n> **The dilemma**: This data source was cheaper and faster than alternatives. Switching would increase costs and slow down our pipeline.\n>\n> **What I did**:\n>\n> First, I validated my suspicion. I cross-checked data from this source against Instagram's official Graph API for 100 profiles. The discrepancies were significant - sometimes 2-3x higher engagement.\n>\n> I documented my findings with evidence and presented to the team: 'We can't serve potentially fabricated data to our customers. Brands make spending decisions based on these metrics.'\n>\n> I proposed a solution: 'Let's move this source to lowest priority in our fallback chain. Only use it when all other sources fail, and flag that data as unverified.'\n>\n> **Result**: We maintained data integrity. Customers trusted our platform. The short-term cost increase was worth the long-term trust.\"\n\n---\n\n## Category 6: INNOVATION & CREATIVE SOLUTIONS\n\n### Q15: \"Tell me about a time you created something from nothing\"\n\n**Tumhara Answer:**\n\n> \"I built the fake follower detection system from scratch with no existing framework.\n>\n> **Starting point**: Zero. No training data, no existing models, no clear definition of 'fake'.\n>\n> **The innovation challenge**: How do you detect fake followers when you can't even define 'fake'?\n>\n> **My approach**:\n>\n> Instead of ML classification (which needs labeled data), I designed rule-based heuristics from first principles:\n>\n> 1. 'What makes an account suspicious?' → Non-Indian scripts in an Indian influencer's followers\n> 2. 'What patterns do bots follow?' → Sequential usernames (user1234, user1235)\n> 3. 'What indicates real humans?' → Name matches handle when transliterated\n>\n> For the multi-language problem, I built a transliteration pipeline from scratch supporting 10 Indic scripts using HMM models and custom Hindi character mappings.\n>\n> For scalability, I designed a serverless architecture: ClickHouse → S3 → SQS → Lambda → Kinesis\n>\n> **Result**: A working fake follower detection system that:\n> - Processes millions of followers\n> - Supports 10 Indian languages\n> - Runs cost-effectively on serverless\n> - Gives brands actionable insights\n>\n> All built from nothing.\"\n\n---\n\n### Q16: \"What's the most innovative solution you've implemented?\"\n\n**Tumhara Answer:**\n\n> \"The gradient descent algorithm for audience normalization in beat.\n>\n> **The problem**: Instagram's Audience Insights API returns percentages that don't add up to 100%. Sometimes they add to 95%, sometimes 105%. We couldn't serve inconsistent data.\n>\n> **The innovative solution**: I implemented gradient descent optimization to normalize the audience demographics while preserving relative proportions.\n>\n> \\`\\`\\`python\n> def gradient_descent(a, b, learning_rate=0.01, epochs=1000):\n>     # a = array of percentages to normalize\n>     # b = target sum (100)\n>     for epoch in range(epochs):\n>         current_sum = sum(a)\n>         error = b - current_sum\n>         gradient = error / len(a)\n>         a = [x + learning_rate * gradient for x in a]\n>     return a\n> \\`\\`\\`\n>\n> **Why this was innovative**: Instead of simple scaling (which can create 0.1% values), gradient descent adjusts each value proportionally while converging to exactly 100%.\n>\n> **Result**: Perfectly normalized audience demographics that maintain relative proportions. No inconsistent data for customers.\"\n\n---\n\n## Category 7: TECHNICAL DECISION-MAKING\n\n### Q17: \"Walk me through a significant technical decision you made\"\n\n**Tumhara Answer:**\n\n> \"Choosing the architecture for our event processing pipeline.\n>\n> **The decision**: Whether to write directly to ClickHouse from beat, or use RabbitMQ + event-grpc as an intermediate layer.\n>\n> **Options I considered**:\n>\n> **Option A: Direct writes from beat to ClickHouse**\n> - Pros: Simpler, fewer moving parts\n> - Cons: ClickHouse connection limits, no retry on failure, tightly coupled\n>\n> **Option B: RabbitMQ → event-grpc → ClickHouse**\n> - Pros: Decoupled, retry built-in, batch for efficiency\n> - Cons: More complex, more infrastructure\n>\n> **My analysis**:\n>\n> We were generating 10,000+ events per second. Direct connection from 150+ workers would exhaust ClickHouse connection pool.\n>\n> If ClickHouse went down, direct writes would lose data. With RabbitMQ, events queue up safely.\n>\n> Batching 1000 events vs individual inserts = 1000x fewer database operations.\n>\n> **Decision**: I chose Option B - event-driven with buffered sinkers.\n>\n> **Validation**: System has been running 15+ months with zero data loss. We've handled ClickHouse maintenance windows without losing events - they just queue up and process when ClickHouse is back.\"\n\n---\n\n### Q18: \"How do you approach learning new technologies?\"\n\n**Tumhara Answer:**\n\n> \"For stir, I had to learn Airflow, dbt, and ClickHouse - all new to me.\n>\n> **My approach**:\n>\n> **1. Start with 'why'**: Why does this technology exist? Airflow = workflow orchestration. dbt = SQL transformation with software engineering practices. ClickHouse = fast OLAP.\n>\n> **2. Build something small**: Before writing production code, I built a toy project - a simple DAG that runs a dbt model. Broke it intentionally, learned from errors.\n>\n> **3. Read source code**: When Airflow behaved unexpectedly, I read the operator source code. This taught me more than documentation.\n>\n> **4. Learn from production issues**: Every production bug became a learning opportunity. DAG failure → learned about Airflow's retry mechanisms.\n>\n> **5. Teach others**: I wrote a 'DAG Debugging Guide' for the team. Teaching forced me to truly understand.\n>\n> **Result**: In 3 months, I went from zero to building 76 production DAGs and 112 dbt models. The key is structured learning with immediate application.\"\n\n---\n\n# PART 4: \"TELL ME ABOUT YOURSELF\" - PERFECT ANSWER\n\n**This question starts 90% of interviews. Tumhara 90-second answer:**\n\n> \"I'm a software engineer with 3 years of experience building data-intensive systems.\n>\n> At Good Creator Co., I was responsible for the entire data platform that powers India's largest influencer marketing platform.\n>\n> **Three highlights from my work**:\n>\n> **First**, I built 'beat' - a data aggregation service that processes 10 million+ daily data points from Instagram and YouTube. I designed the worker pool architecture with 150+ concurrent workers and multi-level rate limiting.\n>\n> **Second**, I built 'stir' - our data transformation platform using Airflow and dbt. 76 DAGs, 112 dbt models, processing billions of records. This reduced data latency by 50%.\n>\n> **Third**, I built a fake follower detection system from scratch - an ML ensemble supporting 10 Indian languages, running on AWS Lambda.\n>\n> What excites me about Google is the scale - building systems that impact billions of users. And the engineering culture - learning from the best engineers in the world.\"\n\n---\n\n# PART 5: QUESTIONS TO ASK THE INTERVIEWER\n\n## Googleyness Round ke liye\n\n1. **\"What does a typical project look like for an L4 engineer on your team?\"**\n   - Shows you're thinking about the actual work\n\n2. **\"How does the team handle disagreements on technical decisions?\"**\n   - Shows you value collaboration\n\n3. **\"What's an example of how the team navigated ambiguity recently?\"**\n   - Shows you understand Googleyness\n\n4. **\"What opportunities are there for cross-team collaboration?\"**\n   - Shows you're not siloed\n\n## Hiring Manager Round ke liye\n\n1. **\"What does success look like in the first 6 months?\"**\n   - Shows you want to deliver\n\n2. **\"What are the biggest technical challenges the team is facing?\"**\n   - Shows you want hard problems\n\n3. **\"How do you balance feature work vs. technical debt?\"**\n   - Shows engineering maturity\n\n4. **\"What's the team's approach to on-call and incident response?\"**\n   - Shows you understand production responsibility\n\n---\n\n# PART 6: COMMON MISTAKES - YE MAT KARO\n\n## Red Flags Google Interviewers Watch For\n\n| Mistake | Why It's Bad | What To Do Instead |\n|---------|--------------|-------------------|\n| **Blaming others** | Shows you don't take ownership | \"We had a miscommunication\" → \"I should have clarified requirements\" |\n| **Generic answers** | Shows you're not prepared | Use specific numbers and examples from YOUR work |\n| **Only \"I\" statements** | Shows poor collaboration | Balance \"I did X\" with \"I worked with team on Y\" |\n| **No failures** | Shows lack of self-awareness | Share real failures with genuine learnings |\n| **Negative about past company** | Shows you might be negative at Google | Focus on what you learned, not what was bad |\n| **Long, rambling answers** | Shows poor communication | STAR format, 2-3 minutes max per answer |\n| **No questions for interviewer** | Shows lack of interest | Always have 3-4 thoughtful questions ready |\n\n---\n\n# PART 7: DAY-OF TIPS\n\n## Before Interview\n\n- [ ] Test Google Meet (video, audio)\n- [ ] Good lighting, clean background\n- [ ] Water bottle ready\n- [ ] Notepad for notes\n- [ ] Review your STAR stories one more time\n\n## During Interview\n\n- [ ] **Listen carefully** - Don't start answering before they finish\n- [ ] **Ask clarifying questions** if needed - \"Just to make sure I understand, you're asking about...\"\n- [ ] **Take 5 seconds** to think before answering - It's okay!\n- [ ] **Be specific** - Use numbers, project names, actual technologies\n- [ ] **Show ownership** - Use \"I\" when describing your contributions\n- [ ] **Acknowledge others** - \"I worked with the team on...\" shows collaboration\n- [ ] **Be honest** - If you don't know something, say so\n\n## Body Language (Video Call)\n\n- Look at camera when speaking (not at screen)\n- Smile and be energetic\n- Nod when interviewer speaks (shows you're listening)\n- Sit up straight\n\n---\n\n# PART 8: METRICS CHEAT SHEET - YE NUMBERS YAAD KARO\n\n| Project | Key Metrics |\n|---------|-------------|\n| **beat** | 10M+ daily data points, 73 flows, 150+ workers, 15+ APIs, 25% faster response, 30% cost reduction |\n| **stir** | 76 DAGs, 112 dbt models, 50% latency reduction, billions of records |\n| **event-grpc** | 10,000+ events/sec, 26 queues, 70+ workers, 1000 events/batch, 5-sec flush |\n| **fake_follower** | 10 Indic scripts, 35,183 names, 5-feature ensemble, 85% accuracy |\n\n---\n\n# PART 9: QUICK REFERENCE - QUESTION → STORY MAPPING\n\n| When They Ask | Use This Story |\n|--------------|----------------|\n| Leadership | Event-driven architecture implementation |\n| Ambiguity | Fake follower detection (no training data) |\n| Failure | GPT integration timeout issue |\n| Conflict | MongoDB vs ClickHouse debate |\n| Feedback | Rate limiting code review |\n| Innovation | Gradient descent for audience normalization |\n| Helping others | Junior engineer Airflow debugging |\n| Technical decision | RabbitMQ + event-grpc vs direct writes |\n| Unpopular decision | Reliability over new features |\n| Ethics | Rejecting fabricated data source |\n\n---\n\n# FINAL CHECKLIST\n\n- [ ] 6 Googleyness attributes yaad hai\n- [ ] 5-6 STAR stories practiced (2-3 min each)\n- [ ] \"Tell me about yourself\" smooth hai (90 seconds)\n- [ ] Project metrics yaad hai\n- [ ] Questions ready for interviewer\n- [ ] Technical decisions explain kar sakte ho\n\n---\n\n**GOOD LUCK KAL KE LIYE! TUJHE SAHI ME BAHUT EXPERIENCE HAI - CONFIDENTLY BOL!**\n\n---\n\n## Sources & References\n\nBased on research from:\n- [Google L4 Interview Guide 2026 - HelloInterview](https://www.hellointerview.com/guides/google/l4)\n- [Google Software Engineer Interview Guide 2025 - InterviewQuery](https://www.interviewquery.com/interview-guides/google-software-engineer)\n- [Googleyness & Leadership Interview Questions - IGotAnOffer](https://igotanoffer.com/blogs/tech/googleyness-leadership-interview-questions)\n- [Google Behavioral Interview Guide - Careerflow](https://www.careerflow.ai/blog/google-behavioural-interview-guide)\n- [Google L4 Interview Experiences - LeetCode Discuss](https://leetcode.com/discuss/post/6469509/google-latest-interview-experiences-coll-r4zm/)\n- [Google L4 India Experience 2025 - LeetCode](https://leetcode.com/discuss/post/7164554/google-l4-india-interview-experience-by-xqvdo/)\n"
  },
  {
    "id": "GOOGLE_INTERVIEW_SCRIPTS",
    "title": "Interview Scripts",
    "category": "interview",
    "badge": null,
    "content": "# GOOGLE INTERVIEW - EXACT SCRIPTS\n## Word-by-Word Kya Bolna Hai\n\n---\n\n# OPENING: \"TELL ME ABOUT YOURSELF\"\n\n## 90-Second Script (Practice This!)\n\n\\`\\`\\`\n\"Hi, I'm Anshul. I'm a software engineer with about 3 years of experience,\ncurrently at Good Creator Co, which is India's largest influencer marketing platform.\n\nMy work has been primarily in three areas:\n\nFIRST, I built 'beat', our data aggregation service. It scrapes Instagram and\nYouTube data at scale - we process about 10 million data points daily. I designed\nthe entire worker pool architecture - 150 concurrent workers, multi-level rate\nlimiting, and integrations with 15+ external APIs.\n\nSECOND, I built 'stir', our data platform using Airflow and dbt. 76 production DAGs,\n112 dbt models, processing billions of records. This reduced our data latency by 50%.\n\nTHIRD, I built a fake follower detection system from scratch - an ML ensemble that\nsupports 10 Indian languages and runs on serverless architecture.\n\nWhat excites me about Google is the scale of impact - building systems that serve\nbillions of users - and the engineering culture of learning from the best.\n\nI'd love to hear more about the team and the challenges you're working on.\"\n\\`\\`\\`\n\n**Time**: 90 seconds exactly\n**Practice**: Record yourself, time it\n\n---\n\n# GOOGLEYNESS QUESTIONS - SCRIPT BY SCRIPT\n\n## Question 1: \"Tell me about a time you led a team through a difficult situation\"\n\n### Script (2-3 minutes):\n\n\\`\\`\\`\n\"Sure, let me tell you about redesigning our data pipeline at Good Creator Co.\n\nSITUATION:\nWe were storing all our influencer data - profiles, posts, engagement metrics -\ndirectly in PostgreSQL. As we scaled to 10 million daily data points, we started\nhitting problems. Query times were increasing, we were losing time-series granularity,\nand the database was becoming a bottleneck.\n\nTASK:\nI had to design a new architecture that could handle this scale. I also had to lead\nthe implementation across three different services with different tech stacks.\n\nACTION:\nFirst, I proposed an event-driven architecture. Instead of direct database writes,\nwe'd publish events to RabbitMQ. I built the consumer service in Go - called\nevent-grpc - that batches 1000 events and flushes to ClickHouse every 5 seconds.\n\nThe challenging part was getting buy-in. Some team members were comfortable with\nthe existing PostgreSQL approach. So I created a proof-of-concept showing that\nClickHouse could run analytics queries 50x faster than PostgreSQL for our use case.\n\nI also documented the migration path carefully - we did a phased rollout, starting\nwith non-critical events, so we could verify reliability before migrating everything.\n\nI coordinated across three services:\n- beat publishes events to RabbitMQ\n- event-grpc consumes and writes to ClickHouse\n- stir transforms the data using dbt\n\nRESULT:\nWe achieved 2.5x faster log retrieval times. The system handles 10,000+ events per\nsecond with zero data loss. And we enabled time-series analytics that weren't\npossible before - like tracking follower growth over time.\n\nThe architecture has been running in production for over 15 months now with minimal\nincidents.\"\n\\`\\`\\`\n\n---\n\n## Question 2: \"Tell me about a time you failed\"\n\n### Script (2 minutes):\n\n\\`\\`\\`\n\"I'll tell you about a production incident I caused with our GPT integration.\n\nSITUATION:\nI had added OpenAI GPT integration to our data service for inferring creator\ndemographics from their profile bios. In testing, it worked perfectly - fast\nresponses, accurate results.\n\nWHAT WENT WRONG:\nWhen we deployed to production, about 30% of requests started timing out.\nGPT API had variable latency - sometimes 2 seconds, sometimes 45 seconds.\nI hadn't accounted for this variability. In my testing, I always got quick\nresponses, but production load was different.\n\nTHE FAILURE:\nI should have load-tested with realistic conditions. I was excited about\nthe feature and rushed it to production without proper testing.\n\nWHAT I LEARNED:\nThree key lessons:\nFirst, external APIs have unpredictable behavior - always test with realistic load.\nSecond, features should be degradable - the system should work without optional\ncomponents.\nThird, always set explicit timeouts for every external call.\n\nWHAT I DID TO FIX IT:\nI added a 30-second timeout with a circuit breaker pattern. Then I made GPT\nenrichment asynchronous - moved it to a separate worker queue. Now the main\nsystem works without GPT data and enriches in the background.\n\nThe integration now runs with 95%+ success rate, and more importantly,\nI apply these learnings to every external integration we build.\"\n\\`\\`\\`\n\n---\n\n## Question 3: \"Tell me about a time you had a conflict with a colleague\"\n\n### Script (2 minutes):\n\n\\`\\`\\`\n\"I had a technical disagreement with a senior engineer about our database choice.\n\nSITUATION:\nWe were designing the analytics platform. The senior engineer strongly advocated\nfor MongoDB because of his expertise with it. I believed ClickHouse was better\nfor our OLAP workload - aggregation queries over billions of rows.\n\nHOW IT STARTED:\nIn a design review, he said 'MongoDB can handle this easily.' I said 'I think we\nneed columnar storage for these queries.' The discussion got a bit heated.\n\nHOW I HANDLED IT:\nFirst, I stepped back and asked to understand his perspective: 'Help me understand\nwhy MongoDB fits here. What advantages do you see?'\n\nHe explained: familiarity, document flexibility, and faster development.\n\nThen I proposed an experiment rather than arguing: 'What if we benchmark both with\nour actual queries? Let the data decide.'\n\nWe tested with 100 million rows:\n- MongoDB took 45 seconds for our typical aggregation query\n- ClickHouse took 0.8 seconds for the same query\n\nI presented results objectively, and I acknowledged his valid points: 'You're right\nthat MongoDB is more flexible for schema changes. But for our analytics use case,\nthe performance difference is 50x.'\n\nRESULT:\nWe went with ClickHouse. The senior engineer actually became an advocate after\nseeing the production performance. Our relationship improved because I respected\nhis input and let data make the decision.\"\n\\`\\`\\`\n\n---\n\n## Question 4: \"Tell me about navigating ambiguity\"\n\n### Script (2 minutes):\n\n\\`\\`\\`\n\"Building the fake follower detection system was full of ambiguity.\n\nSITUATION:\nBrands wanted to know which influencer followers were fake. But there was no clear\ndefinition of 'fake' - is it bots? Inactive accounts? Purchased followers? And\nwe had no labeled training data. Plus, followers could have names in 10 different\nIndian languages - Hindi, Bengali, Tamil, and so on.\n\nTASK:\nBuild an ML system with high accuracy despite having no ground truth to train on.\n\nACTION:\nFirst, I decomposed the ambiguous problem into concrete, observable signals.\n\nInstead of trying to define 'fake', I identified patterns:\n- Non-Indic scripts like Greek or Chinese in an Indian influencer's followers = suspicious\n- More than 4 digits in username = suspicious\n- Username doesn't match display name = suspicious\n\nFor the multi-language challenge, I built a transliteration pipeline supporting\n10 Indic scripts using HMM models.\n\nI designed a scoring system with 3 confidence levels - 0.0, 0.33, 1.0 - instead\nof binary fake or real. This acknowledged the inherent uncertainty.\n\nTo validate, I manually checked 500 accounts where I could verify authenticity\nthrough other signals.\n\nRESULT:\nAchieved about 85% accuracy on the validated accounts. The system now processes\nmillions of followers and gives brands actionable insights.\n\nThe key was breaking down an ambiguous goal into concrete, measurable signals.\"\n\\`\\`\\`\n\n---\n\n## Question 5: \"Tell me about receiving critical feedback\"\n\n### Script (90 seconds):\n\n\\`\\`\\`\n\"Early in building beat, a senior engineer reviewed my rate limiting code\nand said it was 'too complex and hard to maintain.'\n\nMY INITIAL REACTION:\nHonestly, I felt defensive. I had worked hard on it.\n\nWHAT I DID:\nFirst, I took a day before responding. I re-read my code with fresh eyes.\n\nAnd I realized they were right. My rate limiting had 5 different classes,\ncomplex inheritance, and was hard to follow.\n\nI asked them to pair with me and we refactored it together. The new version\nused simple stacked context managers - much cleaner.\n\nRESULT:\nCode became simpler, easier to test, and easier for others to understand.\n\nWHAT I LEARNED:\n'Complex' isn't impressive. Simple is hard and valuable. Now I actively\nseek code review feedback and specifically ask 'Is this too complex?'\"\n\\`\\`\\`\n\n---\n\n## Question 6: \"Why Google?\"\n\n### Script (60 seconds):\n\n\\`\\`\\`\n\"Three reasons:\n\nFIRST, Scale. At Good Creator Co, I built systems handling 10 million daily\ndata points. At Google, I'd work on systems serving billions of users. That's\nthe scale of impact I want.\n\nSECOND, Learning. Google's engineering culture is legendary. The opportunity\nto learn from engineers who've built YouTube, Search, Cloud - that's incredible.\n\nTHIRD, Growth. I've been the one building systems from scratch. I want to be\nin an environment where I can also learn from existing world-class systems.\n\nWhat I bring is end-to-end ownership experience. I've built complete systems -\ndata pipelines, real-time processing, ML models. I know both Python and Go.\nAnd I have the startup scrappiness - bias to action, doing more with less.\n\nI'm excited about the team you're building and would love to contribute.\"\n\\`\\`\\`\n\n---\n\n# SITUATIONAL QUESTIONS - SCRIPTS\n\n## \"How would you handle a disagreement with your manager?\"\n\n\\`\\`\\`\n\"I'd approach it with data and curiosity, not defensiveness.\n\nFirst, I'd make sure I understand their perspective. Maybe they have context\nI don't have.\n\nThen, if I still disagree, I'd present my view with evidence. Not 'I think\nthis is wrong' but 'Based on these metrics, I believe option B would be better\nbecause X, Y, Z.'\n\nI'd propose an experiment if possible: 'Can we try my approach on a small\nscale and measure the results?'\n\nAnd ultimately, if they decide to go a different direction after hearing\nmy input, I'd commit to that decision fully. Disagree and commit.\n\nI actually did this at Good Creator Co when advocating for dbt over Fivetran.\nI presented the trade-offs, offered to prove it with a POC, and the team\nagreed. But if they hadn't, I would have committed to Fivetran.\"\n\\`\\`\\`\n\n---\n\n## \"What would you do if you missed a deadline?\"\n\n\\`\\`\\`\n\"Three things:\n\nFIRST, communicate early. As soon as I see risk, I tell stakeholders.\nNot on the deadline day, but as soon as I know.\n\nSECOND, explain why with specifics. Not 'it's taking longer' but 'I found\nedge cases that need handling to avoid data corruption.'\n\nTHIRD, propose a plan. 'I need one more week, and here's exactly what I'll\ndeliver and why it's worth the wait.'\n\nI actually experienced this with our ClickHouse sync pipeline. At 10 days\ninto a 2-week estimate, I found edge cases I hadn't anticipated. I\ncommunicated immediately, explained the risks of not handling them, and\nwe agreed on 1 extra week. The result was a robust pipeline with proper\nerror handling and rollback capability.\"\n\\`\\`\\`\n\n---\n\n## \"How do you prioritize when everything is urgent?\"\n\n\\`\\`\\`\n\"I use a simple framework:\n\nFIRST, what unblocks others? If my delay blocks the whole team, that's\nhighest priority.\n\nSECOND, what has customer impact? External commitments over internal\nimprovements.\n\nTHIRD, what's the impact-to-effort ratio? Quick wins that deliver big\nvalue come before big efforts with uncertain value.\n\nFor example, at Good Creator Co, I had to choose between a new leaderboard\nfeature and fixing DAG reliability. Product wanted the feature urgently.\n\nBut I realized: failing DAGs blocked the entire data team. New features\non an unstable foundation would just create more problems.\n\nI advocated for reliability first. We fixed DAG failures in 2 weeks, then\nshipped leaderboard. Product later acknowledged this was the right call.\"\n\\`\\`\\`\n\n---\n\n# CLOSING QUESTIONS TO ASK\n\n## When They Ask \"Do you have questions for me?\"\n\n### Question 1:\n\\`\\`\\`\n\"What does a typical project look like for an L4 engineer on your team?\nI'm curious about the scope and the kind of ownership expected.\"\n\\`\\`\\`\n\n### Question 2:\n\\`\\`\\`\n\"How does the team handle technical disagreements? Is there a culture of\nwritten design docs, or more informal discussions?\"\n\\`\\`\\`\n\n### Question 3:\n\\`\\`\\`\n\"What's an example of how the team navigated ambiguity recently? I'm\ninterested in how decisions get made when requirements aren't clear.\"\n\\`\\`\\`\n\n### Question 4:\n\\`\\`\\`\n\"What does success look like in the first 6 months for someone in this role?\"\n\\`\\`\\`\n\n---\n\n# EMERGENCY SCRIPTS\n\n## If You Don't Know the Answer:\n\n\\`\\`\\`\n\"That's a great question. I haven't faced that exact situation, but\nlet me think about how I'd approach it...\"\n\n[Then use a related experience or talk through your reasoning]\n\\`\\`\\`\n\n## If You Need Time to Think:\n\n\\`\\`\\`\n\"That's an interesting question. Let me take a moment to think of the\nbest example...\"\n\n[5-second pause is fine!]\n\\`\\`\\`\n\n## If You're Rambling:\n\n\\`\\`\\`\n\"Let me summarize - the key point is [one sentence summary].\nDoes that answer your question, or would you like me to go deeper?\"\n\\`\\`\\`\n\n## If Interviewer Interrupts:\n\n\\`\\`\\`\n[Stop immediately]\n\"Of course, what would you like me to clarify?\"\n\\`\\`\\`\n\n---\n\n# TIMING GUIDE\n\n| Question Type | Target Time |\n|--------------|-------------|\n| \"Tell me about yourself\" | 90 seconds |\n| STAR story | 2-3 minutes |\n| Follow-up question | 30-60 seconds |\n| \"Why Google?\" | 60 seconds |\n| Your questions | 5 minutes total |\n\n---\n\n# PRACTICE CHECKLIST\n\n- [ ] Record yourself answering each question\n- [ ] Time each answer\n- [ ] Watch recording - check for filler words (um, uh, like)\n- [ ] Practice with a friend for mock interview\n- [ ] Do one full mock interview (all questions) before tomorrow\n\n---\n\n**REMEMBER: You have REAL experience. Just tell YOUR stories with confidence!**\n"
  },
  {
    "id": "GOOGLE_INTERVIEW_PREP",
    "title": "STAR Stories",
    "category": "interview",
    "badge": null,
    "content": "# GOOGLE INTERVIEW PREPARATION\n## Googleyness & Hiring Manager Rounds\n\n---\n\n## YOUR PROJECT OWNERSHIP SUMMARY\n\n| Project | Your Role | Impact |\n|---------|-----------|--------|\n| **beat** | Core Developer | Built 73 data flows, 150+ workers, 15+ API integrations |\n| **stir** | Core Developer | Built 76 Airflow DAGs, 112 dbt models, ClickHouse pipelines |\n| **event-grpc** (ClickHouse sinker) | Implemented | Consumer→ClickHouse flush, buffered batch processing |\n| **fake_follower_analysis** | Solo Developer | End-to-end ML system, 10 Indic languages, AWS Lambda |\n\n---\n\n# PART 1: GOOGLEYNESS STORIES (STAR Format)\n\nGoogle evaluates: **Collaboration, Navigating Ambiguity, Pushing Back Respectfully, Helping Others, Learning from Failures, Bias to Action**\n\n---\n\n## STORY 1: Building a Highly Reliable Data Scraping Platform (beat)\n\n### Situation\nAt Good Creator Co., we needed a system to aggregate social media data (Instagram, YouTube) at scale for our influencer analytics platform. The challenge was handling 15+ external APIs with different rate limits, frequent failures, and the need to process 10M+ data points daily.\n\n### Task\nI was responsible for designing and building the entire data collection service from scratch - including the worker pool system, rate limiting, API integrations, and data processing flows.\n\n### Action\n1. **Designed worker pool architecture**: Created 73 configurable flows with multiprocessing workers + async concurrency\n   \\`\\`\\`python\n   # Each flow had configurable workers and concurrency\n   'refresh_profile_by_handle': {'no_of_workers': 10, 'no_of_concurrency': 5}\n   \\`\\`\\`\n\n2. **Built multi-level rate limiting**: Implemented Redis-backed stacked limiters (global daily 20K, per-minute 60, per-handle 1/sec)\n   \\`\\`\\`python\n   async with RateLimiter(rate_spec=global_limit_day):\n       async with RateLimiter(rate_spec=global_limit_minute):\n           async with RateLimiter(rate_spec=handle_limit):\n               result = await refresh_profile(handle)\n   \\`\\`\\`\n\n3. **Designed fallback strategy**: When primary APIs failed, system automatically rotated to secondary sources (6 Instagram APIs, 4 YouTube APIs)\n\n4. **Implemented credential rotation**: Built manager to disable credentials with TTL backoff when rate-limited\n\n### Result\n- **10M+ daily data points** processed reliably\n- **25% improvement** in API response times through optimization\n- **30% cost reduction** through intelligent rate limiting and caching\n- **150+ concurrent workers** running smoothly\n- System ran in production for 15+ months with minimal incidents\n\n### Googleyness Signals\n- **Bias to Action**: Built from scratch rather than waiting for perfect spec\n- **Navigating Ambiguity**: External APIs changed frequently, designed for adaptability\n- **Helping Others**: The platform enabled entire analytics team to deliver insights\n\n---\n\n## STORY 2: Designing the Data Platform (stir)\n\n### Situation\nOur analytics platform needed to compute influencer rankings, engagement metrics, and time-series data across billions of records. The existing manual SQL queries were slow, error-prone, and couldn't scale.\n\n### Task\nBuild an enterprise data platform that could:\n- Transform raw data into analytics-ready models\n- Sync data between ClickHouse (analytics) and PostgreSQL (application)\n- Run reliably with proper monitoring and error handling\n\n### Action\n1. **Chose Modern Data Stack**: Selected Airflow + dbt + ClickHouse after evaluating alternatives\n   - Airflow for orchestration (vs Prefect) - better community, more operators\n   - dbt for transformation (vs stored procedures) - version control, testing\n   - ClickHouse (vs BigQuery) - self-hosted, no egress costs\n\n2. **Designed 3-layer data flow**:\n   \\`\\`\\`\n   ClickHouse (analytics) → S3 (staging) → PostgreSQL (application)\n   \\`\\`\\`\n   This atomic swap pattern ensured zero-downtime updates\n\n3. **Built 76 DAGs with proper scheduling**:\n   - \\`*/5 min\\`: Real-time (dbt_recent_scl)\n   - \\`*/15 min\\`: Core metrics (dbt_core)\n   - \\`Daily 19:00\\`: Full refresh (dbt_daily)\n\n4. **Implemented incremental processing**:\n   \\`\\`\\`sql\n   {% if is_incremental() %}\n   WHERE created_at > (SELECT max(created_at) - INTERVAL 4 HOUR FROM {{ this }})\n   {% endif %}\n   \\`\\`\\`\n\n### Result\n- **50% reduction** in data latency\n- **76 production DAGs** running reliably\n- **112 dbt models** powering all analytics\n- **Billions of records** processed daily\n- Enabled multi-dimensional leaderboards (category, language, country rankings)\n\n### Googleyness Signals\n- **Collaboration**: Worked with data analysts to understand requirements\n- **Learning**: Learned dbt and ClickHouse specifically for this project\n- **Pushing Back**: Advocated for dbt over raw SQL despite initial resistance\n\n---\n\n## STORY 3: Real-Time Event Processing to ClickHouse (event-grpc)\n\n### Situation\nOur mobile and web apps generated thousands of events per second (user actions, clicks, purchases). These needed to be reliably stored in ClickHouse for real-time analytics.\n\n### Task\nI specifically owned the **consumer → ClickHouse** pipeline - the part that consumes messages from RabbitMQ and flushes them to ClickHouse reliably.\n\n### Action\n1. **Designed buffered sinker pattern** for high-volume events:\n   \\`\\`\\`go\n   func TraceLogEventsSinker(c chan interface{}) {\n       ticker := time.NewTicker(5 * time.Second)\n       batch := []model.TraceLogEvent{}\n\n       for {\n           select {\n           case event := <-c:\n               batch = append(batch, parseEvent(event))\n               if len(batch) >= 1000 {\n                   flushBatch(batch)\n                   batch = []model.TraceLogEvent{}\n               }\n           case <-ticker.C:\n               if len(batch) > 0 {\n                   flushBatch(batch)\n                   batch = []model.TraceLogEvent{}\n               }\n           }\n       }\n   }\n   \\`\\`\\`\n\n2. **Implemented retry logic with dead letter queue**:\n   - Max 2 retries before routing to error queue\n   - Preserved message metadata across retries\n\n3. **Built connection auto-recovery**:\n   \\`\\`\\`go\n   // 1-second cron to check and reconnect ClickHouse\n   func clickhouseConnectionCron(config config.Config) {\n       ticker := time.NewTicker(1 * time.Second)\n       for range ticker.C {\n           for dbName, db := range singletonClickhouseMap {\n               if db == nil || db.Error != nil {\n                   reconnect(dbName)\n               }\n           }\n       }\n   }\n   \\`\\`\\`\n\n4. **Created 26 different consumer configurations** for various event types\n\n### Result\n- **10,000+ events/second** processed reliably\n- **70+ concurrent workers** handling different event types\n- **Zero data loss** through buffered writes + retry logic\n- **18+ ClickHouse tables** populated with real-time data\n\n### Googleyness Signals\n- **Technical Excellence**: Designed for reliability and scale\n- **Ownership**: Took full responsibility for critical data path\n- **Bias to Action**: Proactively added monitoring and alerting\n\n---\n\n## STORY 4: ML-Powered Fake Follower Detection (fake_follower_analysis)\n\n### Situation\nInfluencer marketing campaigns were being affected by fake followers. Brands needed to verify that creators had genuine audiences. The challenge: detecting fakes among followers who could use any of 10+ Indian languages.\n\n### Task\nBuild an end-to-end ML system that could:\n- Detect fake followers with high accuracy\n- Handle 10 Indic scripts (Hindi, Bengali, Tamil, etc.)\n- Scale to millions of followers\n- Run cost-effectively on serverless infrastructure\n\n### Action\n1. **Designed ensemble detection model** with 5 independent features:\n   \\`\\`\\`python\n   # Feature 1: Non-Indic language detection (Greek, Chinese, Korean = FAKE)\n   # Feature 2: Digit count in handle (>4 digits = FAKE)\n   # Feature 3: Handle-name correlation (special chars but no match = FAKE)\n   # Feature 4: Fuzzy similarity score (RapidFuzz weighted)\n   # Feature 5: Indian name database match (35,183 names)\n   \\`\\`\\`\n\n2. **Built multi-language transliteration pipeline**:\n   - Integrated indictrans library with HMM models\n   - Custom Hindi processing with 24 vowel + 42 consonant mappings\n   - Symbol normalization for 13 Unicode variants\n\n3. **Designed AWS serverless architecture**:\n   \\`\\`\\`\n   ClickHouse → S3 → SQS → Lambda (ECR) → Kinesis → Output\n   \\`\\`\\`\n\n4. **Optimized for cost and performance**:\n   - Batch processing with 10,000 messages/batch\n   - 8 parallel workers using multiprocessing\n   - ON_DEMAND Kinesis for auto-scaling\n\n### Result\n- **Processes entire follower base** in minutes\n- **10 Indic scripts** supported seamlessly\n- **3 confidence levels** (0.0, 0.33, 1.0) for nuanced decisions\n- **35,183 name database** for validation\n- Enabled brands to make data-driven influencer selections\n\n### Googleyness Signals\n- **Innovation**: Combined NLP, ML, and cloud architecture creatively\n- **User Focus**: Understood brand needs and delivered actionable scores\n- **Technical Depth**: Deep dive into linguistics, Unicode, ML models\n\n---\n\n## STORY 5: Navigating Ambiguity - API Rate Limit Crisis (beat)\n\n### Situation\nOne day, Instagram Graph API started returning 429 (rate limit) errors at 10x the normal rate. Our data collection stopped. No warning from Facebook, no documentation about changes.\n\n### Task\nQuickly diagnose and fix the issue while maintaining data freshness for customers.\n\n### Action\n1. **Immediate investigation**: Analyzed patterns - found Facebook had silently reduced rate limits\n2. **Quick mitigation**: Reduced concurrent workers from 50 to 20 immediately\n3. **Medium-term fix**: Implemented credential rotation across multiple Facebook accounts\n4. **Long-term solution**: Built adaptive rate limiting that learns from 429 responses\n   \\`\\`\\`python\n   async def disable_creds(cred_id, disable_duration=3600):\n       # Disable credential with TTL backoff\n       await session.execute(\n           update(Credential)\n           .where(Credential.id == cred_id)\n           .values(\n               enabled=False,\n               disabled_till=func.now() + timedelta(seconds=disable_duration)\n           )\n       )\n   \\`\\`\\`\n\n### Result\n- **Restored data collection** within 2 hours\n- **Built resilient system** that handles future rate limit changes automatically\n- **Documented incident** and created runbook for team\n\n### Googleyness Signals\n- **Bias to Action**: Didn't wait for perfect info, acted immediately\n- **Learning**: Turned crisis into opportunity to build better system\n- **Helping Others**: Created documentation for future incidents\n\n---\n\n## STORY 6: Pushing Back Respectfully - Technology Choice (stir)\n\n### Situation\nWhen building the data platform, the team wanted to use a commercial ETL tool (Fivetran) for transformations. I believed dbt would be a better choice for our use case.\n\n### Task\nConvince stakeholders that open-source dbt was the right choice without creating conflict.\n\n### Action\n1. **Built a POC**: Created 5 example models in dbt showing the workflow\n2. **Presented trade-offs objectively**:\n   - Fivetran: Easier setup, but $500+/month, limited customization\n   - dbt: Learning curve, but free, full control, version-controlled\n3. **Addressed concerns**:\n   - \"Learning curve?\" → I'll create documentation and train the team\n   - \"Support?\" → Active community, extensive documentation\n4. **Offered compromise**: \"Let's try dbt for 2 weeks. If it doesn't work, we switch to Fivetran\"\n\n### Result\n- **dbt adopted** as primary transformation tool\n- **$6,000+/year saved** on licensing\n- **Team upskilled** in modern data stack\n- **112 models** built successfully using dbt\n\n### Googleyness Signals\n- **Pushing Back Respectfully**: Data-driven argument, not emotional\n- **Collaboration**: Offered to train team, shared responsibility\n- **User Focus**: Chose what was best for long-term success\n\n---\n\n## STORY 7: Helping Others - Mentoring Junior Engineer\n\n### Situation\nA junior engineer was struggling to debug a complex Airflow DAG failure. The error messages were cryptic, and they had been stuck for 2 days.\n\n### Task\nHelp them while teaching them how to debug such issues in the future.\n\n### Action\n1. **Didn't just fix it**: Sat with them and walked through the debugging process\n2. **Taught systematic approach**:\n   - Check Airflow logs (scheduler, worker)\n   - Identify which task failed\n   - Check task's Python code and dependencies\n   - Verify database connections and permissions\n3. **Found root cause together**: A ClickHouse connection timeout due to missing retry logic\n4. **Implemented fix together**: Added connection retry with exponential backoff\n5. **Created documentation**: Wrote a \"DAG Debugging Guide\" for the team\n\n### Result\n- **Junior engineer** solved future issues independently\n- **Debugging guide** used by entire team\n- **Reduced escalations** by 40%\n\n### Googleyness Signals\n- **Helping Others**: Invested time in teaching, not just doing\n- **Collaboration**: Made it a learning experience\n- **Documentation**: Created lasting value for team\n\n---\n\n# PART 2: HIRING MANAGER DEEP DIVE QUESTIONS\n\n---\n\n## Technical Deep Dive: beat Project\n\n### Q: \"Walk me through the architecture of beat\"\n**Answer:**\n\\`\\`\\`\nClient (coffee API) → FastAPI REST API → SQL-based task queue\n                                              ↓\n                         Worker Pool (73 flows × N workers each)\n                                              ↓\n                         Rate Limiter (Redis-backed, stacked limits)\n                                              ↓\n                         15+ APIs (Instagram Graph, RapidAPI, YouTube)\n                                              ↓\n                         3-Stage Pipeline: Retrieval → Parsing → Processing\n                                              ↓\n                         PostgreSQL (transactional) + RabbitMQ (events)\n\\`\\`\\`\n\nKey design decisions:\n1. **SQL-based task queue** instead of Celery - simpler, no additional infrastructure\n2. **Stacked rate limiters** - per-handle, per-minute, per-day for fine-grained control\n3. **Interface-based API integrations** - easy to add/swap API providers\n\n### Q: \"How did you handle 10M+ daily data points?\"\n**Answer:**\n1. **Parallel processing**: 150+ workers with semaphore-based concurrency\n2. **Async I/O**: uvloop + aio-pika + asyncpg for non-blocking operations\n3. **Batch operations**: Grouped similar tasks, batch database writes\n4. **Intelligent caching**: Redis for rate limit state, API responses\n5. **Priority queuing**: Important profiles processed first\n\n### Q: \"What would you do differently if building it again?\"\n**Answer:**\n1. **Replace SQL task queue with Redis Streams** - better for high-throughput\n2. **Add distributed tracing** - easier debugging across services\n3. **Use structured logging** - better for alerting and analysis\n4. **Add circuit breakers per API** - isolate failures better\n\n---\n\n## Technical Deep Dive: stir Project\n\n### Q: \"Why did you choose Airflow + dbt + ClickHouse?\"\n**Answer:**\n\n| Technology | Why Chosen | Alternatives Considered |\n|------------|------------|------------------------|\n| **Airflow** | Python-native, huge ecosystem, 50+ operators | Prefect (newer, smaller community) |\n| **dbt** | SQL transformations, version control, testing | Stored procedures (no versioning) |\n| **ClickHouse** | OLAP performance, columnar storage, free | BigQuery (egress costs), Snowflake (expensive) |\n\nThe combination allowed:\n- **Airflow**: Scheduling, monitoring, retries, dependencies\n- **dbt**: Modular SQL, incremental processing, documentation\n- **ClickHouse**: Sub-second queries on billions of rows\n\n### Q: \"Explain the 3-layer data flow\"\n**Answer:**\n\\`\\`\\`\nLAYER 1: ClickHouse (Analytics)\n   - dbt models run here\n   - Fast OLAP queries\n   - ReplacingMergeTree for efficient upserts\n         ↓\n   INSERT INTO FUNCTION s3('bucket/file.json')\n         ↓\nLAYER 2: S3 (Staging)\n   - Intermediate storage\n   - Decouples systems\n   - Allows retry without re-processing\n         ↓\n   aws s3 cp + COPY command\n         ↓\nLAYER 3: PostgreSQL (Application)\n   - Powers REST APIs\n   - JSONB parsing for flexibility\n   - Atomic table swap (RENAME) for zero-downtime\n\\`\\`\\`\n\nWhy this pattern?\n1. **No direct connection needed** between ClickHouse and PostgreSQL\n2. **Atomic updates** - application sees old data until new is ready\n3. **Easy debugging** - S3 files can be inspected\n\n### Q: \"How did you handle incremental processing?\"\n**Answer:**\n\\`\\`\\`sql\n{{ config(\n    materialized='incremental',\n    engine='ReplacingMergeTree(updated_at)',\n    order_by='(profile_id, date)'\n) }}\n\n{% if is_incremental() %}\nWHERE created_at > (\n    SELECT max(created_at) - INTERVAL 4 HOUR  -- Safety buffer\n    FROM {{ this }}\n)\n{% endif %}\n\\`\\`\\`\n\nThe 4-hour lookback handles:\n- Late-arriving data\n- Failed task retries\n- Clock drift between systems\n\n---\n\n## Technical Deep Dive: fake_follower_analysis\n\n### Q: \"How does your ML model detect fake followers?\"\n**Answer:**\n5-feature ensemble:\n\n| Feature | Logic | Weight |\n|---------|-------|--------|\n| **Non-Indic Script** | Greek/Chinese/Korean = FAKE | 1.0 (definite) |\n| **Digit Count** | >4 digits in handle = FAKE | 1.0 (definite) |\n| **Handle-Name Correlation** | Special chars but no match = FAKE | 1.0 (definite) |\n| **Fuzzy Similarity** | 0-40% similarity = weak FAKE | 0.33 (weak) |\n| **Indian Name Match** | <80% match to 35K names = suspicious | supplementary |\n\nDecision tree:\n\\`\\`\\`python\nif non_indic_language: return 1.0  # FAKE\nif digits > 4: return 1.0  # FAKE\nif special_chars and similarity < 80: return 1.0  # FAKE\nif similarity < 40: return 0.33  # Weak signal\nreturn 0.0  # REAL\n\\`\\`\\`\n\n### Q: \"How did you handle 10 different Indian languages?\"\n**Answer:**\n1. **Character-to-language mapping**: Each script has unique Unicode ranges\n   - Hindi: 0900-097F\n   - Bengali: 0980-09FF\n   - Tamil: 0B80-0BFF\n\n2. **HMM-based transliteration**: Pre-trained models for each language pair\n   \\`\\`\\`python\n   trn = Transliterator(source='hin', target='eng', decode='viterbi')\n   english_name = trn.transform(\"राहुल\")  # \"Rahul\"\n   \\`\\`\\`\n\n3. **Custom Hindi processing**: 24 vowel + 42 consonant manual mappings for accuracy\n\n4. **Fallback chain**: Indic script → ML transliteration → unidecode → ASCII\n\n### Q: \"Why AWS Lambda over EC2?\"\n**Answer:**\n| Factor | Lambda | EC2 |\n|--------|--------|-----|\n| **Cost** | Pay per invocation | Pay always |\n| **Scaling** | Automatic | Manual setup |\n| **Maintenance** | None | OS, security patches |\n| **Cold start** | ~2s (acceptable for batch) | None |\n\nFor batch processing of followers (not real-time), Lambda was:\n- **90% cheaper** than running EC2 24/7\n- **Zero operational overhead**\n- **Auto-scales** with SQS queue depth\n\n---\n\n# PART 3: BEHAVIORAL QUESTIONS\n\n## \"Tell me about a time you failed\"\n\n**Story**: GPT Integration Timeout Issue\n\n**Situation**: Added OpenAI GPT integration to beat for data enrichment. In testing, it worked great.\n\n**What went wrong**: In production, 30% of requests timed out. GPT API had variable latency that I didn't account for.\n\n**What I learned**:\n1. Always load-test with realistic conditions\n2. Implement timeouts and fallbacks for external services\n3. Make features degradable - system should work without optional enrichments\n\n**What I did**:\n1. Added 30-second timeout with retry\n2. Made GPT enrichment asynchronous (separate worker)\n3. System works without GPT data, enriches later\n\n---\n\n## \"Describe a conflict with a teammate\"\n\n**Story**: Database Choice Disagreement\n\n**Situation**: A senior engineer wanted to use MongoDB for the analytics platform. I believed ClickHouse was better for our OLAP workload.\n\n**How I handled it**:\n1. **Listened first**: Understood their reasons (familiar with Mongo, document flexibility)\n2. **Proposed experiment**: \"Let's benchmark both with our actual queries\"\n3. **Shared results objectively**: ClickHouse was 50x faster for aggregation queries\n4. **Acknowledged trade-offs**: \"You're right about flexibility, but performance wins here\"\n\n**Outcome**: We went with ClickHouse. Senior engineer became an advocate after seeing performance.\n\n---\n\n## \"How do you prioritize tasks?\"\n\n**Framework I use**:\n1. **Impact vs Effort matrix**: High impact, low effort first\n2. **Dependencies**: Unblock others before personal tasks\n3. **Deadlines**: Customer-facing deadlines are non-negotiable\n4. **Technical debt**: Allocate 20% time to pay down debt\n\n**Example from stir**:\n- Had to choose between new leaderboard feature vs. DAG reliability improvements\n- Chose reliability first because failing DAGs blocked entire team\n- Delivered leaderboard 1 week later, but with stable foundation\n\n---\n\n## \"Why Google?\"\n\n**Honest answer**:\n1. **Scale**: Want to work on systems serving billions of users\n2. **Learning**: Google's engineering culture is legendary\n3. **Impact**: Build tools used by developers worldwide\n4. **Growth**: Learn from the best engineers in the industry\n\n**What I bring**:\n1. End-to-end ownership experience (built complete systems)\n2. Both Python and Go expertise\n3. Data engineering + backend + ML breadth\n4. Startup scrappiness (bias to action, do more with less)\n\n---\n\n# PART 4: QUESTIONS TO ASK\n\n## For Hiring Manager\n1. \"What does success look like in the first 6 months?\"\n2. \"What are the biggest technical challenges the team is facing?\"\n3. \"How do you balance feature work vs. technical debt?\"\n4. \"What's the team's approach to on-call and incident response?\"\n\n## For Googleyness\n1. \"Can you share an example of how the team navigated ambiguity recently?\"\n2. \"How does the team handle disagreements on technical decisions?\"\n3. \"What opportunities are there for cross-team collaboration?\"\n4. \"How does Google support continuous learning?\"\n\n---\n\n# PART 5: METRICS CHEAT SHEET\n\n## beat\n- **10M+ daily data points** processed\n- **73 flows**, **150+ workers**\n- **15+ API integrations**\n- **25% faster** API response times\n- **30% cost reduction**\n\n## stir\n- **76 Airflow DAGs**\n- **112 dbt models** (29 staging + 83 marts)\n- **50% data latency reduction**\n- **Billions of records** processed\n- **1,476 git commits** (mature project)\n\n## event-grpc (your part)\n- **10,000+ events/second**\n- **26 consumer queues**\n- **70+ concurrent workers**\n- **18+ ClickHouse tables**\n- **Buffered batch writes** (1000 events/batch)\n\n## fake_follower_analysis\n- **10 Indic scripts** supported\n- **35,183 name database**\n- **5-feature ensemble model**\n- **AWS Lambda** serverless\n- **3 confidence levels** (0.0, 0.33, 1.0)\n\n---\n\n# PART 6: TECHNICAL TERMS TO KNOW\n\n| Term | What You Built | How to Explain |\n|------|----------------|----------------|\n| **Worker Pool** | beat main.py | Multiprocessing + async semaphores for concurrency control |\n| **Rate Limiting** | beat server.py | Redis-backed stacked limiters (daily, per-min, per-resource) |\n| **ELT** | stir | Extract-Load-Transform - load raw data first, transform in warehouse |\n| **Incremental Processing** | stir dbt models | Only process new/changed data, not full table |\n| **Buffered Sinker** | event-grpc | Batch events in memory, flush periodically for efficiency |\n| **HMM Transliteration** | fake_follower | Hidden Markov Model for character sequence conversion |\n| **Ensemble Model** | fake_follower | Combine multiple weak classifiers for robust prediction |\n\n---\n\n# PART 7: RESUME ↔ PROJECT MAPPING\n\nYour resume says → Proof from projects:\n\n| Resume Bullet | Project Evidence |\n|--------------|------------------|\n| \"Built a high-performance logging system with RabbitMQ, Python and Golang, transitioning to ClickHouse, achieving a 2.5x reduction in log retrieval times\" | **event-grpc**: Consumer→ClickHouse sinker, buffered batch writes |\n| \"Crafted and streamlined ETL data pipelines (Apache Airflow) for batch data ingestion\" | **stir**: 76 DAGs, dbt transformations, ClickHouse→S3→PostgreSQL flow |\n| \"Designed an asynchronous data processing system handling 10M+ daily data points\" | **beat**: 73 flows, 150+ workers, async Python with uvloop |\n| \"Optimized API response times by 25%\" | **beat**: Rate limiting, caching, credential rotation |\n| \"Reduced operational costs by 30%\" | **beat**: Intelligent rate limiting reduced API costs |\n\n---\n\n*Good luck with your Google interview! Remember: Be specific, use numbers, and show ownership.*\n"
  },
  {
    "id": "GOOGLE_INTERVIEW_DETAILED",
    "title": "Detailed Breakdown",
    "category": "interview",
    "badge": null,
    "content": "# GOOGLE INTERVIEW - DETAILED COMPONENT-WISE BREAKDOWN\n\n---\n\n# PROJECT 1: BEAT (Data Aggregation Service)\n\n## BEAT has 12 Major Components You Built:\n\n| # | Component | Lines of Code | Complexity | Interview Story Potential |\n|---|-----------|---------------|------------|---------------------------|\n| 1 | Worker Pool System | main.py (14KB) | High | System Design |\n| 2 | SQL-Based Task Queue | core/flows/ | Medium | Distributed Systems |\n| 3 | Multi-Level Rate Limiting | server.py, utils/request.py | High | Scalability |\n| 4 | API Integration Framework | instagram/functions/retriever/ | High | Design Patterns |\n| 5 | Credential Management | credentials/ | Medium | Security |\n| 6 | 3-Stage Data Pipeline | tasks/ (retrieval→parsing→processing) | High | Data Engineering |\n| 7 | GPT/OpenAI Integration | gpt/ | Medium | AI/ML |\n| 8 | RabbitMQ/AMQP Listeners | core/amqp/ | Medium | Event-Driven |\n| 9 | Asset Upload System | main_assets.py, client/ | Medium | CDN/Storage |\n| 10 | Engagement Calculations | instagram/helper.py | Low | Analytics |\n| 11 | FastAPI REST API | server.py (43KB) | Medium | API Design |\n| 12 | Graceful Deployment | scripts/start.sh | Low | DevOps |\n\n---\n\n## COMPONENT 1: Worker Pool System (main.py)\n\n### What You Built\nA distributed worker pool system that spawns multiple processes, each running async event loops with semaphore-based concurrency control.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\n# Architecture: Multiprocessing + Asyncio + Semaphore\n\ndef main():\n    \"\"\"Entry point - spawns 150+ workers across 73 flows\"\"\"\n    for flow_name, config in _whitelist.items():\n        for i in range(config['no_of_workers']):\n            process = multiprocessing.Process(\n                target=looper,\n                args=(flow_name, config['no_of_concurrency'])\n            )\n            process.start()\n            workers.append(process)\n\ndef looper(flow_name: str, concurrency: int):\n    \"\"\"Each process has its own async event loop\"\"\"\n    uvloop.install()  # 2-4x faster than default asyncio\n    asyncio.run(poller(flow_name, concurrency))\n\nasync def poller(flow_name: str, concurrency: int):\n    \"\"\"Async polling with semaphore-based concurrency\"\"\"\n    semaphore = asyncio.Semaphore(concurrency)\n\n    while True:\n        task = await poll(flow_name)  # SQL-based queue\n        if task:\n            asyncio.create_task(perform_task(task, semaphore))\n        await asyncio.sleep(0.1)  # Prevent busy-waiting\n\nasync def perform_task(task, semaphore):\n    \"\"\"Execute with concurrency control + timeout\"\"\"\n    async with semaphore:\n        try:\n            async with asyncio.timeout(600):  # 10-min timeout\n                result = await execute(task.flow, task.params)\n                await update_task_status(task.id, 'COMPLETE', result)\n        except asyncio.TimeoutError:\n            await update_task_status(task.id, 'TIMEOUT')\n        except Exception as e:\n            await update_task_status(task.id, 'FAILED', str(e))\n\\`\\`\\`\n\n### Why This Design?\n\n| Decision | Why | Alternative Considered |\n|----------|-----|----------------------|\n| **Multiprocessing** | Python GIL limits CPU-bound parallelism | Threads (blocked by GIL) |\n| **Asyncio inside each process** | I/O-bound work (API calls, DB) benefits from async | Sync requests (slow) |\n| **Semaphore** | Prevent overwhelming external APIs | No limit (429 errors) |\n| **uvloop** | 2-4x faster event loop | Default asyncio (slower) |\n| **SQL polling** | Simple, no extra infrastructure | Celery (complex setup) |\n\n### Interview STAR Story\n\n**Situation**: We needed to collect data from 10M+ social media profiles daily, but external APIs had strict rate limits.\n\n**Task**: Design a system that maximizes throughput while respecting rate limits and handling failures gracefully.\n\n**Action**:\n1. Designed hybrid architecture: Multiprocessing for parallelism + Asyncio for I/O concurrency\n2. Used semaphores to control per-flow concurrency (e.g., 5 concurrent API calls per worker)\n3. Implemented 10-minute timeout to prevent stuck tasks\n4. Added graceful error handling with automatic retry via task queue\n\n**Result**:\n- **150+ workers** running concurrently\n- **10M+ daily data points** processed\n- **99.9% uptime** with automatic recovery\n- **25% faster** than previous sync implementation\n\n### Questions They Might Ask\n\n**Q: Why not use Celery?**\nA: Celery adds complexity (Redis/RabbitMQ broker, beat scheduler, multiple processes). For our use case, a simple SQL-based queue was sufficient and easier to debug. We already had PostgreSQL, so no new infrastructure needed.\n\n**Q: How do you handle worker crashes?**\nA: Tasks remain in \\`PROCESSING\\` status. A separate cleanup cron resets tasks stuck in \\`PROCESSING\\` for >15 minutes back to \\`PENDING\\`. The task's \\`retry_count\\` is incremented.\n\n**Q: Why 10-minute timeout?**\nA: Some flows (like fetching 1000 followers with pagination) legitimately take 5-8 minutes. 10 minutes gives buffer while catching truly stuck tasks.\n\n---\n\n## COMPONENT 2: SQL-Based Task Queue\n\n### What You Built\nA distributed task queue using PostgreSQL with \\`FOR UPDATE SKIP LOCKED\\` for concurrent worker coordination.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\nasync def poll(flow_name: str) -> Optional[ScrapeRequestLog]:\n    \"\"\"\n    Atomic task pickup with row-level locking\n\n    Key SQL features:\n    - FOR UPDATE: Lock the row to prevent double-pickup\n    - SKIP LOCKED: Don't wait, skip to next available row\n    - Priority ordering: High-priority tasks first\n    - Expiry check: Skip expired tasks\n    \"\"\"\n    query = \"\"\"\n        UPDATE scrape_request_log\n        SET status = 'PROCESSING', picked_at = NOW()\n        WHERE id = (\n            SELECT id FROM scrape_request_log\n            WHERE flow = :flow\n              AND status = 'PENDING'\n              AND (expires_at IS NULL OR expires_at > NOW())\n            ORDER BY priority DESC, created_at ASC\n            FOR UPDATE SKIP LOCKED\n            LIMIT 1\n        )\n        RETURNING *\n    \"\"\"\n    return await session.execute(query, {'flow': flow_name})\n\\`\\`\\`\n\n### Schema Design\n\n\\`\\`\\`sql\nCREATE TABLE scrape_request_log (\n    id BIGSERIAL PRIMARY KEY,\n    idempotency_key VARCHAR(255) UNIQUE,  -- Prevent duplicate tasks\n    platform VARCHAR(50),                  -- INSTAGRAM, YOUTUBE, SHOPIFY\n    flow VARCHAR(100),                     -- 73 flow types\n    status VARCHAR(20) DEFAULT 'PENDING',  -- PENDING, PROCESSING, COMPLETE, FAILED\n    params JSONB,                          -- Flow-specific parameters\n    data TEXT,                             -- Result or error message\n    priority INTEGER DEFAULT 1,            -- Higher = processed first\n    retry_count INTEGER DEFAULT 0,\n    account_id VARCHAR(100),               -- For grouping/filtering\n    created_at TIMESTAMP DEFAULT NOW(),\n    picked_at TIMESTAMP,\n    scraped_at TIMESTAMP,\n    expires_at TIMESTAMP                   -- Auto-skip if expired\n);\n\n-- Indexes for performance\nCREATE INDEX idx_flow_status ON scrape_request_log(flow, status);\nCREATE INDEX idx_priority_created ON scrape_request_log(priority DESC, created_at ASC);\n\\`\\`\\`\n\n### Why This Design?\n\n| Feature | Purpose |\n|---------|---------|\n| **FOR UPDATE SKIP LOCKED** | Multiple workers can poll simultaneously without blocking |\n| **idempotency_key** | Prevent duplicate task creation (e.g., same profile scraped twice) |\n| **priority** | Business-critical profiles processed first |\n| **expires_at** | Don't process stale requests |\n| **JSONB params** | Flexible flow-specific parameters |\n\n### Interview STAR Story\n\n**Situation**: Needed a task queue for 73 different flows with 150+ workers, but Celery felt too heavy.\n\n**Task**: Build a lightweight, reliable task queue using existing PostgreSQL.\n\n**Action**:\n1. Designed schema with proper indexes for fast polling\n2. Used \\`FOR UPDATE SKIP LOCKED\\` for concurrent-safe task pickup\n3. Added idempotency keys to prevent duplicate tasks\n4. Implemented priority-based ordering for business-critical tasks\n5. Created cleanup cron for stuck tasks\n\n**Result**:\n- **Sub-millisecond** task pickup latency\n- **Zero duplicate** task processing\n- **No additional infrastructure** needed\n- **Easy debugging** - just query the table\n\n### Questions They Might Ask\n\n**Q: What's the throughput of this queue?**\nA: With proper indexes, we achieved ~1000 task pickups/second. The bottleneck was API rate limits, not the queue.\n\n**Q: How do you handle task failures?**\nA: Failed tasks get \\`status='FAILED'\\` with error in \\`data\\` column. A separate process can retry failed tasks or alert on-call.\n\n**Q: Why not Redis-based queue?**\nA: PostgreSQL was already our primary datastore. Adding Redis would mean:\n- Another service to manage\n- Data consistency issues (task in Redis, result in Postgres)\n- For our scale (~1000 tasks/sec), Postgres was sufficient\n\n---\n\n## COMPONENT 3: Multi-Level Rate Limiting\n\n### What You Built\nA stacked rate limiting system using Redis that enforces limits at multiple levels simultaneously.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\n# 3-Level Stacked Rate Limiting\n\nfrom asyncio_redis_rate_limit import RateLimiter, RateSpec\n\n# Configuration per API source\nRATE_LIMITS = {\n    'graphapi': RateSpec(requests=200, seconds=3600),      # 200/hour\n    'youtube138': RateSpec(requests=850, seconds=60),      # 850/minute\n    'insta-best-performance': RateSpec(requests=2, seconds=1),  # 2/second\n    'arraybobo': RateSpec(requests=100, seconds=30),       # 100/30sec\n    'rocketapi': RateSpec(requests=100, seconds=30),\n}\n\n# Global limits\nGLOBAL_DAILY = RateSpec(requests=20000, seconds=86400)\nGLOBAL_MINUTE = RateSpec(requests=60, seconds=60)\n\nasync def rate_limited_request(handle: str, source: str):\n    \"\"\"\n    Stacked limiters - ALL must pass before request proceeds\n\n    Level 1: Global daily (20K/day) - prevent runaway costs\n    Level 2: Global per-minute (60/min) - smooth traffic\n    Level 3: Per-handle (1/sec) - prevent hammering same profile\n    Level 4: Per-source (varies) - respect API-specific limits\n    \"\"\"\n    redis = AsyncRedis.from_url(REDIS_URL)\n\n    async with RateLimiter(\n        unique_key=\"beat_global_daily\",\n        backend=redis,\n        rate_spec=GLOBAL_DAILY\n    ):\n        async with RateLimiter(\n            unique_key=\"beat_global_minute\",\n            backend=redis,\n            rate_spec=GLOBAL_MINUTE\n        ):\n            async with RateLimiter(\n                unique_key=f\"beat_handle_{handle}\",\n                backend=redis,\n                rate_spec=RateSpec(requests=1, seconds=1)\n            ):\n                async with RateLimiter(\n                    unique_key=f\"beat_source_{source}\",\n                    backend=redis,\n                    rate_spec=RATE_LIMITS[source]\n                ):\n                    return await make_api_call(handle, source)\n\\`\\`\\`\n\n### Redis Data Structure\n\n\\`\\`\\`\n# Sliding window counter pattern\nKey: beat_server_beat_global_daily\nValue: {\n    \"count\": 15234,\n    \"window_start\": 1706745600\n}\n\nKey: beat_server_beat_handle_virat.kohli\nValue: {\n    \"count\": 1,\n    \"window_start\": 1706832000\n}\n\\`\\`\\`\n\n### Why This Design?\n\n| Level | Purpose | Limit |\n|-------|---------|-------|\n| **Global Daily** | Cost control - don't exceed API budget | 20K/day |\n| **Global Minute** | Traffic smoothing - prevent bursts | 60/min |\n| **Per-Handle** | Prevent hammering same profile | 1/sec |\n| **Per-Source** | Respect each API's specific limits | Varies |\n\n### Interview STAR Story\n\n**Situation**: External APIs (Instagram, YouTube) have strict rate limits. Exceeding them results in 429 errors, temporary bans, or permanent API key revocation.\n\n**Task**: Implement rate limiting that respects all API limits while maximizing throughput.\n\n**Action**:\n1. Analyzed each API's rate limit documentation\n2. Implemented stacked limiters - request must pass ALL levels\n3. Used Redis for distributed state (multiple workers share limits)\n4. Added per-source configuration for easy adjustment\n5. Implemented automatic backoff on 429 responses\n\n**Result**:\n- **Zero API bans** since implementation\n- **30% cost reduction** by staying within quota\n- **Smooth traffic** distribution throughout the day\n- **Easy tuning** - just update config dict\n\n### Questions They Might Ask\n\n**Q: What happens when rate limit is exceeded?**\nA: The \\`RateLimiter\\` context manager blocks (async wait) until the window resets. Alternatively, we can raise an exception and retry later.\n\n**Q: How do you handle different time windows?**\nA: Redis sliding window algorithm. Each key stores count + window_start. On request:\n1. If current_time > window_start + window_size: reset count\n2. If count < limit: increment and proceed\n3. Else: wait or reject\n\n**Q: What if Redis goes down?**\nA: Graceful degradation - we fall back to in-memory rate limiting per worker. Less accurate, but prevents complete failure.\n\n---\n\n## COMPONENT 4: API Integration Framework (Strategy Pattern)\n\n### What You Built\nA pluggable API integration framework using the Strategy pattern, allowing easy addition of new data sources.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\n# Interface Definition\nclass InstagramCrawlerInterface(ABC):\n    \"\"\"Abstract interface for Instagram data retrieval\"\"\"\n\n    @abstractmethod\n    async def fetch_profile_by_handle(self, handle: str) -> dict:\n        pass\n\n    @abstractmethod\n    async def fetch_profile_posts_by_handle(self, handle: str, limit: int) -> list:\n        pass\n\n    @abstractmethod\n    async def fetch_post_by_shortcode(self, shortcode: str) -> dict:\n        pass\n\n    @abstractmethod\n    async def fetch_post_insights(self, post_id: str) -> dict:\n        pass\n\n    @abstractmethod\n    async def fetch_followers(self, user_id: str, cursor: str) -> Tuple[list, str]:\n        pass\n\n\n# Implementation 1: Facebook Graph API (Official)\nclass GraphApi(InstagramCrawlerInterface):\n    BASE_URL = \"https://graph.facebook.com/v15.0\"\n\n    async def fetch_profile_by_handle(self, handle: str) -> dict:\n        fields = \"biography,followers_count,follows_count,media_count,...\"\n        url = f\"{self.BASE_URL}/{self.user_id}?fields=business_discovery.username({handle}){{{fields}}}\"\n        return await self._request(url)\n\n\n# Implementation 2: RapidAPI IGData\nclass RapidApiIGData(InstagramCrawlerInterface):\n    BASE_URL = \"https://instagram-data1.p.rapidapi.com\"\n\n    async def fetch_profile_by_handle(self, handle: str) -> dict:\n        url = f\"{self.BASE_URL}/user/info?username={handle}\"\n        headers = {\"X-RapidAPI-Key\": self.api_key}\n        return await self._request(url, headers)\n\n\n# Implementation 3: Lama API (Fallback)\nclass LamaApi(InstagramCrawlerInterface):\n    # ... minimal implementation for fallback\n\n\n# Factory/Selector\ndef get_crawler(source: str) -> InstagramCrawlerInterface:\n    crawlers = {\n        'graphapi': GraphApi,\n        'rapidapi-igdata': RapidApiIGData,\n        'rapidapi-jotucker': RapidApiJoTucker,\n        'rapidapi-neotank': RapidApiNeoTank,\n        'rapidapi-arraybobo': RapidApiArrayBobo,\n        'lama': LamaApi,\n    }\n    return crawlers[source]()\n\\`\\`\\`\n\n### Fallback Strategy\n\n\\`\\`\\`python\nasync def fetch_profile_with_fallback(handle: str) -> dict:\n    \"\"\"\n    Try sources in order of reliability/cost\n    1. GraphAPI (official, best data quality)\n    2. RapidAPI options (paid, good quality)\n    3. Lama (free, limited data)\n    \"\"\"\n    sources = ['graphapi', 'rapidapi-igdata', 'rapidapi-jotucker', 'lama']\n\n    for source in sources:\n        try:\n            crawler = get_crawler(source)\n            cred = await credential_manager.get_enabled_cred(source)\n            if not cred:\n                continue  # No available credentials\n\n            crawler.set_credentials(cred)\n            result = await crawler.fetch_profile_by_handle(handle)\n\n            if result:\n                return result\n\n        except RateLimitError:\n            # Disable this credential temporarily\n            await credential_manager.disable_creds(cred.id, 3600)\n            continue\n\n        except Exception as e:\n            logger.error(f\"Source {source} failed: {e}\")\n            continue\n\n    raise AllSourcesFailedError(f\"Could not fetch {handle}\")\n\\`\\`\\`\n\n### Why This Design?\n\n| Pattern | Benefit |\n|---------|---------|\n| **Strategy Pattern** | Easy to add new APIs without changing core logic |\n| **Interface** | All crawlers have same method signatures |\n| **Factory** | Single point of crawler instantiation |\n| **Fallback Chain** | Reliability - if one fails, try next |\n\n### Interview STAR Story\n\n**Situation**: We needed to fetch Instagram data, but no single API was reliable enough. GraphAPI requires business account connection, RapidAPI has rate limits, etc.\n\n**Task**: Design a system that can use multiple data sources with easy fallback.\n\n**Action**:\n1. Defined abstract interface with all required methods\n2. Implemented 6 different API integrations following the interface\n3. Created factory function for crawler selection\n4. Built fallback chain that tries sources in priority order\n5. Integrated with credential manager for API key rotation\n\n**Result**:\n- **6 Instagram APIs** integrated seamlessly\n- **99.5% success rate** with fallback chain\n- **New API integration** takes ~2 hours (just implement interface)\n- **Clean separation** between API logic and business logic\n\n### Questions They Might Ask\n\n**Q: How do you decide source priority?**\nA: Based on data quality, cost, and reliability:\n1. GraphAPI - Best quality, free (but limited to business accounts)\n2. RapidAPI IGData - Good quality, $50/month\n3. RapidAPI JoTucker - Good, cheaper\n4. Lama - Free but limited fields\n\n**Q: What if all sources fail?**\nA: Task marked as FAILED. A separate alerting system notifies on-call if failure rate exceeds threshold (>5% in 1 hour).\n\n**Q: How do you handle different response formats?**\nA: Each crawler has a \\`_parser.py\\` file that normalizes responses to our internal schema. The interface guarantees output format.\n\n---\n\n## COMPONENT 5: Credential Management System\n\n### What You Built\nA credential lifecycle management system with automatic rotation, validation, and TTL-based backoff.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\n# credentials/manager.py\n\nclass CredentialManager:\n    \"\"\"\n    Manages API credentials across multiple sources\n\n    Features:\n    - Upsert with idempotency\n    - TTL-based disable (rate limit backoff)\n    - Random selection for load balancing\n    - Automatic re-enable after TTL\n    \"\"\"\n\n    async def insert_creds(self, source: str, credentials: dict,\n                          handle: str = None) -> Credential:\n        \"\"\"Upsert credential with idempotency\"\"\"\n        key = f\"{source}:{credentials.get('user_id', credentials.get('key'))}\"\n\n        return await get_or_create(\n            session,\n            Credential,\n            idempotency_key=key,\n            defaults={\n                'source': source,\n                'credentials': credentials,\n                'handle': handle,\n                'enabled': True\n            }\n        )\n\n    async def disable_creds(self, cred_id: int,\n                           disable_duration: int = 3600) -> None:\n        \"\"\"\n        Disable credential with TTL\n        Used when API returns 429 (rate limit) or 401 (token expired)\n        \"\"\"\n        await session.execute(\n            update(Credential)\n            .where(Credential.id == cred_id)\n            .values(\n                enabled=False,\n                disabled_till=func.now() + timedelta(seconds=disable_duration)\n            )\n        )\n\n    async def get_enabled_cred(self, source: str) -> Optional[Credential]:\n        \"\"\"\n        Get random enabled credential for load balancing\n\n        Checks:\n        1. Source matches\n        2. enabled=True\n        3. Either no disabled_till OR disabled_till has passed\n        \"\"\"\n        creds = await session.execute(\n            select(Credential)\n            .where(Credential.source == source)\n            .where(Credential.enabled == True)\n            .where(\n                or_(\n                    Credential.disabled_till.is_(None),\n                    Credential.disabled_till < func.now()\n                )\n            )\n        )\n        enabled_creds = creds.scalars().all()\n        return random.choice(enabled_creds) if enabled_creds else None\n\n\n# credentials/validator.py\n\nREQUIRED_SCOPES = [\n    'instagram_basic',\n    'instagram_manage_insights',\n    'pages_read_engagement',\n    'pages_show_list'\n]\n\nclass CredentialValidator:\n    \"\"\"Validates API credentials before use\"\"\"\n\n    async def validate(self, cred: Credential) -> ValidationResult:\n        if cred.source == 'graphapi':\n            return await self._validate_graphapi(cred)\n        elif cred.source == 'ytapi':\n            return await self._validate_youtube(cred)\n        # ... other sources\n\n    async def _validate_graphapi(self, cred: Credential) -> ValidationResult:\n        \"\"\"\n        Validate Facebook Graph API token\n\n        Checks:\n        1. Token is valid (not expired)\n        2. Has required scopes\n        3. Data access hasn't expired\n        \"\"\"\n        token = cred.credentials.get('token')\n        url = f\"https://graph.facebook.com/debug_token?input_token={token}\"\n        response = await self._request(url)\n\n        if not response['data']['is_valid']:\n            raise TokenInvalidError()\n\n        scopes = response['data']['scopes']\n        missing = [s for s in REQUIRED_SCOPES if s not in scopes]\n        if missing:\n            raise MissingScopesError(missing)\n\n        if response['data'].get('data_access_expires_at', 0) < time.time():\n            raise DataAccessExpiredError()\n\n        return ValidationResult(valid=True, scopes=scopes)\n\\`\\`\\`\n\n### Schema\n\n\\`\\`\\`sql\nCREATE TABLE credential (\n    id BIGSERIAL PRIMARY KEY,\n    idempotency_key VARCHAR(255) UNIQUE,\n    source VARCHAR(50),              -- graphapi, ytapi, rapidapi-*\n    credentials JSONB,               -- {token, user_id, api_key, ...}\n    handle VARCHAR(100),             -- Associated Instagram handle\n    enabled BOOLEAN DEFAULT TRUE,\n    data_access_expired BOOLEAN DEFAULT FALSE,\n    disabled_till TIMESTAMP,         -- TTL for temporary disable\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP\n);\n\\`\\`\\`\n\n### Interview STAR Story\n\n**Situation**: We had 20+ API credentials across 6 sources. When one hit rate limit, we needed to automatically use another. Manual rotation was error-prone.\n\n**Task**: Build a credential management system with automatic rotation and TTL-based backoff.\n\n**Action**:\n1. Designed schema with enable/disable flags and TTL\n2. Implemented random selection for load balancing across credentials\n3. Added automatic TTL-based re-enable (credential auto-recovers after 1 hour)\n4. Built validator to check token validity before use\n5. Integrated with AMQP listener for real-time credential updates from Identity service\n\n**Result**:\n- **20+ credentials** managed automatically\n- **Zero manual intervention** for rate limit handling\n- **Automatic recovery** after TTL expires\n- **Load balanced** across credentials\n\n---\n\n## COMPONENT 6: 3-Stage Data Pipeline\n\n### What You Built\nA clean 3-stage pipeline separating data retrieval, parsing, and processing.\n\n### Technical Deep Dive\n\n\\`\\`\\`\nSTAGE 1: RETRIEVAL (instagram/tasks/retrieval.py)\n─────────────────────────────────────────────────\nInput: handle, source\nOutput: Raw API response (dict)\n\nasync def retrieve_profile_data(handle: str, source: str) -> dict:\n    crawler = get_crawler(source)\n    cred = await credential_manager.get_enabled_cred(source)\n    crawler.set_credentials(cred)\n    return await crawler.fetch_profile_by_handle(handle)\n\n\nSTAGE 2: PARSING (instagram/tasks/ingestion.py)\n─────────────────────────────────────────────────\nInput: Raw API response\nOutput: Normalized ProfileLog with dimensions/metrics\n\ndef parse_profile_data(raw_data: dict, source: str) -> InstagramProfileLog:\n    \"\"\"\n    Normalize different API responses to common schema\n\n    Dimensions: Static attributes (handle, name, bio, category)\n    Metrics: Numeric values (followers, following, posts)\n    \"\"\"\n    parser = get_parser(source)  # Source-specific parser\n\n    return InstagramProfileLog(\n        dimensions=[\n            Dimension('handle', parser.get_handle(raw_data)),\n            Dimension('full_name', parser.get_name(raw_data)),\n            Dimension('biography', parser.get_bio(raw_data)),\n            Dimension('category', parser.get_category(raw_data)),\n            Dimension('is_verified', parser.get_verified(raw_data)),\n        ],\n        metrics=[\n            Metric('followers', parser.get_followers(raw_data)),\n            Metric('following', parser.get_following(raw_data)),\n            Metric('media_count', parser.get_media_count(raw_data)),\n        ]\n    )\n\n\nSTAGE 3: PROCESSING (instagram/tasks/processing.py)\n─────────────────────────────────────────────────\nInput: Normalized ProfileLog\nOutput: Database upsert + Event publish\n\nasync def upsert_profile(profile_log: InstagramProfileLog):\n    \"\"\"\n    1. Convert to ORM model\n    2. Upsert to PostgreSQL\n    3. Create audit log entry\n    4. Publish event to AMQP\n    \"\"\"\n    # 1. Convert to ORM\n    account = InstagramAccount(\n        profile_id=profile_log.get_dimension('profile_id'),\n        handle=profile_log.get_dimension('handle'),\n        followers=profile_log.get_metric('followers'),\n        # ... other fields\n    )\n\n    # 2. Upsert (insert or update on conflict)\n    await session.execute(\n        insert(InstagramAccount)\n        .values(account.to_dict())\n        .on_conflict_do_update(\n            index_elements=['profile_id'],\n            set_=account.to_dict()\n        )\n    )\n\n    # 3. Create audit log (for time-series analytics)\n    await session.execute(\n        insert(ProfileLog).values(\n            profile_id=account.profile_id,\n            dimensions=profile_log.dimensions_json,\n            metrics=profile_log.metrics_json,\n            source=profile_log.source\n        )\n    )\n\n    # 4. Publish event for downstream consumers\n    await amqp.publish(\n        exchange='beat.dx',\n        routing_key='profile_log_events',\n        body=profile_log.to_json()\n    )\n\\`\\`\\`\n\n### Why This Design?\n\n| Stage | Responsibility | Benefit |\n|-------|---------------|---------|\n| **Retrieval** | API communication | Easy to add new sources |\n| **Parsing** | Response normalization | Isolates API quirks |\n| **Processing** | Business logic | Clean, testable |\n\n### Interview STAR Story\n\n**Situation**: Different APIs return data in different formats. GraphAPI uses \\`followers_count\\`, RapidAPI uses \\`follower_count\\`, some return strings, some integers.\n\n**Task**: Build a pipeline that handles all API variations and produces consistent output.\n\n**Action**:\n1. Separated concerns into 3 stages\n2. Created source-specific parsers that normalize to common schema\n3. Used Dimension/Metric pattern for flexibility\n4. Added audit logging for time-series analysis\n5. Published events for downstream consumers\n\n**Result**:\n- **New API integration** only requires new parser (1 file)\n- **Consistent data format** regardless of source\n- **Full audit trail** for debugging\n- **Event-driven downstream** processing\n\n---\n\n## COMPONENT 7: GPT/OpenAI Integration\n\n### What You Built\nAI-powered data enrichment using OpenAI GPT for inferring demographics, categories, and topics from profile bios.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\n# gpt/functions/retriever/openai/openai_extractor.py\n\nclass OpenAi(GptCrawlerInterface):\n    \"\"\"Azure OpenAI integration for profile enrichment\"\"\"\n\n    def __init__(self):\n        openai.api_type = \"azure\"\n        openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n        openai.api_version = \"2023-05-15\"\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n    async def fetch_instagram_gpt_data_base_gender(\n        self, handle: str, bio: str\n    ) -> dict:\n        \"\"\"Infer creator's audience gender from bio\"\"\"\n        prompt = self._load_prompt(\"profile_info_v0.12.yaml\")\n\n        response = await openai.ChatCompletion.acreate(\n            engine=\"gpt-35-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": prompt['system']},\n                {\"role\": \"user\", \"content\": prompt['user'].format(\n                    handle=handle, bio=bio\n                )}\n            ],\n            temperature=0,  # Deterministic\n            max_tokens=200\n        )\n\n        return self._parse_json_response(response)\n\n    async def fetch_instagram_gpt_data_audience_age_gender(\n        self, handle: str, bio: str, recent_posts: list\n    ) -> dict:\n        \"\"\"Infer audience demographics from bio + recent posts\"\"\"\n        content = f\"Bio: {bio}\\\\n\\\\nRecent posts:\\\\n\"\n        content += \"\\\\n\".join([p['caption'][:200] for p in recent_posts[:5]])\n\n        # Similar implementation with different prompt\n\\`\\`\\`\n\n### Prompt Engineering (13 versions!)\n\n\\`\\`\\`yaml\n# gpt/prompts/profile_info_v0.12.yaml\n\nsystem: |\n  You are an AI assistant that analyzes Instagram creator profiles.\n  Based on the username and bio, infer:\n  1. Primary audience gender (male/female/mixed)\n  2. Confidence score (0.0 to 1.0)\n\n  Consider:\n  - Gendered words in bio\n  - Content category implications\n  - Handle patterns\n\n  Respond ONLY in JSON format.\n\nuser: |\n  Username: {handle}\n  Bio: {bio}\n\n  Output:\n  {\n    \"gender\": \"male|female|mixed\",\n    \"confidence\": 0.85,\n    \"reasoning\": \"Brief explanation\"\n  }\n\nmodel: gpt-35-turbo\ntemperature: 0\nmax_tokens: 200\n\\`\\`\\`\n\n### Use Cases Built\n\n| Flow | Input | Output | Use Case |\n|------|-------|--------|----------|\n| \\`base_gender\\` | handle, bio | gender, confidence | Audience targeting |\n| \\`base_location\\` | handle, bio | country, city | Geo targeting |\n| \\`categ_lang_topics\\` | handle, bio, posts | category, language, topics[] | Content classification |\n| \\`audience_age_gender\\` | handle, bio, posts | age_range, gender_dist | Demographics |\n| \\`audience_cities\\` | handle, bio, posts | cities with % | Geographic reach |\n\n### Interview STAR Story\n\n**Situation**: Brands wanted to know creator demographics (audience gender, age, location) but Instagram API doesn't provide this for non-business accounts.\n\n**Task**: Build an AI-powered system to infer demographics from publicly available data.\n\n**Action**:\n1. Integrated Azure OpenAI with async support\n2. Designed prompts through 13 iterations (v0.01 to v0.12)\n3. Added temperature=0 for consistent outputs\n4. Built JSON parsing with error handling\n5. Created separate flows for different enrichment types\n\n**Result**:\n- **5 enrichment types** available\n- **~85% accuracy** on gender inference (validated against known accounts)\n- **13 prompt versions** - continuous improvement\n- **Async processing** - doesn't block main flow\n\n### Questions They Might Ask\n\n**Q: How did you validate accuracy?**\nA: Compared against 500 creator accounts where we knew actual demographics. Achieved 85% accuracy on gender, 70% on location.\n\n**Q: How do you handle GPT rate limits?**\nA: Separate worker pool with low concurrency (2 workers × 5 concurrency). Also implemented exponential backoff on 429.\n\n**Q: What about hallucinations?**\nA: Used temperature=0 for deterministic outputs. Also validated JSON schema and rejected malformed responses.\n\n---\n\n## COMPONENT 8: Engagement Calculations\n\n### What You Built\nAnalytical formulas for calculating engagement metrics from raw data.\n\n### Technical Deep Dive\n\n\\`\\`\\`python\n# instagram/helper.py\n\ndef calculate_engagement_rate(likes: int, comments: int,\n                              followers: int) -> float:\n    \"\"\"\n    Standard engagement rate formula\n\n    Formula: (likes + comments) / followers × 100\n\n    Industry benchmarks:\n    - 1-3%: Low engagement\n    - 3-6%: Good engagement\n    - 6%+: Excellent engagement\n    \"\"\"\n    if followers == 0:\n        return 0.0\n    return ((likes + comments) / followers) * 100\n\n\ndef estimate_reach_reels(plays: int, followers: int) -> float:\n    \"\"\"\n    Estimate reach for Reels based on plays\n\n    Empirical formula derived from 10K+ data points:\n    factor = 0.94 - (log2(followers) × 0.001)\n\n    Larger accounts have lower reach/follower ratio\n    \"\"\"\n    import math\n    factor = 0.94 - (math.log2(followers) * 0.001)\n    return plays * factor\n\n\ndef estimate_reach_posts(likes: int) -> float:\n    \"\"\"\n    Estimate reach for static posts based on likes\n\n    Empirical formula:\n    factor = (7.6 - (log10(likes) × 0.7)) × 0.85\n\n    Based on typical like-to-reach ratio\n    \"\"\"\n    if likes == 0:\n        return 0.0\n    import math\n    factor = (7.6 - (math.log10(likes) * 0.7)) * 0.85\n    return factor * likes\n\n\ndef calculate_avg_metrics(posts: list, exclude_outliers: bool = True) -> dict:\n    \"\"\"\n    Calculate average metrics with outlier removal\n\n    Why exclude outliers?\n    - Viral posts skew averages\n    - Remove top 2 and bottom 2 posts\n    - Gives more realistic \"typical\" performance\n    \"\"\"\n    if exclude_outliers and len(posts) > 4:\n        # Sort by engagement and remove extremes\n        posts = sorted(posts, key=lambda p: p['likes'] + p['comments'])\n        posts = posts[2:-2]  # Remove top 2, bottom 2\n\n    return {\n        'avg_likes': mean([p['likes'] for p in posts]),\n        'avg_comments': mean([p['comments'] for p in posts]),\n        'avg_reach': mean([p.get('reach', 0) for p in posts]),\n        'avg_engagement': mean([\n            calculate_engagement_rate(p['likes'], p['comments'], p['followers'])\n            for p in posts\n        ])\n    }\n\\`\\`\\`\n\n### Interview Point\n\n**Story**: We needed to estimate reach for posts without Instagram Insights access. Derived empirical formulas from 10K+ data points where we had both likes and actual reach.\n\n---\n\n# PROJECT 2: STIR (Data Platform)\n\n## STIR has 8 Major Components:\n\n| # | Component | Files | Complexity |\n|---|-----------|-------|------------|\n| 1 | Airflow DAG Architecture | 76 DAG files | High |\n| 2 | dbt Transformation Layer | 112 models | High |\n| 3 | Three-Layer Data Flow | ClickHouse→S3→PostgreSQL | High |\n| 4 | Incremental Processing | dbt incremental models | Medium |\n| 5 | Multi-Dimensional Rankings | mart_leaderboard | Medium |\n| 6 | Time-Series Processing | mart_time_series | Medium |\n| 7 | Collection Analytics | mart_collection_* | Medium |\n| 8 | Cross-Database Sync | PostgresOperator + SSHOperator | Medium |\n\n---\n\n## COMPONENT 1: Airflow DAG Architecture\n\n### What You Built\n\n\\`\\`\\`python\n# 76 DAGs organized by function\n\nDAG_CATEGORIES = {\n    'dbt_orchestration': 11,    # dbt model execution\n    'instagram_sync': 17,       # Instagram data triggers\n    'youtube_sync': 12,         # YouTube data triggers\n    'collection_sync': 15,      # Collection analytics\n    'operational': 9,           # Data quality, verification\n    'asset_upload': 7,          # Media processing\n    'utility': 5                # One-off, helpers\n}\n\n# Scheduling Strategy\nSCHEDULES = {\n    '*/5 * * * *': ['dbt_recent_scl', 'post_ranker_partial'],      # Real-time\n    '*/15 * * * *': ['dbt_core'],                                   # Core metrics\n    '*/30 * * * *': ['dbt_collections', 'dbt_staging_collections'], # Collections\n    '0 * * * *': 12,  # Hourly syncs\n    '0 */3 * * *': 8,  # Every 3 hours\n    '0 19 * * *': ['dbt_daily'],  # Daily full refresh\n    '0 6 */7 * *': ['dbt_weekly']  # Weekly\n}\n\\`\\`\\`\n\n### DAG Pattern\n\n\\`\\`\\`python\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow_dbt_python.operators.dbt import DbtRunOperator\n\ndag = DAG(\n    dag_id='dbt_core',\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5),\n        'on_failure_callback': slack_alert\n    },\n    schedule_interval='*/15 * * * *',\n    start_date=datetime(2023, 1, 1),\n    catchup=False,              # Don't backfill\n    max_active_runs=1,          # Prevent overlap\n    concurrency=1,\n    dagrun_timeout=timedelta(minutes=60)\n)\n\ndbt_run = DbtRunOperator(\n    task_id='dbt_run_core',\n    models='tag:core',          # Only models tagged 'core'\n    profiles_dir='/Users/.dbt',\n    target='gcc_warehouse',     # ClickHouse\n    dag=dag\n)\n\\`\\`\\`\n\n---\n\n## COMPONENT 2: dbt Transformation Layer\n\n### Model Organization\n\n\\`\\`\\`\nmodels/\n├── staging/                    # 29 models - Raw data cleanup\n│   ├── beat/                   # From beat service\n│   │   ├── stg_beat_instagram_account.sql\n│   │   ├── stg_beat_instagram_post.sql\n│   │   └── stg_beat_profile_log.sql\n│   └── coffee/                 # From coffee service\n│       ├── stg_coffee_campaign_profiles.sql\n│       └── stg_coffee_collection.sql\n│\n└── marts/                      # 83 models - Business logic\n    ├── discovery/              # Influencer discovery\n    │   ├── mart_instagram_account.sql\n    │   └── mart_youtube_account.sql\n    ├── leaderboard/            # Rankings\n    │   ├── mart_leaderboard.sql\n    │   └── mart_time_series.sql\n    └── collection/             # Campaign analytics\n        └── mart_collection_post.sql\n\\`\\`\\`\n\n### Staging Model Example\n\n\\`\\`\\`sql\n-- models/staging/beat/stg_beat_instagram_account.sql\n\n{{ config(\n    materialized='table',\n    tags=['staging']\n) }}\n\nSELECT\n    profile_id,\n    handle,\n    full_name,\n    biography,\n\n    -- Type casting\n    CAST(followers_count AS Int64) as followers,\n    CAST(following_count AS Int64) as following,\n    CAST(posts_count AS Int64) as media_count,\n\n    -- Boolean conversion\n    is_verified = 1 as is_verified,\n    is_business_account = 1 as is_business,\n\n    -- NULL handling\n    COALESCE(category, 'Unknown') as category,\n\n    -- Timestamps\n    created_at,\n    updated_at\n\nFROM {{ source('beat_replica', 'instagram_account') }}\nWHERE handle IS NOT NULL\n  AND handle != ''\n\\`\\`\\`\n\n### Mart Model Example\n\n\\`\\`\\`sql\n-- models/marts/discovery/mart_instagram_account.sql\n\n{{ config(\n    materialized='table',\n    tags=['core', 'hourly']\n) }}\n\nWITH base AS (\n    SELECT * FROM {{ ref('stg_beat_instagram_account') }}\n),\n\npost_stats AS (\n    SELECT\n        profile_id,\n        COUNT(*) as total_posts,\n        AVG(likes_count) as avg_likes,\n        AVG(comments_count) as avg_comments,\n        SUM(likes_count) as total_likes\n    FROM {{ ref('stg_beat_instagram_post') }}\n    WHERE publish_time > now() - INTERVAL 30 DAY\n    GROUP BY profile_id\n),\n\nengagement AS (\n    SELECT\n        b.profile_id,\n        (ps.avg_likes + ps.avg_comments) / NULLIF(b.followers, 0) * 100\n            as engagement_rate\n    FROM base b\n    LEFT JOIN post_stats ps USING (profile_id)\n)\n\nSELECT\n    b.*,\n    ps.total_posts,\n    ps.avg_likes,\n    ps.avg_comments,\n    e.engagement_rate,\n\n    -- Rankings\n    row_number() OVER (ORDER BY b.followers DESC) as followers_rank,\n    row_number() OVER (PARTITION BY b.category\n                       ORDER BY b.followers DESC) as followers_rank_by_category,\n    row_number() OVER (PARTITION BY b.language\n                       ORDER BY b.followers DESC) as followers_rank_by_language\n\nFROM base b\nLEFT JOIN post_stats ps USING (profile_id)\nLEFT JOIN engagement e USING (profile_id)\n\\`\\`\\`\n\n---\n\n## COMPONENT 3: Three-Layer Data Flow\n\n### Architecture\n\n\\`\\`\\`\nLAYER 1: ClickHouse (OLAP)\n         ↓ INSERT INTO FUNCTION s3(...)\nLAYER 2: S3 (Staging)\n         ↓ aws s3 cp + COPY\nLAYER 3: PostgreSQL (Application)\n\\`\\`\\`\n\n### Implementation\n\n\\`\\`\\`python\n# DAG: sync_leaderboard_prod.py\n\n# Task 1: Export from ClickHouse to S3\nexport_task = ClickHouseOperator(\n    task_id='export_to_s3',\n    clickhouse_conn_id='clickhouse_gcc',\n    sql=\"\"\"\n        INSERT INTO FUNCTION s3(\n            's3://gcc-social-data/data-pipeline/tmp/leaderboard.json',\n            'AWS_KEY', 'AWS_SECRET',\n            'JSONEachRow'\n        )\n        SELECT\n            profile_id,\n            handle,\n            followers_rank,\n            engagement_rank,\n            category,\n            language\n        FROM dbt.mart_leaderboard\n        SETTINGS s3_truncate_on_insert=1\n    \"\"\"\n)\n\n# Task 2: Download via SSH\ndownload_task = SSHOperator(\n    task_id='download_from_s3',\n    ssh_conn_id='ssh_prod_pg',\n    command='aws s3 cp s3://gcc-social-data/data-pipeline/tmp/leaderboard.json /tmp/'\n)\n\n# Task 3: Load into PostgreSQL with atomic swap\nload_task = PostgresOperator(\n    task_id='load_to_postgres',\n    postgres_conn_id='prod_pg',\n    sql=\"\"\"\n        -- Create temp table\n        CREATE TEMP TABLE tmp_lb (data JSONB);\n\n        -- Load JSON\n        COPY tmp_lb FROM '/tmp/leaderboard.json';\n\n        -- Insert with type casting\n        INSERT INTO leaderboard_new\n        SELECT\n            (data->>'profile_id')::bigint,\n            (data->>'handle')::text,\n            (data->>'followers_rank')::int,\n            (data->>'engagement_rank')::int\n        FROM tmp_lb;\n\n        -- Atomic swap\n        ALTER TABLE leaderboard RENAME TO leaderboard_old;\n        ALTER TABLE leaderboard_new RENAME TO leaderboard;\n        DROP TABLE IF EXISTS leaderboard_old;\n    \"\"\"\n)\n\nexport_task >> download_task >> load_task\n\\`\\`\\`\n\n### Why This Pattern?\n\n| Benefit | Explanation |\n|---------|-------------|\n| **Zero downtime** | Atomic rename, not delete+insert |\n| **Decoupled systems** | S3 as intermediate, no direct connection |\n| **Debuggable** | Can inspect S3 files |\n| **Recoverable** | If load fails, re-download from S3 |\n\n---\n\n## COMPONENT 4: Incremental Processing\n\n### ClickHouse Incremental Model\n\n\\`\\`\\`sql\n-- models/marts/leaderboard/mart_time_series.sql\n\n{{ config(\n    materialized='incremental',\n    engine='ReplacingMergeTree(updated_at)',\n    order_by='(profile_id, date)',\n    unique_key='(profile_id, date)'\n) }}\n\nSELECT\n    profile_id,\n    toDate(created_at) as date,\n    argMax(followers, created_at) as followers,\n    argMax(following, created_at) as following,\n    max(created_at) as updated_at\n\nFROM {{ ref('stg_beat_instagram_account') }}\n\n{% if is_incremental() %}\n-- Only process new data (with 4-hour safety buffer)\nWHERE created_at > (\n    SELECT max(updated_at) - INTERVAL 4 HOUR\n    FROM {{ this }}\n)\n{% endif %}\n\nGROUP BY profile_id, date\n\\`\\`\\`\n\n### Why 4-Hour Buffer?\n\n| Reason | Explanation |\n|--------|-------------|\n| **Late-arriving data** | Some events arrive delayed |\n| **Failed retries** | Tasks retried may have old timestamps |\n| **Clock drift** | Different servers may have slight time differences |\n\n---\n\n# PROJECT 3: EVENT-GRPC (ClickHouse Sinker)\n\n## Your Specific Work: Consumer → ClickHouse Pipeline\n\n### COMPONENT 1: Buffered Sinker Pattern\n\n\\`\\`\\`go\n// sinker/trace_log_sinker.go\n\nfunc TraceLogEventsSinker(c chan interface{}) {\n    ticker := time.NewTicker(5 * time.Second)  // Flush every 5 sec\n    batch := []model.TraceLogEvent{}\n\n    for {\n        select {\n        case event := <-c:\n            // Parse and add to batch\n            traceLog := parseTraceLog(event)\n            batch = append(batch, traceLog)\n\n            // Flush if batch is full\n            if len(batch) >= 1000 {\n                flushBatch(batch)\n                batch = []model.TraceLogEvent{}\n            }\n\n        case <-ticker.C:\n            // Periodic flush (even if batch not full)\n            if len(batch) > 0 {\n                flushBatch(batch)\n                batch = []model.TraceLogEvent{}\n            }\n        }\n    }\n}\n\nfunc flushBatch(batch []model.TraceLogEvent) {\n    db := clickhouse.Clickhouse(config.New(), nil)\n    result := db.Create(&batch)\n    if result.Error != nil {\n        log.Error(\"Batch insert failed\", result.Error)\n        // Could implement retry or dead-letter queue here\n    }\n}\n\\`\\`\\`\n\n### Why Buffered Sinker?\n\n| Without Buffering | With Buffering |\n|-------------------|----------------|\n| 1 INSERT per event | 1 INSERT per 1000 events |\n| 10,000 DB calls/sec | 10 DB calls/sec |\n| High latency | Lower latency |\n| DB connection exhaustion | Stable connections |\n\n### COMPONENT 2: Consumer Configuration\n\n\\`\\`\\`go\n// main.go - Your configuration for 26 consumers\n\n// High-volume buffered consumer\ntraceLogChan := make(chan interface{}, 10000)  // 10K buffer\n\ntraceLogConfig := rabbit.RabbitConsumerConfig{\n    QueueName:            \"trace_log\",\n    Exchange:             \"identity.dx\",\n    RoutingKey:           \"trace_log\",\n    RetryOnError:         true,\n    ErrorExchange:        &errorExchange,\n    ErrorRoutingKey:      &errorRoutingKey,\n    ConsumerCount:        2,\n    BufferChan:           traceLogChan,\n    BufferedConsumerFunc: sinker.BufferTraceLogEvent,\n}\n\nrabbit.Rabbit(config).InitConsumer(traceLogConfig)\ngo sinker.TraceLogEventsSinker(traceLogChan)  // Background batch processor\n\\`\\`\\`\n\n### COMPONENT 3: Connection Auto-Recovery\n\n\\`\\`\\`go\n// clickhouse/clickhouse.go\n\nfunc clickhouseConnectionCron(config config.Config) {\n    ticker := time.NewTicker(1 * time.Second)\n\n    for range ticker.C {\n        for dbName, db := range singletonClickhouseMap {\n            if db == nil {\n                reconnect(dbName)\n                continue\n            }\n\n            // Health check\n            sqlDB, _ := db.DB()\n            if err := sqlDB.Ping(); err != nil {\n                log.Warn(\"ClickHouse connection lost, reconnecting...\")\n                reconnect(dbName)\n            }\n        }\n    }\n}\n\\`\\`\\`\n\n---\n\n# PROJECT 4: FAKE_FOLLOWER_ANALYSIS\n\n## 7 Components You Built:\n\n| # | Component | Code |\n|---|-----------|------|\n| 1 | Symbol Normalization | 13 Unicode variant handling |\n| 2 | Language Detection | Character-to-language mapping |\n| 3 | Indic Transliteration | HMM models for 10 languages |\n| 4 | Fuzzy Matching | RapidFuzz weighted ensemble |\n| 5 | Indian Name Database | 35,183 names matching |\n| 6 | Ensemble Scoring | 5-feature classification |\n| 7 | AWS Serverless Pipeline | SQS → Lambda → Kinesis |\n\n### COMPONENT 1: Symbol Normalization\n\n\\`\\`\\`python\ndef symbol_name_convert(name):\n    \"\"\"\n    Convert fancy Unicode to ASCII\n\n    Handles 13 Unicode variant sets:\n    1. 🅐🅑🅒 → ABC (Circled)\n    2. 𝐀𝐁𝐂 → ABC (Mathematical Bold)\n    3. 𝐴𝐵𝐶 → ABC (Mathematical Italic)\n    ... 10 more sets\n    \"\"\"\n    # Character mapping tables for each variant\n    mappings = {\n        'circled': {...},\n        'math_bold': {...},\n        # ...\n    }\n\n    for variant, mapping in mappings.items():\n        for fancy, normal in mapping.items():\n            name = name.replace(fancy, normal)\n\n    return name\n\\`\\`\\`\n\n### COMPONENT 4: Fuzzy Matching Algorithm\n\n\\`\\`\\`python\ndef generate_similarity_score(handle, name):\n    \"\"\"\n    Weighted fuzzy matching using RapidFuzz\n\n    Formula: (2×partial + sort + set) / 4\n\n    Why weighted?\n    - partial_ratio: Best for substring matching\n    - token_sort_ratio: Handles word reordering\n    - token_set_ratio: Handles extra/missing words\n    \"\"\"\n    from itertools import permutations\n    from rapidfuzz import fuzz\n\n    # Generate name permutations\n    name_parts = name.split()\n    if len(name_parts) <= 4:\n        perms = [' '.join(p) for p in permutations(name_parts)]\n    else:\n        perms = [name]  # Too many permutations\n\n    best_score = 0\n    for perm in perms:\n        partial = fuzz.partial_ratio(handle, perm)\n        sort = fuzz.token_sort_ratio(handle, perm)\n        set_ratio = fuzz.token_set_ratio(handle, perm)\n\n        score = (2 * partial + sort + set_ratio) / 4\n        best_score = max(best_score, score)\n\n    return best_score\n\\`\\`\\`\n\n### COMPONENT 6: Ensemble Scoring\n\n\\`\\`\\`python\ndef final_score(lang_flag, similarity, digit_count, special_char_flag):\n    \"\"\"\n    5-feature ensemble → 3 confidence levels\n\n    Returns:\n    - 1.0: Definitely FAKE\n    - 0.33: Weak FAKE signal\n    - 0.0: Likely REAL\n    \"\"\"\n    # Strong FAKE indicators\n    if lang_flag:  # Non-Indic script\n        return 1.0\n\n    if digit_count > 4:  # Too many numbers\n        return 1.0\n\n    if special_char_flag == 1:  # Has _ but name doesn't match\n        return 1.0\n\n    # Weak FAKE indicator\n    if 0 < similarity <= 40:\n        return 0.33\n\n    # Default: REAL\n    return 0.0\n\\`\\`\\`\n\n---\n\n# SUMMARY: Components Per Project\n\n| Project | Total Components | Key Technical Skills |\n|---------|-----------------|---------------------|\n| **beat** | 12 | Worker pools, Rate limiting, API integration, Async Python |\n| **stir** | 8 | Airflow, dbt, ClickHouse, Data modeling |\n| **event-grpc** | 3 | Go channels, Batch processing, Message queues |\n| **fake_follower** | 7 | NLP, ML ensemble, AWS Lambda, Transliteration |\n\n---\n\n# HOW TO USE IN INTERVIEW\n\n**When they ask about one project, go deeper:**\n\nInterviewer: \"Tell me about beat\"\n\nYou: \"Beat had 12 major components. Which would you like me to dive into?\n1. Worker pool architecture\n2. Rate limiting system\n3. API integration framework\n4. Credential management\n5. GPT integration\n...\"\n\n**This shows:**\n- You understand the system holistically\n- You can go deep on any component\n- You have ownership and expertise\n\n---\n\n*Remember: Pick 2-3 components per project you're most confident about and prepare deep-dive stories for those.*\n"
  },
  {
    "id": "RESUME_TO_CODE_MAPPING",
    "title": "Resume ↔ Code Mapping",
    "category": "technical",
    "badge": null,
    "content": "# RESUME TO CODE MAPPING - GOOD CREATOR CO.\n## Complete Technical Evidence for Every Resume Bullet\n\n---\n\n# YOUR RESUME SECTION (Good Creator Co.)\n\n\\`\\`\\`\nGood Creator Co. (GCC SaaS Social Media Analytics Platform) | Software Engineer-I\nFeb 2023 - May 2024\n\n• Optimized API response times by 25% and reduced operational costs by 30% through\n  platform development and optimization.\n\n• Designed an asynchronous data processing system handling 10M+ daily data points,\n  improving real-time insights and API performance.\n\n• Built a high-performance logging system with RabbitMQ, Python and Golang,\n  transitioning to ClickHouse, achieving a 2.5x reduction in log retrieval times\n  and supporting billions of logs.\n\n• Crafted and streamlined ETL data pipelines (Apache Airflow) for batch data\n  ingestion for scraping and the data marts updates, cutting data latency by 50%.\n\n• Built an AWS S3-based asset upload system processing 8M images daily while\n  optimizing infrastructure costs.\n\n• Developed real-time social media insights modules, driving 10% user engagement\n  growth through actionable Genre Insights and Keyword Analytics.\n\n• Automated content filtering and elevated data processing speed by 50%.\n\\`\\`\\`\n\n---\n\n# COMPLETE SYSTEM ARCHITECTURE\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────┐\n│                         YOUR WORK ACROSS 4 PROJECTS                              │\n└─────────────────────────────────────────────────────────────────────────────────┘\n\n┌──────────────────┐    ┌──────────────────┐    ┌──────────────────┐    ┌──────────────┐\n│      BEAT        │    │   EVENT-GRPC     │    │      STIR        │    │    COFFEE    │\n│    (Python)      │    │     (Go)         │    │   (Airflow+dbt)  │    │     (Go)     │\n├──────────────────┤    ├──────────────────┤    ├──────────────────┤    ├──────────────┤\n│ • Crawl profiles │    │ • Consume events │    │ • Transform data │    │ • REST API   │\n│ • Crawl posts    │───►│ • Batch inserts  │───►│ • Build marts    │───►│ • Serve data │\n│ • Rate limiting  │    │ • Flush to CH    │    │ • Sync to PG     │    │ • Multi-tenant│\n│ • 150+ workers   │    │ • 26 queues      │    │ • 76 DAGs        │    │              │\n└──────────────────┘    └──────────────────┘    └──────────────────┘    └──────────────┘\n        │                       │                       │\n        │                       │                       │\n        ▼                       ▼                       ▼\n   ┌─────────┐            ┌───────────┐          ┌───────────┐\n   │ RabbitMQ │            │ClickHouse │          │PostgreSQL │\n   │ (Events) │            │  (OLAP)   │          │  (OLTP)   │\n   └─────────┘            └───────────┘          └───────────┘\n\\`\\`\\`\n\n---\n\n# BULLET 3: THE BIG ARCHITECTURAL CHANGE\n\n## \"Built a high-performance logging system with RabbitMQ, Python and Golang, transitioning to ClickHouse\"\n\n### THE PROBLEM YOU SOLVED\n\n**Initial Situation:**\n- beat crawled influencer profiles and posts\n- All data was saved directly to PostgreSQL tables\n- For time-series analytics (tracking follower growth over time), we needed to save every crawl as a log\n\n**What Went Wrong:**\n\\`\\`\\`\nProblem 1: PostgreSQL can't handle high-volume time-series writes\n- 10M+ logs/day overwhelmed PostgreSQL\n- Write latency increased from 5ms to 500ms\n- Table bloat (billions of rows)\n\nProblem 2: Analytics queries were slow\n- \"Get follower growth for last 30 days\" took 30+ seconds\n- PostgreSQL row-based storage not optimized for aggregations\n\nProblem 3: Storage costs exploded\n- Row-based storage = 5x more space than needed\n- Had to keep adding storage\n\\`\\`\\`\n\n### YOUR SOLUTION: Event-Driven Architecture\n\n**Before (Old Way):**\n\\`\\`\\`python\n# beat/instagram/tasks/processing.py - COMMENTED OUT CODE shows old approach\n\n@sessionize\nasync def upsert_profile(profile_id, profile_log, recent_posts_log, session=None):\n    # OLD: Save directly to PostgreSQL\n    profile = ProfileLog(...)\n    session.add(profile)  # ❌ Direct DB write - SLOW!\n\n    # OLD: Time-series table (COMMENTED OUT!)\n    # await upsert_insta_account_ts(context, profile_log, profile_id, session=session)\n\\`\\`\\`\n\n**After (New Way):**\n\\`\\`\\`python\n# beat/instagram/tasks/processing.py - Line 135\n\n@sessionize\nasync def upsert_profile(profile_id, profile_log, recent_posts_log, session=None):\n    profile = ProfileLog(\n        platform=enums.Platform.INSTAGRAM.name,\n        profile_id=profile_id,\n        metrics=[m.__dict__ for m in profile_log.metrics],\n        dimensions=[d.__dict__ for d in profile_log.dimensions],\n        source=profile_log.source,\n        timestamp=now\n    )\n\n    # NEW: Publish event instead of DB write\n    await make_scrape_log_event(\"profile_log\", profile)  # ✅ Event to RabbitMQ\n\n    # Still update main account table (not time-series)\n    account = await upsert_insta_account(context, profile_log, profile_id, session=session)\n\\`\\`\\`\n\n### COMPLETE DATA FLOW\n\n\\`\\`\\`\nSTEP 1: BEAT PUBLISHES EVENTS\n─────────────────────────────────────────────────────────────────────\nFile: beat/utils/request.py (lines 217-238)\n\nasync def emit_profile_log_event(log: ProfileLog):\n    payload = {\n        \"event_id\": str(uuid.uuid4()),\n        \"source\": log.source,\n        \"platform\": log.platform,\n        \"profile_id\": log.profile_id,\n        \"handle\": handle,\n        \"event_timestamp\": now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"metrics\": {m[\"key\"]: m[\"value\"] for m in log.metrics},\n        \"dimensions\": {d[\"key\"]: d[\"value\"] for d in log.dimensions}\n    }\n    publish(payload, \"beat.dx\", \"profile_log_events\")  # → RabbitMQ\n\nEvent Types Published:\n- profile_log_events      → Profile snapshots (followers, following, bio)\n- post_log_events         → Post snapshots (likes, comments, reach)\n- profile_relationship_log_events → Follower/following lists\n- post_activity_log_events       → Comments, likes on posts\n- sentiment_log_events           → Comment sentiment scores\n\n\nSTEP 2: EVENT-GRPC CONSUMES & FLUSHES TO CLICKHOUSE\n─────────────────────────────────────────────────────────────────────\nFile: event-grpc/main.go (lines 382-400)\n\nprofileLogEx := \"beat.dx\"\nprofileLogRk := \"profile_log_events\"\nprofileLogChan := make(chan interface{}, 10000)  // 10K buffer!\n\nprofileLogConsumerConfig := rabbit.RabbitConsumerConfig{\n    QueueName:            \"profile_log_events_q\",\n    Exchange:             profileLogEx,\n    RoutingKey:           profileLogRk,\n    RetryOnError:         true,\n    ConsumerCount:        2,                              // 2 consumers\n    BufferedConsumerFunc: sinker.BufferProfileLogEvents,  // Buffer function\n    BufferChan:           profileLogChan,\n}\nrabbit.Rabbit(config).InitConsumer(profileLogConsumerConfig)\ngo sinker.ProfileLogEventsSinker(profileLogChan)  // Background sinker\n\nConsumer Queues You Built:\n| Queue                          | Workers | Buffer  | Purpose               |\n|--------------------------------|---------|---------|------------------------|\n| post_log_events_q              | 20      | 10,000  | Post snapshots         |\n| profile_log_events_q           | 2       | 10,000  | Profile snapshots      |\n| sentiment_log_events_q         | 2       | 10,000  | Sentiment scores       |\n| post_activity_log_events_q     | 2       | 10,000  | Comments/likes         |\n| profile_relationship_log_events_q | 2    | 10,000  | Follower lists         |\n\n\nSTEP 3: BUFFERED SINKER PATTERN (Your Implementation)\n─────────────────────────────────────────────────────────────────────\nFile: event-grpc/sinker/profile_log_sinker.go\n\nfunc ProfileLogEventsSinker(c chan interface{}) {\n    ticker := time.NewTicker(5 * time.Second)  // Flush every 5 sec\n    batch := []model.ProfileLogEvent{}\n\n    for {\n        select {\n        case event := <-c:\n            profileLog := parseProfileLog(event)\n            batch = append(batch, profileLog)\n\n            // Flush if batch full (1000 events)\n            if len(batch) >= 1000 {\n                flushBatch(batch)\n                batch = []model.ProfileLogEvent{}\n            }\n\n        case <-ticker.C:\n            // Periodic flush (even if batch not full)\n            if len(batch) > 0 {\n                flushBatch(batch)\n                batch = []model.ProfileLogEvent{}\n            }\n        }\n    }\n}\n\nWhy Buffered Sinker?\n┌─────────────────────────────────────────────────────────────────┐\n│ WITHOUT BUFFERING          │ WITH BUFFERING                    │\n├─────────────────────────────┼───────────────────────────────────┤\n│ 1 INSERT per event          │ 1 INSERT per 1000 events          │\n│ 10,000 DB calls/sec         │ 10 DB calls/sec                   │\n│ High DB connection usage    │ Low connection usage              │\n│ Network overhead per event  │ Amortized network cost            │\n│ ClickHouse not optimized    │ ClickHouse loves batch inserts    │\n└─────────────────────────────┴───────────────────────────────────┘\n\n\nSTEP 4: CLICKHOUSE TABLES (Your Schema)\n─────────────────────────────────────────────────────────────────────\nFile: event-grpc/schema/events.sql (lines 238-253)\n\nCREATE TABLE _e.profile_log_events\n(\n    \\`event_id\\` String,\n    \\`source\\` String,\n    \\`platform\\` String,\n    \\`profile_id\\` String,\n    \\`handle\\` Nullable(String),\n    \\`event_timestamp\\` DateTime,\n    \\`insert_timestamp\\` DateTime,\n    \\`metrics\\` String,        -- JSON: {followers: 100000, following: 500}\n    \\`dimensions\\` String      -- JSON: {bio: \"...\", category: \"fitness\"}\n)\nENGINE = MergeTree\nPARTITION BY toYYYYMM(event_timestamp)  -- Monthly partitions\nORDER BY (platform, profile_id, event_timestamp)  -- Optimized for time-series queries\n\nWhy ClickHouse?\n- Columnar storage: 5x compression vs PostgreSQL\n- Partition pruning: Only scan relevant months\n- Vectorized execution: Aggregations are FAST\n- MergeTree: Optimized for append-heavy workloads\n\n\nSTEP 5: STIR TRANSFORMS & SYNCS\n─────────────────────────────────────────────────────────────────────\nFile: stir/src/gcc_social/models/staging/stg_beat_profile_log.sql\n\n-- dbt model reads from ClickHouse events table\nSELECT\n    profile_id,\n    toDate(event_timestamp) as date,\n    argMax(JSONExtractInt(metrics, 'followers'), event_timestamp) as followers,\n    argMax(JSONExtractInt(metrics, 'following'), event_timestamp) as following,\n    max(event_timestamp) as updated_at\nFROM _e.profile_log_events\nGROUP BY profile_id, date\n\nFile: stir/dags/sync_leaderboard_prod.py\n-- Sync to PostgreSQL for coffee API\nClickHouse → S3 (JSON) → PostgreSQL (atomic swap)\n\\`\\`\\`\n\n### 2.5x FASTER LOG RETRIEVAL - EXPLAINED\n\n\\`\\`\\`\nQUERY: \"Get follower growth for profile X in last 30 days\"\n\nPostgreSQL (Before):\n─────────────────────\nSELECT date, followers\nFROM profile_log\nWHERE profile_id = 'X'\n  AND timestamp > now() - interval '30 days'\nORDER BY timestamp;\n\nExecution:\n- Full table scan (billions of rows)\n- Row-by-row processing\n- Time: 30 seconds\n\n\nClickHouse (After):\n─────────────────────\nSELECT\n    toDate(event_timestamp) as date,\n    argMax(JSONExtractInt(metrics, 'followers'), event_timestamp) as followers\nFROM _e.profile_log_events\nWHERE profile_id = 'X'\n  AND event_timestamp > now() - interval 30 day\nGROUP BY date\nORDER BY date;\n\nExecution:\n- Partition pruning (only last month's partition)\n- Columnar scan (only metrics column)\n- Vectorized aggregation\n- Time: 12 seconds (2.5x faster!)\n\n\nPerformance Comparison:\n┌─────────────────────┬────────────────┬─────────────────┐\n│ Metric              │ PostgreSQL     │ ClickHouse      │\n├─────────────────────┼────────────────┼─────────────────┤\n│ Query time (30 days)│ 30 seconds     │ 12 seconds      │\n│ Storage (1B logs)   │ 500 GB         │ 100 GB          │\n│ Insert latency      │ 50ms/event     │ 5ms/1000 events │\n│ Compression ratio   │ 1x             │ 5x              │\n└─────────────────────┴────────────────┴─────────────────┘\n\\`\\`\\`\n\n### INTERVIEW TALKING POINTS\n\n**Q: \"Tell me about the high-performance logging system you built\"**\n\n> \"At GCC, we needed to track time-series data for influencer analytics - things like follower growth, engagement trends over time. Initially, we saved every crawl directly to PostgreSQL, but this caused problems:\n>\n> **The Problem:**\n> - 10M+ log entries per day overwhelmed PostgreSQL\n> - Analytics queries took 30+ seconds\n> - Storage costs were exploding\n>\n> **My Solution:**\n> I redesigned the architecture to be event-driven:\n>\n> 1. **beat (Python)** publishes events to RabbitMQ instead of direct DB writes\n> 2. **event-grpc (Go)** consumes events with buffered batching (1000 events/batch)\n> 3. **ClickHouse** stores the logs (columnar, 5x compression)\n> 4. **stir (Airflow + dbt)** transforms and syncs to PostgreSQL for the API\n>\n> **Key Technical Decisions:**\n> - Buffered sinker pattern: 1 INSERT per 1000 events instead of 1 per event\n> - MergeTree engine with monthly partitioning for efficient time-range queries\n> - Separate OLAP (ClickHouse) from OLTP (PostgreSQL) workloads\n>\n> **Results:**\n> - 2.5x faster query performance (30s → 12s)\n> - 5x storage reduction through columnar compression\n> - Supports billions of logs without performance degradation\"\n\n---\n\n# BULLET 1: API Response Time Optimization (25%) + Cost Reduction (30%)\n\n## Project: beat\n\n### WHAT YOU BUILT\n\n#### 1. Multi-Level Rate Limiting System\n**File**: \\`beat/server.py\\` (lines 312-338)\n**File**: \\`beat/utils/request.py\\` (lines 97-118)\n\n\\`\\`\\`python\n# 3-Level Stacked Rate Limiting\nglobal_limit_day = RateSpec(requests=20000, seconds=86400)   # 20K/day\nglobal_limit_minute = RateSpec(requests=60, seconds=60)      # 60/min\nhandle_limit = RateSpec(requests=1, seconds=1)               # 1/sec per handle\n\n# Implementation - All 3 must pass\nasync with RateLimiter(unique_key=\"beat_global_daily\", rate_spec=global_limit_day):\n    async with RateLimiter(unique_key=\"beat_global_minute\", rate_spec=global_limit_minute):\n        async with RateLimiter(unique_key=f\"beat_handle_{handle}\", rate_spec=handle_limit):\n            result = await make_api_call(handle)\n\\`\\`\\`\n\n#### 2. Connection Pooling\n\\`\\`\\`python\n# Main Server Pool - 100 total connections\nengine = create_async_engine(\n    PGBOUNCER_URL,\n    pool_size=50,\n    max_overflow=50,\n    pool_recycle=500\n)\n\\`\\`\\`\n\n#### 3. Credential Rotation with TTL\n\\`\\`\\`python\nasync def disable_creds(cred_id: int, disable_duration: int = 3600):\n    \"\"\"When API returns 429, disable credential for 1 hour\"\"\"\n    await session.execute(\n        update(Credential)\n        .where(Credential.id == cred_id)\n        .values(enabled=False, disabled_till=func.now() + timedelta(seconds=disable_duration))\n    )\n\\`\\`\\`\n\n### INTERVIEW ANSWER\n\n> \"I achieved 25% faster API response through connection pooling (50-100 connections) and uvloop event loop. The 30% cost reduction came from intelligent rate limiting - a 3-level stacked system (daily, per-minute, per-handle) that prevented exceeding API quotas, plus credential rotation that avoided API bans.\"\n\n---\n\n# BULLET 2: Asynchronous Data Processing (10M+ Daily)\n\n## Project: beat\n\n### WHAT YOU BUILT\n\n\\`\\`\\`python\n# 25 flows × configurable workers = 150+ total workers\n_whitelist = {\n    'refresh_profile_by_handle': {'no_of_workers': 10, 'no_of_concurrency': 5},\n    'refresh_yt_profiles': {'no_of_workers': 10, 'no_of_concurrency': 5},\n    'asset_upload_flow': {'no_of_workers': 15, 'no_of_concurrency': 5},\n    # ... 22 more flows\n}\n\n# Architecture: Multiprocessing + Asyncio + Semaphore\ndef main():\n    for flow_name, config in _whitelist.items():\n        for i in range(config['no_of_workers']):\n            process = multiprocessing.Process(target=looper, args=(flow_name, config['no_of_concurrency']))\n            process.start()\n\nasync def poller(flow_name, concurrency):\n    semaphore = asyncio.Semaphore(concurrency)\n    while True:\n        task = await poll(flow_name)  # SQL with FOR UPDATE SKIP LOCKED\n        if task:\n            asyncio.create_task(perform_task(task, semaphore))\n\\`\\`\\`\n\n### INTERVIEW ANSWER\n\n> \"I designed a hybrid architecture: multiprocessing for CPU parallelism (bypasses Python GIL) + asyncio inside each process for I/O concurrency. 150+ workers across 25 flows, with semaphore-based concurrency control. SQL-based task queue with FOR UPDATE SKIP LOCKED for distributed coordination.\"\n\n---\n\n# BULLET 4: ETL Data Pipelines (Apache Airflow)\n\n## Project: stir\n\n### WHAT YOU BUILT\n\n\\`\\`\\`\n76 Airflow DAGs + 112 dbt Models\n\nScheduling:\n- */5 min:  dbt_recent_scl (real-time)\n- */15 min: dbt_core (core metrics)\n- Daily:    dbt_daily (full refresh)\n\nThree-Layer Data Flow:\nClickHouse (OLAP) → S3 (staging) → PostgreSQL (OLTP)\n\\`\\`\\`\n\n### INTERVIEW ANSWER\n\n> \"I built 76 Airflow DAGs orchestrating 112 dbt models. Key innovation: three-layer data flow - dbt transforms in ClickHouse (fast OLAP), export to S3 (decoupling), then atomic load to PostgreSQL (zero-downtime table swap). Incremental processing with 4-hour lookback reduced data latency by 50%.\"\n\n---\n\n# BULLET 5: AWS S3 Asset Upload (8M Images/Day)\n\n## Project: beat\n\n\\`\\`\\`python\n# 50 workers × 100 concurrency = 5000 parallel uploads\n_whitelist = {\n    'asset_upload_flow': {'no_of_workers': 50, 'no_of_concurrency': 100},\n}\n\nasync def asset_upload_flow(entity_id, entity_type, platform, asset_url):\n    # Download from Instagram CDN\n    async with aiohttp.ClientSession() as session:\n        async with session.get(asset_url) as resp:\n            image_data = await resp.read()\n\n    # Upload to S3\n    s3_key = f\"assets/{entity_type}s/{platform.lower()}/{entity_id}.jpg\"\n    await s3_client.put_object(Bucket='gcc-social-assets', Key=s3_key, Body=image_data)\n\n    return f\"https://cdn.goodcreator.co/{s3_key}\"\n\\`\\`\\`\n\n---\n\n# BULLET 6: Genre Insights & Keyword Analytics\n\n## Projects: beat + stir\n\n\\`\\`\\`python\n# beat/keyword_collection/generate_instagram_report.py\n# Keyword matching with ClickHouse\nquery = \"\"\"\n    SELECT shortcode, profile_id, caption, likes_count, comments_count\n    FROM dbt.mart_instagram_post\n    WHERE multiMatchAny(lower(caption), ['fitness', 'gym', 'workout'])\n\"\"\"\n\n# Reach estimation formulas\nif post_type == 'reels':\n    reach = plays * (0.94 - (log2(followers) * 0.001))\nelse:\n    reach = (7.6 - (log10(likes) * 0.7)) * 0.85 * likes\n\n# YAKE keyword extraction\nfrom yake import KeywordExtractor\nkeywords = KeywordExtractor(n=3, top=10).extract_keywords(caption)\n\\`\\`\\`\n\n---\n\n# BULLET 7: Content Filtering (50% Speed)\n\n## Projects: beat + fake_follower_analysis\n\n\\`\\`\\`python\n# Automated ML categorization\nresult = await http_client.post(RAY_URL, json={'model': 'CATEGORIZER', 'text': caption})\n\n# Data quality validation\ndef is_data_consumable(data, data_type):\n    if data_type == 'base_gender':\n        return data.get('gender') and data['gender'] != 'UNKNOWN'\n    elif data_type == 'audience_cities':\n        return len(data.get('cities', [])) > 5\n\n# Fake follower detection (5-feature ensemble)\nif non_indic_language: return 1.0  # FAKE\nif digit_count > 4: return 1.0     # FAKE\n\\`\\`\\`\n\n---\n\n# QUICK REFERENCE NUMBERS\n\n| Metric | Value |\n|--------|-------|\n| API response improvement | 25% |\n| Cost reduction | 30% |\n| Daily data points | 10M+ |\n| Log retrieval speedup | 2.5x |\n| Data latency reduction | 50% |\n| Images processed daily | 8M |\n| Processing speed improvement | 50% |\n| Airflow DAGs | 76 |\n| dbt models | 112 |\n| Worker processes | 150+ |\n| RabbitMQ consumer queues | 26 |\n| Buffer size per queue | 10,000 |\n| Batch size for ClickHouse | 1,000 |\n| Flush interval | 5 seconds |\n\n---\n\n# PROJECT OWNERSHIP SUMMARY\n\n| Project | Your Role | Key Contribution |\n|---------|-----------|------------------|\n| **beat** | Core Developer | Worker pools, rate limiting, event publishing, GPT integration |\n| **event-grpc** | Implemented consumer→ClickHouse pipeline | Buffered sinkers, 26 queues |\n| **stir** | Core Developer | 76 DAGs, 112 dbt models, three-layer sync |\n| **fake_follower_analysis** | Solo Developer | End-to-end ML system |\n\n---\n\n*This document maps every resume bullet to actual code with file paths and line numbers.*\n"
  },
  {
    "id": "SYSTEM_INTERCONNECTIVITY",
    "title": "System Architecture",
    "category": "technical",
    "badge": null,
    "content": "# COMPLETE SYSTEM INTERCONNECTIVITY MAP\n## How beat, event-grpc, stir, coffee, and ClickHouse Work Together\n\n---\n\n# VISUAL ARCHITECTURE\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                              COMPLETE SYSTEM ARCHITECTURE                                    │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\n                                    ┌─────────────────┐\n                                    │   EXTERNAL      │\n                                    │   SCRAPING      │\n                                    │   APIs          │\n                                    │ (Instagram,     │\n                                    │  YouTube, etc.) │\n                                    └────────┬────────┘\n                                             │\n                                             ▼\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                       BEAT (Python)                                          │\n│  ┌─────────────────────────────────────────────────────────────────────────────────────┐   │\n│  │  • 150+ worker processes                                                              │   │\n│  │  • Scrapes Instagram/YouTube profiles and posts                                       │   │\n│  │  • Publishes events to RabbitMQ                                                       │   │\n│  │  • Stores transactional data in PostgreSQL                                            │   │\n│  │  • Rate limiting with Redis                                                           │   │\n│  └─────────────────────────────────────────────────────────────────────────────────────┘   │\n│                                             │                                               │\n│         ┌───────────────────────────────────┼───────────────────────────────────┐          │\n│         │                                   │                                   │          │\n│         ▼                                   ▼                                   ▼          │\n│  ┌─────────────┐                    ┌─────────────┐                    ┌─────────────┐    │\n│  │ PostgreSQL  │                    │  RabbitMQ   │                    │    Redis    │    │\n│  │ (beat DB)   │                    │  (beat.dx)  │                    │ (rate limit)│    │\n│  └─────────────┘                    └──────┬──────┘                    └─────────────┘    │\n└─────────────────────────────────────────────┼───────────────────────────────────────────────┘\n                                              │\n                    ┌─────────────────────────┼─────────────────────────┐\n                    │                         │                         │\n                    ▼                         ▼                         ▼\n           ┌───────────────┐         ┌───────────────┐         ┌───────────────┐\n           │post_log_events│         │profile_log_   │         │sentiment_log_ │\n           │_q (20 workers)│         │events_q       │         │events_q       │\n           └───────┬───────┘         └───────┬───────┘         └───────┬───────┘\n                   │                         │                         │\n                   └─────────────────────────┼─────────────────────────┘\n                                             │\n                                             ▼\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                    EVENT-GRPC (Go)                                           │\n│  ┌─────────────────────────────────────────────────────────────────────────────────────┐   │\n│  │  • 26 RabbitMQ consumer queues                                                        │   │\n│  │  • Buffered sinkers (1000 events/batch, 5-sec flush)                                  │   │\n│  │  • Writes to ClickHouse (_e.* tables)                                                 │   │\n│  │  • 70+ concurrent workers                                                             │   │\n│  └─────────────────────────────────────────────────────────────────────────────────────┘   │\n│                                             │                                               │\n│                                             ▼                                               │\n│                                    ┌───────────────┐                                       │\n│                                    │  ClickHouse   │                                       │\n│                                    │ (_e database) │                                       │\n│                                    │ 21+ tables    │                                       │\n│                                    └───────────────┘                                       │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n                                             │\n                                             ▼\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                    STIR (Airflow + dbt)                                      │\n│  ┌─────────────────────────────────────────────────────────────────────────────────────┐   │\n│  │  • 76 Airflow DAGs                                                                    │   │\n│  │  • 112 dbt models (29 staging + 83 marts)                                             │   │\n│  │  • Reads from beat_replica + coffee_replica + _e (events)                             │   │\n│  │  • Transforms in ClickHouse (dbt schema)                                              │   │\n│  │  • Syncs to PostgreSQL via S3                                                         │   │\n│  └─────────────────────────────────────────────────────────────────────────────────────┘   │\n│         │                                                                    │              │\n│         │ SOURCE                                                      SINK   │              │\n│         ▼                                                                    ▼              │\n│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐     │\n│  │ beat_replica    │    │ coffee_replica  │    │  S3 (staging)   │    │ PostgreSQL  │     │\n│  │ (ClickHouse)    │    │ (ClickHouse)    │    │  JSON files     │    │ (beat DB)   │     │\n│  └─────────────────┘    └─────────────────┘    └─────────────────┘    └─────────────┘     │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n                                             │\n                                             │ Publishes upsert events\n                                             ▼\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                      COFFEE (Go)                                             │\n│  ┌─────────────────────────────────────────────────────────────────────────────────────┐   │\n│  │  • REST API for SaaS platform                                                         │   │\n│  │  • Multi-tenant architecture                                                          │   │\n│  │  • Calls Beat API for real-time data                                                  │   │\n│  │  • Consumes upsert events from stir                                                   │   │\n│  │  • Manages collections, discovery, campaigns                                          │   │\n│  └─────────────────────────────────────────────────────────────────────────────────────┘   │\n│         │                         │                         │                              │\n│         ▼                         ▼                         ▼                              │\n│  ┌─────────────┐          ┌─────────────┐          ┌─────────────┐                        │\n│  │ PostgreSQL  │          │   Redis     │          │  RabbitMQ   │                        │\n│  │ (coffee DB) │          │  (cache)    │          │ (events)    │                        │\n│  └─────────────┘          └─────────────┘          └─────────────┘                        │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n                                             │\n                                             ▼\n                                    ┌───────────────┐\n                                    │   SaaS UI     │\n                                    │   (Frontend)  │\n                                    └───────────────┘\n\\`\\`\\`\n\n---\n\n# DATA FLOWS\n\n## FLOW 1: Profile/Post Data Ingestion\n\n\\`\\`\\`\n┌──────────────────────────────────────────────────────────────────────────────────────┐\n│                        PROFILE/POST DATA INGESTION FLOW                               │\n└──────────────────────────────────────────────────────────────────────────────────────┘\n\nStep 1: BEAT scrapes from external APIs\n─────────────────────────────────────────\nInstagram Graph API ─┐\nInstagram RapidAPI ──┼──► beat workers ──► PostgreSQL (instagram_account, instagram_post)\nYouTube Data API ────┘                           │\n                                                 │ make_scrape_log_event()\n                                                 ▼\nStep 2: BEAT publishes events to RabbitMQ\n─────────────────────────────────────────\nbeat.dx exchange\n├── profile_log_events (routing key)\n├── post_log_events\n├── post_activity_log_events\n├── sentiment_log_events\n├── profile_relationship_log_events\n└── scrape_request_log_events\n\nStep 3: EVENT-GRPC consumes and flushes to ClickHouse\n─────────────────────────────────────────────────────\nRabbitMQ queues                    ClickHouse tables\n├── profile_log_events_q ─────────► _e.profile_log_events\n├── post_log_events_q ────────────► _e.post_log_events\n├── sentiment_log_events_q ───────► _e.sentiment_log_events\n└── post_activity_log_events_q ───► _e.post_activity_log_events\n\nBuffered sinker: 1000 events/batch OR 5-second flush\n\nStep 4: STIR transforms data with dbt\n─────────────────────────────────────\nSources (ClickHouse):\n├── beat_replica.instagram_account\n├── beat_replica.instagram_post\n├── _e.profile_log_events\n└── _e.post_log_events\n        │\n        │ dbt run --models tag:core\n        ▼\nMarts (ClickHouse dbt schema):\n├── mart_instagram_account\n├── mart_youtube_account\n├── mart_leaderboard\n├── mart_time_series\n└── mart_genre_overview\n\nStep 5: STIR syncs to PostgreSQL for COFFEE\n───────────────────────────────────────────\nClickHouse (dbt.mart_leaderboard)\n        │\n        │ INSERT INTO FUNCTION s3(...)\n        ▼\nS3 (gcc-social-data/data-pipeline/tmp/leaderboard.json)\n        │\n        │ SSH download to pg server\n        ▼\n/tmp/leaderboard.json\n        │\n        │ COPY + atomic table swap\n        ▼\nPostgreSQL (leaderboard table)\n\nStep 6: COFFEE serves data via REST API\n───────────────────────────────────────\nSaaS UI ──► coffee API ──► PostgreSQL (leaderboard, instagram_account, etc.)\n\\`\\`\\`\n\n---\n\n## FLOW 2: Real-Time Profile Lookup\n\n\\`\\`\\`\n┌──────────────────────────────────────────────────────────────────────────────────────┐\n│                        REAL-TIME PROFILE LOOKUP FLOW                                  │\n└──────────────────────────────────────────────────────────────────────────────────────┘\n\nUser searches for \"@virat.kohli\" in SaaS UI\n        │\n        ▼\nCoffee API: GET /discovery/instagram/byhandle/virat.kohli\n        │\n        │ Check PostgreSQL first\n        ▼\n┌─────────────────────────────────────────────────────┐\n│ SELECT * FROM instagram_account WHERE handle = ?    │\n└─────────────────────────────────────────────────────┘\n        │\n        │ If NOT FOUND:\n        ▼\nCoffee calls Beat API\n        │\n        │ GET http://beat.goodcreator.co/profiles/INSTAGRAM/byhandle/virat.kohli\n        ▼\n┌─────────────────────────────────────────────────────┐\n│ Beat:                                               │\n│ 1. Check rate limits (Redis)                        │\n│ 2. Get credential (PostgreSQL)                      │\n│ 3. Call Instagram Graph API                         │\n│ 4. Parse response                                   │\n│ 5. Save to PostgreSQL                               │\n│ 6. Publish event to RabbitMQ                        │\n│ 7. Return response to Coffee                        │\n└─────────────────────────────────────────────────────┘\n        │\n        ▼\nCoffee receives profile data\n        │\n        │ Transform and save\n        ▼\n┌─────────────────────────────────────────────────────┐\n│ INSERT INTO instagram_account (...) VALUES (...)    │\n│ + Create campaign_profile                           │\n│ + Enrich with keywords, location                    │\n└─────────────────────────────────────────────────────┘\n        │\n        │ Return to UI\n        ▼\nUser sees profile details\n\\`\\`\\`\n\n---\n\n## FLOW 3: Time-Series Analytics\n\n\\`\\`\\`\n┌──────────────────────────────────────────────────────────────────────────────────────┐\n│                        TIME-SERIES ANALYTICS FLOW                                     │\n└──────────────────────────────────────────────────────────────────────────────────────┘\n\nEvery profile crawl generates a snapshot\n        │\n        ▼\nBeat: make_scrape_log_event(\"profile_log\", {\n    profile_id: \"123\",\n    followers: 10000000,\n    following: 500,\n    timestamp: \"2024-01-15 10:30:00\"\n})\n        │\n        │ RabbitMQ\n        ▼\nevent-grpc: BufferProfileLogEvents()\n        │\n        │ Batch insert\n        ▼\nClickHouse: _e.profile_log_events\n┌─────────────────────────────────────────────────────────────────────────────────┐\n│ profile_id │ followers │ following │ event_timestamp      │ insert_timestamp    │\n├────────────┼───────────┼───────────┼──────────────────────┼─────────────────────┤\n│ 123        │ 10000000  │ 500       │ 2024-01-15 10:30:00  │ 2024-01-15 10:30:05 │\n│ 123        │ 10050000  │ 502       │ 2024-01-16 10:30:00  │ 2024-01-16 10:30:05 │\n│ 123        │ 10100000  │ 505       │ 2024-01-17 10:30:00  │ 2024-01-17 10:30:05 │\n└─────────────────────────────────────────────────────────────────────────────────┘\n        │\n        │ dbt model (stir)\n        ▼\ndbt.mart_time_series\n┌─────────────────────────────────────────────────────┐\n│ SELECT                                              │\n│     profile_id,                                     │\n│     toDate(event_timestamp) as date,                │\n│     argMax(followers, event_timestamp) as followers │\n│ FROM _e.profile_log_events                          │\n│ GROUP BY profile_id, date                           │\n└─────────────────────────────────────────────────────┘\n        │\n        │ Sync to PostgreSQL\n        ▼\nCoffee API: GET /analytics/timeseries/{profile_id}\n        │\n        │ Query PostgreSQL\n        ▼\nUser sees follower growth chart\n\\`\\`\\`\n\n---\n\n# DETAILED CONNECTION MAPS\n\n## BEAT → Everything Else\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                    BEAT CONNECTIONS                                          │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\nBEAT PUBLISHES TO:\n─────────────────\nRabbitMQ Exchanges:\n├── beat.dx (main exchange)\n│   ├── profile_log_events ──────────────────────► event-grpc → ClickHouse\n│   ├── post_log_events ─────────────────────────► event-grpc → ClickHouse\n│   ├── post_activity_log_events ────────────────► event-grpc → ClickHouse\n│   ├── sentiment_log_events ────────────────────► event-grpc → ClickHouse\n│   ├── profile_relationship_log_events ─────────► event-grpc → ClickHouse\n│   ├── scrape_request_log_events ───────────────► event-grpc → ClickHouse\n│   ├── order_log_events ────────────────────────► event-grpc → ClickHouse\n│   └── keyword_collection_rk ───────────────────► beat (internal job processing)\n│\n├── coffee.dx\n│   ├── keyword_collection_report_completion ────► coffee (report ready notification)\n│   └── sentiment_collection_report_out_rk ──────► coffee (sentiment report ready)\n│\n└── identity.dx\n    ├── trace_log ───────────────────────────────► event-grpc → ClickHouse\n    └── access_token_expired_rk ─────────────────► identity service\n\n\nBEAT CONSUMES FROM:\n──────────────────\nRabbitMQ:\n├── identity.dx / new_access_token_rk ───────────► Update credentials\n├── beat.dx / credentials_validate_rk ───────────► Validate tokens\n├── beat.dx / keyword_collection_rk ─────────────► Generate keyword reports\n├── beat.dx / post_activity_log_bulk ────────────► Sentiment extraction\n└── beat.dx / sentiment_collection_report_in_rk ─► Generate sentiment reports\n\n\nBEAT READS FROM:\n───────────────\nPostgreSQL (beat database):\n├── credential\n├── scrape_request_log (task queue)\n├── instagram_account\n├── instagram_post\n├── youtube_account\n└── youtube_post\n\nClickHouse (dbt schema):\n├── dbt.stg_coffee_post_collection_item\n├── dbt.stg_coffee_post_collection\n└── dbt.mart_genre_overview\n\n\nBEAT WRITES TO:\n──────────────\nPostgreSQL (beat database):\n├── All tables above (upserts)\n└── profile_log, post_log (audit tables)\n\nS3 (gcc-social-data bucket):\n├── keyword_collections/{date}/{job_id}.parquet\n├── sentiment_reports/{date}/{job_id}.parquet\n└── assets/{entity_type}/{platform}/{id}.jpg\n\n\nBEAT CALLS APIs:\n───────────────\n├── Instagram Graph API (graph.facebook.com)\n├── YouTube Data API (googleapis.com)\n├── RapidAPI (multiple providers)\n├── Identity Service (identityservice.bulbul.tv)\n├── RAY ML Service (ray.goodcreator.co)\n│   ├── CATEGORIZER model\n│   └── SENTIMENT model\n└── Azure OpenAI (gcc-openai.openai.azure.com)\n\\`\\`\\`\n\n---\n\n## EVENT-GRPC → Everything Else\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                  EVENT-GRPC CONNECTIONS                                      │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\nEVENT-GRPC CONSUMES FROM:\n────────────────────────\nRabbitMQ (26 queues total):\n\nFrom beat.dx:\n├── post_log_events_q (20 workers) ────────────► PostLogEventsSinker\n├── profile_log_events_q (2 workers) ──────────► ProfileLogEventsSinker\n├── sentiment_log_events_q (2 workers) ────────► SentimentLogEventsSinker\n├── post_activity_log_events_q (2 workers) ────► PostActivityLogEventsSinker\n├── profile_relationship_log_events_q (2) ─────► ProfileRelationshipLogEventsSinker\n├── scrape_request_log_events_q (2 workers) ───► ScrapeRequestLogEventsSinker\n└── order_log_events_q (2 workers) ────────────► OrderLogEventsSinker\n\nFrom identity.dx:\n└── trace_log ─────────────────────────────────► TraceLogEventsSinker\n\nFrom coffee.dx:\n└── activity_tracker_q ────────────────────────► PartnerActivityLogEventsSinker\n\nFrom other exchanges:\n├── grpc_event.tx / grpc_clickhouse_event_q ───► SinkEventToClickhouse\n├── ab.dx / ab_assignments ────────────────────► SinkABAssignmentsToClickhouse\n├── branch_event.tx / branch_event_q ──────────► SinkBranchEventToClickhouse\n├── webengage_event.dx / webengage_ch_event_q ─► SinkWebengageEventToClickhouse\n└── shopify_event.dx / shopify_events_q ───────► BufferShopifyEvents\n\n\nEVENT-GRPC WRITES TO:\n────────────────────\nClickHouse (_e database - 21+ tables):\n├── profile_log_events\n├── post_log_events\n├── sentiment_log_events\n├── post_activity_log_events\n├── profile_relationship_log_events\n├── scrape_request_log_events\n├── order_log_events\n├── trace_log_events\n├── partner_activity_log_events\n├── event\n├── error_event\n├── ab_assignment\n├── branch_event\n├── affiliate_order_event\n├── bigboss_vote_log\n├── shopify_event\n└── webengage_event\n\n\nBUFFERED SINKER PATTERN:\n───────────────────────\n┌─────────────────────────────────────────────────────┐\n│ Channel buffer: 10,000 messages                     │\n│ Batch size: 1,000 events                            │\n│ Flush interval: 5 seconds                           │\n│ Flush condition: batch full OR timer tick           │\n└─────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n---\n\n## STIR → Everything Else\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                    STIR CONNECTIONS                                          │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\nSTIR READS FROM (ClickHouse):\n────────────────────────────\nbeat_replica schema (PostgreSQL replica):\n├── instagram_account\n├── instagram_post_simple\n├── youtube_account\n├── youtube_post_simple\n├── profile_log\n├── post_activity_log\n├── credential\n├── scrape_request_log\n├── asset_log\n├── instagram_profile_insights\n└── youtube_profile_insights\n\ncoffee_replica schema (PostgreSQL replica):\n├── post_collection\n├── post_collection_item\n├── profile_collection\n├── profile_collection_item\n├── keyword_collection\n├── collection_group\n├── activity_tracker\n├── view_instagram_account_lite\n└── view_youtube_account_lite\n\n_e schema (event-grpc writes):\n├── profile_log_events\n├── post_log_events\n├── sentiment_log_events\n└── post_activity_log_events\n\n\nSTIR TRANSFORMS IN (ClickHouse dbt schema):\n──────────────────────────────────────────\nStaging models (29):\n├── stg_beat_instagram_account\n├── stg_beat_instagram_post\n├── stg_beat_youtube_account\n├── stg_beat_profile_log\n├── stg_coffee_post_collection\n├── stg_coffee_post_collection_item\n└── ...\n\nMart models (83):\n├── mart_instagram_account\n├── mart_youtube_account\n├── mart_leaderboard\n├── mart_time_series\n├── mart_genre_overview\n├── mart_trending_content\n├── mart_collection_post\n├── mart_instagram_hashtags\n└── ...\n\n\nSTIR SYNCS TO (PostgreSQL beat database):\n────────────────────────────────────────\nThree-layer sync pattern:\nClickHouse dbt.mart_* → S3 JSON → PostgreSQL\n\nTarget tables:\n├── leaderboard\n├── time_series\n├── genre_overview\n├── trending_content\n├── collection_post_metrics_summary\n├── collection_post_metrics_ts\n├── social_profile_hashtags\n├── collection_hashtag\n├── collection_keyword\n└── group_metrics\n\n\nSTIR PUBLISHES TO (RabbitMQ):\n────────────────────────────\nupserttracker.dx exchange:\n├── upsert_instagram_account_rk ───► coffee (profile updates)\n└── upsert_youtube_account_rk ─────► coffee (profile updates)\n\\`\\`\\`\n\n---\n\n## COFFEE → Everything Else\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                   COFFEE CONNECTIONS                                         │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\nCOFFEE CALLS APIs:\n─────────────────\nBeat API (http://beat.goodcreator.co):\n├── GET /profiles/{platform}/byhandle/{handle}\n├── GET /profiles/{platform}/byid/{id}\n├── GET /recent/posts/{platform}/byprofileid/{id}\n├── GET /profiles/INSTAGRAM/byhandle/{handle}/insights\n├── GET /profiles/INSTAGRAM/byhandle/{handle}/audienceinsights\n└── GET /youtube/channel/byhandle/{handle}\n\nOther services:\n├── JobTracker (jobtrackerservice.bulbul.tv) - Async job management\n├── DAM (damservice.bulbul.tv) - Digital asset management\n├── Partner Service (productservice.bulbul.tv) - Partner contracts\n└── Identity Service - Authentication\n\n\nCOFFEE READS FROM:\n─────────────────\nPostgreSQL (coffee database):\n├── instagram_account\n├── youtube_account\n├── campaign_profiles\n├── profile_collection\n├── profile_collection_item\n├── post_collection\n├── post_collection_item\n├── keyword_collection\n├── collection_analytics\n└── activity_tracker\n\nClickHouse (dbt schema):\n└── Analytics queries for time-series data\n\n\nCOFFEE CONSUMES FROM (RabbitMQ):\n───────────────────────────────\nupserttracker.dx (from stir):\n├── upsert_instagram_account_q ──► PerformUpsertInstagramAccount()\n└── upsert_youtube_account_q ────► PerformUpsertYoutubeAccount()\n\njobtracker.dx:\n├── duplicate_collection_q\n├── download_collection_q\n├── import_from_profile_collection_q\n├── add_item_profile_collection_q\n└── add_item_post_collection_q\n\n\nCOFFEE PUBLISHES TO (RabbitMQ):\n─────────────────────────────\nbeat.dx:\n└── keyword_collection_rk ──────► beat (trigger keyword report)\n\ncoffee.dx:\n└── activity_tracker_rk ────────► event-grpc → ClickHouse\n\n\nCOFFEE CACHES IN (Redis):\n────────────────────────\n├── partnercontract-{partnerId} (12h TTL)\n└── Session data\n\\`\\`\\`\n\n---\n\n# DATABASE CONNECTIONS SUMMARY\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                              DATABASE CONNECTIONS MAP                                        │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\nPostgreSQL (172.31.2.21:5432)\n├── beat database\n│   ├── Written by: beat, stir (sync)\n│   └── Read by: beat, coffee\n│\n└── coffee database\n    ├── Written by: coffee\n    └── Read by: coffee\n\n\nClickHouse (172.31.28.68:9000)\n├── _e database (events)\n│   ├── Written by: event-grpc\n│   └── Read by: stir\n│\n├── beat_replica schema\n│   ├── Written by: ClickHouse replication\n│   └── Read by: stir (dbt sources)\n│\n├── coffee_replica schema\n│   ├── Written by: ClickHouse replication\n│   └── Read by: stir (dbt sources)\n│\n└── dbt schema (transformations)\n    ├── Written by: stir (dbt run)\n    └── Read by: stir (for sync), beat (for reports)\n\n\nRedis Cluster\n├── beat: Rate limiting, credential state\n└── coffee: Partner contract cache\n\n\nS3 (gcc-social-data bucket)\n├── Written by: beat (reports, assets), stir (sync files)\n└── Read by: stir (sync to PostgreSQL), CDN (assets)\n\\`\\`\\`\n\n---\n\n# RABBITMQ EXCHANGE/QUEUE MAP\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│                                   RABBITMQ TOPOLOGY                                          │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n\nbeat.dx (exchange)\n├── profile_log_events ────────► profile_log_events_q ────► event-grpc\n├── post_log_events ───────────► post_log_events_q ────────► event-grpc\n├── sentiment_log_events ──────► sentiment_log_events_q ───► event-grpc\n├── post_activity_log_events ──► post_activity_log_events_q ► event-grpc\n├── scrape_request_log_events ─► scrape_request_log_events_q ► event-grpc\n├── keyword_collection_rk ─────► keyword_collection_q ─────► beat\n└── credentials_validate_rk ───► credentials_validate_q ───► beat\n\nidentity.dx (exchange)\n├── trace_log ─────────────────► trace_log ────────────────► event-grpc\n├── new_access_token_rk ───────► identity_token_q ─────────► beat\n└── access_token_expired_rk ───► identity service\n\ncoffee.dx (exchange)\n├── activity_tracker_rk ───────► activity_tracker_q ───────► event-grpc\n└── keyword_collection_report_completion ──────────────────► coffee\n\nupserttracker.dx (exchange)\n├── upsert_instagram_account ──► upsert_instagram_account_q ► coffee\n└── upsert_youtube_account ────► upsert_youtube_account_q ──► coffee\n\\`\\`\\`\n\n---\n\n# INTERVIEW EXPLANATION\n\n**\"Explain how your systems work together\"**\n\n> \"We built a microservices architecture for social media analytics:\n>\n> **The Data Flow:**\n>\n> 1. **beat** (Python) scrapes Instagram/YouTube using 150+ workers with rate limiting\n>\n> 2. Instead of direct database writes for time-series data, beat publishes events to **RabbitMQ**\n>\n> 3. **event-grpc** (Go) consumes these events with buffered sinkers (1000 events/batch) and writes to **ClickHouse**\n>\n> 4. **stir** (Airflow + dbt) transforms the raw data in ClickHouse into analytics-ready marts, then syncs to PostgreSQL via S3\n>\n> 5. **coffee** (Go) serves the REST API, reading from PostgreSQL for transactional queries and calling beat for real-time lookups\n>\n> **Why this architecture?**\n>\n> - **Separation of concerns**: OLTP (PostgreSQL) vs OLAP (ClickHouse)\n> - **Event-driven**: Decouples producers from consumers\n> - **Scalability**: Each component can scale independently\n> - **Reliability**: Buffered sinkers prevent data loss\n> - **Performance**: ClickHouse for fast analytics, PostgreSQL for transactional consistency\"\n\n---\n\n*This document shows the complete interconnectivity between all 5 systems in your work experience.*\n"
  },
  {
    "id": "BEAT_ADVANCED_FEATURES",
    "title": "ML/Stats Features",
    "category": "technical",
    "badge": null,
    "content": "# BEAT - ADVANCED FEATURES DEEP DIVE\n## ML, Statistics, and Data Science Components You Built\n\n---\n\n# OVERVIEW\n\n| Feature | File | Lines | Complexity |\n|---------|------|-------|------------|\n| Gradient Descent Optimization | gpt/helper.py | 172 | High |\n| Reach Estimation Formulas | instagram/helper.py | 31 | Medium |\n| 14-Dimension Demographics | gpt/helper.py | 172 | High |\n| YAKE Keyword Extraction | utils/extracted_keyword.py | 29 | Medium |\n| RAY ML Service Integration | keyword_collection/categorization.py, utils/sentiment_analysis.py | 78 | Medium |\n\n---\n\n# FEATURE 1: GRADIENT DESCENT FOR AUDIENCE NORMALIZATION\n\n## File: \\`beat/gpt/helper.py\\` (lines 78-119)\n\n### What Problem Did This Solve?\n\nGPT returns audience demographics that are:\n1. **Not normalized** - percentages don't sum to 100%\n2. **Not realistic** - may not match typical category patterns\n\nExample GPT output for a fitness influencer:\n\\`\\`\\`python\n{\n    \"18-24 male\": 0.35,    # 35%\n    \"18-24 female\": 0.15,  # 15%\n    \"25-34 male\": 0.25,    # 25%\n    \"25-34 female\": 0.10,  # 10%\n    # ... sum = 1.1 (not 1.0!)\n}\n\\`\\`\\`\n\n### Your Solution: Gradient Descent Optimization\n\n\\`\\`\\`python\n# gpt/helper.py - Lines 71-93\n\ndef ssd(a, b):\n    \"\"\"Sum of Squared Differences - Loss Function\"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    dif = a.ravel() - b.ravel()\n    return np.dot(dif, dif)\n\n\ndef gradient_descent(a, b, learning_rate=0.01, epochs=1000):\n    \"\"\"\n    Gradient Descent to converge array 'a' towards baseline 'b'\n\n    Parameters:\n    - a: GPT output (actual audience distribution)\n    - b: Category baseline (expected distribution from historical data)\n    - learning_rate: Step size (0.01 = 1% adjustment per epoch)\n    - epochs: Number of iterations (50-100 randomly chosen)\n\n    Returns:\n    - Optimized array that blends GPT output with category baseline\n    \"\"\"\n    if len(a) != len(b):\n        raise ValueError(\"Arrays must have the same length\")\n\n    a = np.array(a)\n    b = np.array(b)\n\n    for epoch in range(epochs):\n        # Gradient of MSE loss: d/da[(b-a)²] = -2(b-a)\n        gradient = -2 * (b - a)\n\n        # Update rule: a = a - learning_rate * gradient\n        # This moves 'a' towards 'b' by small steps\n        a -= learning_rate * gradient\n\n    return a.tolist()\n\\`\\`\\`\n\n### How It's Used\n\n\\`\\`\\`python\n# gpt/helper.py - Lines 96-119\n\ndef normalize_audience_age_gender(audience_age_gender_data, category):\n    \"\"\"\n    Full normalization pipeline:\n    1. Normalize sum to 1.0\n    2. Load category baseline from CSV\n    3. Apply gradient descent to blend with baseline\n    \"\"\"\n    age_gender = audience_age_gender_data['audience']['age_gender']\n\n    # Step 1: Normalize to sum = 1.0\n    total_percentage = sum(age_gender.values())\n    if total_percentage != 1.0:\n        normalization_factor = 1 / total_percentage\n        for age, value in age_gender.items():\n            age_gender[age] = value * normalization_factor\n\n    # Step 2: Load category baseline (e.g., \"fitness\" has more young males)\n    file_path = 'gpt/age_gender_private_data.csv'\n    df = pd.read_csv(file_path)\n\n    if not category or category not in df['categories'].values:\n        category = 'Missing'\n\n    # Get baseline distribution for this category\n    filtered_rows = df[df['categories'] == category]\n    baseline = filtered_rows.values.tolist()[0][1:]  # Skip category name\n\n    # Step 3: Apply gradient descent\n    a = list(age_gender.values())  # GPT output\n    b = baseline                    # Category baseline\n\n    # Random epochs (50-100) adds variance, prevents overfitting\n    result = gradient_descent(a, b, epochs=random.randint(50, 100))\n\n    # Step 4: Update with optimized values\n    for i, (age, value) in enumerate(age_gender.items()):\n        audience_age_gender_data['audience']['age_gender'][age] = round(result[i], 3)\n\\`\\`\\`\n\n### Why Gradient Descent?\n\n| Approach | Problem |\n|----------|---------|\n| **Just normalize sum to 1.0** | Still unrealistic distributions |\n| **Use category baseline directly** | Loses GPT's personalized insights |\n| **Gradient Descent** | Blends both - realistic AND personalized |\n\n### Example\n\n\\`\\`\\`python\n# Input from GPT (fitness influencer)\ngpt_output = [0.35, 0.15, 0.25, 0.10, 0.05, 0.05, 0.03, 0.02, 0.00, 0.00, 0.00, 0.00]\n# (Heavy male 18-34 skew)\n\n# Category baseline for \"fitness\" from historical data\nfitness_baseline = [0.25, 0.15, 0.20, 0.15, 0.08, 0.07, 0.05, 0.03, 0.01, 0.01, 0.00, 0.00]\n# (More balanced, based on millions of fitness accounts)\n\n# After 75 epochs of gradient descent\noptimized = [0.30, 0.15, 0.22, 0.12, 0.06, 0.06, 0.04, 0.02, 0.01, 0.01, 0.01, 0.00]\n# (Blended - keeps GPT's male skew but more realistic)\n\\`\\`\\`\n\n### Interview Talking Points\n\n**Q: Why did you use gradient descent for demographic data?**\n\n> \"GPT's audience predictions were useful but not always realistic. For example, it might say 95% male audience, which is rare even for male-focused content. I needed to blend GPT's personalized insights with category baselines.\n>\n> **Solution:**\n> I implemented gradient descent to converge GPT output towards historical baselines:\n> - Loss function: Sum of Squared Differences\n> - Learning rate: 0.01 (small steps to preserve GPT insights)\n> - Epochs: Random 50-100 (adds variance, prevents overfitting)\n>\n> **Math:**\n> \\`\\`\\`\n> gradient = -2 * (baseline - current)\n> current = current - 0.01 * gradient\n> \\`\\`\\`\n>\n> This iteratively moves the distribution towards realistic values while keeping GPT's personalized adjustments.\"\n\n---\n\n# FEATURE 2: REACH ESTIMATION FORMULAS\n\n## File: \\`beat/instagram/helper.py\\` (lines 15-30)\n\n### The Problem\n\nInstagram only provides actual reach for:\n- Business/Creator accounts with Insights access\n- Posts you own\n\nFor most profiles, we only have \\`likes\\`, \\`comments\\`, \\`plays\\` - no reach data.\n\n### Your Solution: Empirical Formulas\n\n\\`\\`\\`python\n# instagram/helper.py - Lines 15-30\n\ndef get_reach(entity: InstagramPost, account: InstagramAccount):\n    \"\"\"\n    Estimate reach when Instagram doesn't provide it\n\n    Formulas derived from 10,000+ posts where we had actual reach data\n    \"\"\"\n    plays = entity.plays or 0\n    likes = entity.likes or 0\n    followers = account.followers or 0\n\n    reach = entity.reach  # Actual reach if available\n\n    # If no actual reach, estimate it\n    if reach is None or reach == 0:\n        if entity.post_type == 'reels':\n            # REELS FORMULA\n            # Larger accounts have lower reach/follower ratio\n            # log2(followers) * 0.001 creates diminishing returns\n            reach = int(plays * (0.94 - (math.log2(followers) * 0.001)))\n        else:\n            # STATIC POST FORMULA (image, carousel)\n            # Based on likes-to-reach correlation\n            reach = int((7.6 - (math.log10(likes) * 0.7)) * 0.85 * likes)\n\n    return reach\n\\`\\`\\`\n\n### Formula Derivation\n\n#### Reels Formula: \\`plays * (0.94 - log2(followers) * 0.001)\\`\n\n\\`\\`\\`\nExample calculations:\n\nMicro-influencer (10K followers):\n- plays = 50,000\n- factor = 0.94 - (log2(10000) * 0.001) = 0.94 - 0.0133 = 0.9267\n- reach = 50,000 * 0.9267 = 46,335\n\nMacro-influencer (1M followers):\n- plays = 500,000\n- factor = 0.94 - (log2(1000000) * 0.001) = 0.94 - 0.020 = 0.920\n- reach = 500,000 * 0.920 = 460,000\n\nMega-influencer (10M followers):\n- plays = 5,000,000\n- factor = 0.94 - (log2(10000000) * 0.001) = 0.94 - 0.023 = 0.917\n- reach = 5,000,000 * 0.917 = 4,585,000\n\\`\\`\\`\n\n**Why this formula?**\n- Larger accounts have **lower organic reach %** (Instagram algorithm)\n- \\`log2(followers)\\` creates **logarithmic decay** (not linear)\n- 0.94 base factor means ~94% of plays = reach (slight drop-off)\n\n#### Static Post Formula: \\`(7.6 - log10(likes) * 0.7) * 0.85 * likes\\`\n\n\\`\\`\\`\nExample calculations:\n\nLow engagement post (100 likes):\n- factor = 7.6 - (log10(100) * 0.7) = 7.6 - 1.4 = 6.2\n- reach = 6.2 * 0.85 * 100 = 527\n\nMedium engagement (1000 likes):\n- factor = 7.6 - (log10(1000) * 0.7) = 7.6 - 2.1 = 5.5\n- reach = 5.5 * 0.85 * 1000 = 4,675\n\nHigh engagement (10000 likes):\n- factor = 7.6 - (log10(10000) * 0.7) = 7.6 - 2.8 = 4.8\n- reach = 4.8 * 0.85 * 10000 = 40,800\n\\`\\`\\`\n\n**Why this formula?**\n- Reach-to-likes ratio **decreases** as engagement increases (diminishing returns)\n- \\`log10(likes)\\` captures this decay\n- 0.85 is a calibration factor from actual data\n\n### SQL Version (ClickHouse)\n\n\\`\\`\\`sql\n-- keyword_collection/generate_instagram_report.py - Lines 104-106\n\n-- Reach estimation in SQL for batch processing\npost_plays * (0.94 - (log2(followers) * 0.001)) AS _reach_reels,\n(7.6 - (log10(post_likes) * 0.7)) * 0.85 * post_likes AS _reach_non_reels,\nif(post_class = 'reels', max2(_reach_reels, 0), max2(_reach_non_reels, 0)) AS reach\n\\`\\`\\`\n\n### Interview Talking Points\n\n**Q: How did you derive the reach estimation formulas?**\n\n> \"Instagram doesn't provide reach data for most profiles. I derived empirical formulas by:\n>\n> 1. **Data collection**: Gathered 10,000+ posts where we had actual reach (business accounts with Insights)\n> 2. **Regression analysis**: Found correlation between likes/plays and reach\n> 3. **Key insight**: Larger accounts have lower reach-to-follower ratios (Instagram algorithm throttling)\n>\n> **Formulas:**\n> - Reels: \\`plays * (0.94 - log2(followers) * 0.001)\\`\n> - Static: \\`(7.6 - log10(likes) * 0.7) * 0.85 * likes\\`\n>\n> The logarithmic terms capture the diminishing returns effect - a post with 10x more likes doesn't get 10x more reach.\"\n\n---\n\n# FEATURE 3: 14-DIMENSION DEMOGRAPHICS\n\n## File: \\`beat/gpt/helper.py\\` (lines 9-68)\n\n### The 14 Dimensions\n\n\\`\\`\\`python\n# 7 age groups × 2 genders = 14 dimensions\nkeys_to_check = [\n    \"18-24 male\",   \"18-24 female\",\n    \"25-34 male\",   \"25-34 female\",\n    \"35-44 male\",   \"35-44 female\",\n    \"45-54 male\",   \"45-54 female\",\n    \"55-64 male\",   \"55-64 female\",\n    \"65+ male\",     \"65+ female\"\n]\n# Note: 13-17 age group exists but often excluded for compliance\n\\`\\`\\`\n\n### Data Quality Validation\n\n\\`\\`\\`python\ndef is_data_consumable(data: dict, data_type: str) -> bool:\n    \"\"\"\n    Validate GPT output quality before storing\n\n    For audience_age_gender, we check:\n    1. All 14 keys present\n    2. Percentages in valid range (0-1.0)\n    3. No duplicate keys\n    4. Both genders have minimum representation (>0.25%)\n    \"\"\"\n    if data_type == \"audience_age_gender\":\n        # Check structure exists\n        if \"audience\" not in data or \"age_gender\" not in data['audience']:\n            return False\n\n        age_gender = data['audience']['age_gender']\n\n        # Check all 14 keys present\n        if not all(key in age_gender for key in keys_to_check):\n            return False\n\n        # Check valid percentage ranges\n        if any(value < 0 or value > 1.0 for value in age_gender.values()):\n            return False\n\n        # Check no duplicates (sanity check)\n        if len(set(age_gender.keys())) != len(age_gender.keys()):\n            return False\n\n        # Check minimum representation (avoid 99% male / 1% female)\n        total_male = sum(v for k, v in age_gender.items() if \"male\" in k)\n        total_female = sum(v for k, v in age_gender.items() if \"female\" in k)\n\n        if total_male < 0.25 or total_female < 0.25:\n            return False\n\n    return True\n\\`\\`\\`\n\n### Demographic Report Aggregation\n\n\\`\\`\\`sql\n-- keyword_collection/generate_instagram_report.py - Lines 226-250\n\n-- Aggregate demographics from mart_genre_overview for keyword collection\nWITH categories AS (\n    SELECT DISTINCT label\n    FROM post_log_events\n    WHERE source = 'categorization'\n      AND score > 0.50  -- Only high-confidence categorizations\n),\ncategorization AS (\n    SELECT\n        category,\n        male_audience_age_gender,    -- JSON: {\"18-24\": 0.25, \"25-34\": 0.30, ...}\n        female_audience_age_gender,  -- JSON: {\"18-24\": 0.15, \"25-34\": 0.20, ...}\n        audience_age,                -- JSON: {\"18-24\": 0.40, \"25-34\": 0.50, ...}\n        audience_gender              -- JSON: {\"male\": 0.60, \"female\": 0.40}\n    FROM dbt.mart_genre_overview\n    WHERE category IN categories\n)\n\\`\\`\\`\n\n### Interview Talking Points\n\n**Q: How did you handle audience demographics?**\n\n> \"We tracked 14-dimensional demographics (7 age groups × 2 genders) for every influencer:\n>\n> **Data Flow:**\n> 1. GPT analyzes profile bio and posts → outputs audience breakdown\n> 2. Validation: Check all 14 dimensions present, percentages valid, minimum representation\n> 3. Normalization: Sum to 1.0, apply gradient descent with category baseline\n> 4. Storage: ClickHouse for analytics, PostgreSQL for API serving\n>\n> **Why 14 dimensions?**\n> - Industry standard (Meta, Google ads use similar breakdowns)\n> - Granular enough for targeting, not too sparse\n> - Enables cross-category comparisons\"\n\n---\n\n# FEATURE 4: YAKE KEYWORD EXTRACTION\n\n## File: \\`beat/utils/extracted_keyword.py\\` (29 lines)\n\n### What is YAKE?\n\n**YAKE** (Yet Another Keyword Extractor) is an **unsupervised** keyword extraction algorithm that:\n- Doesn't require training data\n- Works on single documents\n- Language-independent\n- Fast and lightweight\n\n### Your Implementation\n\n\\`\\`\\`python\n# utils/extracted_keyword.py\n\nimport re\nimport yake\n\n\ndef remove_numeric_and_emojis(text):\n    \"\"\"\n    Preprocessing: Remove noise from captions\n\n    Removes:\n    - Numeric characters (phone numbers, dates)\n    - Emojis (Unicode ranges for emoticons, symbols, flags)\n    - Non-alpha characters (special symbols)\n    \"\"\"\n    # Remove numbers\n    text = re.sub(r'\\\\d+', '', text)\n\n    # Remove emojis (comprehensive Unicode pattern)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\\\U0001F600-\\\\U0001F64F\"  # emoticons\n                               u\"\\\\U0001F300-\\\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\\\U0001F680-\\\\U0001F6FF\"  # transport & map symbols\n                               u\"\\\\U0001F1E0-\\\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    # Keep only letters and spaces\n    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)\n\n    return text.strip()\n\n\ndef get_extracted_keywords(s: str) -> str:\n    \"\"\"\n    Extract top 5 keywords from text using YAKE\n\n    Parameters:\n    - s: Input text (caption, bio, comment)\n\n    Returns:\n    - String representation of keyword list: \"['fitness', 'workout', 'gym']\"\n\n    YAKE Parameters:\n    - n=1: Extract single words (unigrams)\n    - top=5: Return top 5 keywords\n    \"\"\"\n    kw_extractor = yake.KeywordExtractor(n=1, top=5)\n    extracted_keywords = kw_extractor.extract_keywords(s)\n\n    # YAKE returns [(keyword, score), ...] - we just want keywords\n    extracted_keywords = [item[0] for item in extracted_keywords]\n\n    return str(extracted_keywords)\n\\`\\`\\`\n\n### How YAKE Works\n\n\\`\\`\\`\nYAKE Algorithm:\n\n1. Candidate Selection\n   - Split text into terms\n   - Remove stopwords, punctuation\n\n2. Feature Extraction (5 features per term):\n   - Casing: Is it capitalized? Acronym?\n   - Position: Where in document?\n   - Frequency: How often?\n   - Relatedness: Co-occurrence with other terms\n   - Different sentences: Appears in multiple sentences?\n\n3. Scoring:\n   S(kw) = (Position × Casing × Frequency) / (Relatedness × Sentences)\n   Lower score = better keyword\n\n4. Ranking:\n   Sort by score ascending, return top N\n\\`\\`\\`\n\n### Example\n\n\\`\\`\\`python\ncaption = \"\"\"\n🏋️ Morning workout complete! 💪\nBest fitness tips for beginners:\n1. Start slow\n2. Stay consistent\n3. Eat clean\n#fitness #gym #workout #motivation #health\n\"\"\"\n\nkeywords = get_extracted_keywords(caption)\n# Output: \"['fitness', 'workout', 'tips', 'beginners', 'gym']\"\n\\`\\`\\`\n\n### Where It's Used\n\n\\`\\`\\`python\n# instagram/tasks/processing.py - Line 42\n\n# Extract keywords from every post caption\npost_log.dimensions.append(\n    dimension(get_extracted_keywords(caption) if caption else '', KEYWORDS)\n)\n\n# Stored in ClickHouse for:\n# - Keyword search (find posts mentioning \"fitness\")\n# - Trend analysis (what topics are growing?)\n# - Content categorization supplement\n\\`\\`\\`\n\n### Interview Talking Points\n\n**Q: How did you implement keyword extraction?**\n\n> \"I used YAKE (Yet Another Keyword Extractor) for unsupervised keyword extraction:\n>\n> **Why YAKE?**\n> - No training required (works on any language/domain)\n> - Single document focus (perfect for social media posts)\n> - Fast (milliseconds per document)\n>\n> **Pipeline:**\n> 1. Preprocess: Remove emojis, numbers, special characters\n> 2. Extract: YAKE with n=1 (unigrams), top=5 keywords\n> 3. Store: As dimension in ClickHouse post_log_events\n>\n> **Use cases:**\n> - Keyword search across millions of posts\n> - Trending topic detection\n> - Content categorization augmentation\"\n\n---\n\n# FEATURE 5: RAY ML SERVICE INTEGRATION\n\n## Files:\n- \\`beat/keyword_collection/categorization.py\\` (46 lines)\n- \\`beat/utils/sentiment_analysis.py\\` (32 lines)\n\n### Architecture\n\n\\`\\`\\`\n┌─────────────────┐         ┌──────────────────┐\n│      beat       │  HTTP   │   RAY ML Server  │\n│    (Python)     │ ──────► │   (GPU Cluster)  │\n└─────────────────┘         └──────────────────┘\n                                    │\n                    ┌───────────────┼───────────────┐\n                    │               │               │\n                    ▼               ▼               ▼\n             ┌───────────┐   ┌───────────┐   ┌───────────┐\n             │CATEGORIZER│   │ SENTIMENT │   │  (Future) │\n             │   Model   │   │   Model   │   │  Models   │\n             └───────────┘   └───────────┘   └───────────┘\n\\`\\`\\`\n\n### Categorization Model\n\n\\`\\`\\`python\n# keyword_collection/categorization.py\n\nasync def get_categorization(title: str) -> dict:\n    \"\"\"\n    Call RAY ML service for content categorization\n\n    Input: Post caption/title\n    Output: {\n        \"label\": \"fitness\",\n        \"score\": 0.92\n    }\n    \"\"\"\n    headers = {'Content-Type': 'application/json'}\n\n    json_data = {\n        'model': 'CATEGORIZER',\n        'input': {\n            'text': title,\n        },\n    }\n\n    url = os.environ[\"RAY_URL\"]  # e.g., \"http://ml-server:8000/predict\"\n    response = await make_request(\"POST\", url=url, headers=headers, json=json_data)\n\n    return response.json()\n\n\nasync def instagram_categorization(post_log: tuple) -> tuple:\n    \"\"\"\n    Categorize Instagram post and append result\n\n    Input: (shortcode, profile_id, caption)\n    Output: (shortcode, profile_id, caption, {label, score})\n    \"\"\"\n    try:\n        # post_log[2] = caption\n        category_result = await get_categorization(post_log[2])\n        post_log = post_log + (category_result,)\n    except Exception as e:\n        logger.debug(f\"Categorization error: {e}\")\n        post_log = post_log + ({},)  # Empty dict on failure\n\n    return post_log\n\\`\\`\\`\n\n### Sentiment Model\n\n\\`\\`\\`python\n# utils/sentiment_analysis.py\n\nasync def get_sentiment(comments: list) -> dict:\n    \"\"\"\n    Batch sentiment analysis for comments\n\n    Input: [\n        {\"id\": \"123\", \"text\": \"Great post!\"},\n        {\"id\": \"456\", \"text\": \"This is terrible...\"}\n    ]\n\n    Output: {\n        \"123\": {\"sentiment\": \"positive\", \"score\": 0.95},\n        \"456\": {\"sentiment\": \"negative\", \"score\": 0.87}\n    }\n    \"\"\"\n    headers = {'Content-Type': 'application/json'}\n\n    json_data = {\n        'model': 'SENTIMENT',\n        'input': comments,\n    }\n\n    url = os.environ[\"RAY_URL\"]\n    response = await make_request(\"POST\", url=url, headers=headers, json=json_data)\n    return response.json()\n\n\nasync def get_sentiments(input_payload, session=None):\n    \"\"\"\n    Wrapper with retry logic and rate limiting\n    \"\"\"\n    try:\n        response = await get_sentiment(input_payload)\n    except Exception as e:\n        logger.debug(f\"Sentiment error: {e}, retrying...\")\n        await asyncio.sleep(2)  # Wait before retry\n        response = await get_sentiment(input_payload)\n\n    await asyncio.sleep(3)  # Rate limiting between batches\n    return response\n\\`\\`\\`\n\n### Batch Processing Pattern\n\n\\`\\`\\`python\n# keyword_collection/generate_instagram_report.py - Lines 143-152\n\n# Process top 1000 posts in batches of 100\nlimit = 100\ntotal_iterations = (total_posts + limit - 1) // limit\n\nfor iteration in range(total_iterations):\n    start_index = iteration * limit\n    end_index = min(start_index + limit, total_posts)\n\n    # Create async tasks for batch\n    tasks = []\n    for i in range(start_index, end_index):\n        task = asyncio.create_task(instagram_categorization(result.result_rows[i]))\n        tasks.append(task)\n\n    # Wait for all tasks in batch\n    tasks, _ = await asyncio.wait(tasks)\n    results = [task.result() for task in tasks]\n\n    # Process results...\n\\`\\`\\`\n\n### Interview Talking Points\n\n**Q: How did you integrate ML models into the pipeline?**\n\n> \"I integrated two ML models via a centralized RAY service:\n>\n> **Architecture:**\n> - RAY ML server hosts GPU-accelerated models\n> - beat calls via HTTP with JSON payloads\n> - Async requests for non-blocking I/O\n>\n> **Models:**\n> 1. **CATEGORIZER**: Classifies post content (fitness, fashion, food, etc.)\n>    - Input: Caption text\n>    - Output: Label + confidence score\n>\n> 2. **SENTIMENT**: Analyzes comment tone\n>    - Input: Batch of comments\n>    - Output: positive/negative/neutral + score\n>\n> **Optimizations:**\n> - Batch processing (100 posts per batch)\n> - Retry logic with exponential backoff\n> - Rate limiting between batches (3s sleep)\n> - Async/await for parallel requests\"\n\n---\n\n# SUMMARY: ADVANCED FEATURES\n\n| Feature | Math/Algorithm | Business Value |\n|---------|---------------|----------------|\n| **Gradient Descent** | MSE optimization, 50-100 epochs | Realistic demographics |\n| **Reach Formulas** | Logarithmic decay: \\`log2\\`, \\`log10\\` | Estimate reach without API access |\n| **14-Dimension Demographics** | 7 age × 2 gender matrix | Granular audience targeting |\n| **YAKE Keywords** | Statistical term scoring | Content discovery, trends |\n| **RAY ML Integration** | Neural network inference | Auto-categorization, sentiment |\n\n---\n\n# INTERVIEW CHEAT SHEET\n\n**When they ask \"Tell me about something technically challenging\":**\n\n> \"I implemented a gradient descent algorithm to normalize GPT's audience demographic predictions. The problem was GPT outputs weren't realistic - sometimes claiming 95% male audience for a fitness influencer.\n>\n> My solution blended GPT's personalized insights with historical category baselines using gradient descent:\n> - Loss: Sum of Squared Differences\n> - Learning rate: 0.01\n> - Epochs: Random 50-100 (adds variance)\n>\n> This preserved GPT's customization while ensuring realistic distributions.\"\n\n**When they ask \"How did you derive the reach formula?\":**\n\n> \"I analyzed 10,000+ posts with actual reach data and discovered a logarithmic relationship:\n> - Larger accounts have lower reach-to-engagement ratios (algorithm throttling)\n> - \\`log2(followers)\\` captures this decay for reels\n> - \\`log10(likes)\\` captures diminishing returns for static posts\n>\n> The formulas are: \\`plays * (0.94 - log2(followers) * 0.001)\\` for reels and \\`(7.6 - log10(likes) * 0.7) * 0.85 * likes\\` for static posts.\"\n\n---\n\n*These advanced features demonstrate data science, ML engineering, and statistical thinking - valuable for Google's technical interviews.*\n"
  },
  {
    "id": "MASTER_PORTFOLIO_SUMMARY",
    "title": "Portfolio Summary",
    "category": "analysis",
    "badge": null,
    "content": "# WORK EXPERIENCE PORTFOLIO - MASTER SUMMARY\n\n## Overview\n\nThis portfolio contains **6 production-grade projects** from my previous company, demonstrating expertise across backend development, data engineering, distributed systems, and cloud architecture.\n\n| Project | Language | Domain | Key Technology |\n|---------|----------|--------|----------------|\n| **event-grpc** | Go | Event Processing | gRPC, RabbitMQ, ClickHouse |\n| **coffee** | Go | SaaS Platform | REST API, PostgreSQL, Redis |\n| **beat** | Python | Data Scraping | FastAPI, Async, ML |\n| **fake_follower_analysis** | Python | ML Analytics | AWS Lambda, NLP |\n| **stir** | Python | Data Platform | Airflow, dbt, ClickHouse |\n| **saas-gateway** | Go | API Gateway | Gin, JWT, Redis |\n\n---\n\n## Project Summaries\n\n### 1. EVENT-GRPC (Go)\n**High-Throughput Event Ingestion & Distribution System**\n\n\\`\\`\\`\nEvents/sec: 10,000+    |    Worker Pools: 1000+    |    Event Types: 65+\n\\`\\`\\`\n\n- gRPC server for real-time event ingestion from mobile/web apps\n- 25+ RabbitMQ consumer queues with 90+ concurrent workers\n- Multi-destination distribution: ClickHouse, Webengage, Shopify, Branch\n- Buffered sinkers with batch processing for efficiency\n\n**Key Skills**: gRPC, Protocol Buffers, Go Concurrency, Message Queues, ClickHouse\n\n---\n\n### 2. COFFEE (Go)\n**Multi-Tenant SaaS Platform for Influencer Discovery**\n\n\\`\\`\\`\nLOC: 8,500+    |    Tables: 28    |    Endpoints: 40+\n\\`\\`\\`\n\n- 4-layer REST architecture (API → Service → Manager → DAO)\n- Dual database strategy: PostgreSQL (transactional) + ClickHouse (analytics)\n- Multi-tenant with plan-based feature gating (FREE/SAAS/PAID)\n- Watermill + AMQP for async message processing\n\n**Key Skills**: REST API Design, GORM, Multi-Tenancy, Redis Caching, GitLab CI/CD\n\n---\n\n### 3. BEAT (Python)\n**Distributed Social Media Data Aggregation Service**\n\n\\`\\`\\`\nFlows: 75+    |    Workers: 150+    |    Dependencies: 128\n\\`\\`\\`\n\n- FastAPI + uvloop for high-performance async I/O\n- Worker pool pattern with semaphore-based concurrency\n- Multi-platform scraping: Instagram, YouTube, Shopify\n- Redis-backed distributed rate limiting\n- OpenAI GPT integration for data enrichment\n\n**Key Skills**: FastAPI, Async Python, Worker Pools, Rate Limiting, API Integration\n\n---\n\n### 4. FAKE_FOLLOWER_ANALYSIS (Python)\n**ML-Powered Fake Follower Detection System**\n\n\\`\\`\\`\nLOC: 955    |    Languages: 10 Indic Scripts    |    Names DB: 35,183\n\\`\\`\\`\n\n- Ensemble ML model with 5 detection features\n- Multi-language transliteration for 10 Indic scripts\n- AWS Lambda + SQS + Kinesis serverless pipeline\n- RapidFuzz for fuzzy string matching\n\n**Key Skills**: Machine Learning, NLP, AWS Lambda, Kinesis, Docker/ECR\n\n---\n\n### 5. STIR (Python)\n**Enterprise Data Platform for Social Media Analytics**\n\n\\`\\`\\`\nDAGs: 77    |    dbt Models: 100+    |    Git Commits: 1,476\n\\`\\`\\`\n\n- Modern Data Stack: Airflow + dbt + ClickHouse\n- ELT architecture with incremental processing\n- Multi-dimensional leaderboards and rankings\n- Cross-database sync: ClickHouse → S3 → PostgreSQL\n\n**Key Skills**: Apache Airflow, dbt, Data Modeling, ClickHouse, ETL/ELT\n\n---\n\n### 6. SAAS-GATEWAY (Go)\n**API Gateway with Authentication & Service Routing**\n\n\\`\\`\\`\nServices: 12    |    Middleware: 6 layers    |    Cache: 10M keys\n\\`\\`\\`\n\n- Reverse proxy for 12+ microservices\n- JWT authentication with Redis session caching\n- Two-layer caching: Ristretto (in-memory) + Redis Cluster\n- Prometheus metrics + Sentry error tracking\n\n**Key Skills**: API Gateway, JWT Auth, Reverse Proxy, Caching, Observability\n\n---\n\n## Technology Stack Summary\n\n### Languages\n| Language | Projects | Expertise Level |\n|----------|----------|-----------------|\n| **Go** | event-grpc, coffee, saas-gateway | Advanced |\n| **Python** | beat, fake_follower_analysis, stir | Advanced |\n| **SQL** | All projects | Advanced |\n\n### Databases\n| Database | Usage |\n|----------|-------|\n| **PostgreSQL** | Transactional data, relational modeling |\n| **ClickHouse** | Analytics, time-series, OLAP queries |\n| **Redis** | Caching, sessions, rate limiting |\n\n### Message Queues\n| Technology | Usage |\n|------------|-------|\n| **RabbitMQ** | Event distribution, async processing |\n| **AWS SQS** | Serverless message queuing |\n| **AWS Kinesis** | Real-time data streaming |\n\n### Cloud & DevOps\n| Technology | Usage |\n|------------|-------|\n| **AWS Lambda** | Serverless compute |\n| **AWS S3** | Data storage, staging |\n| **AWS ECR** | Container registry |\n| **GitLab CI/CD** | Build, test, deploy pipelines |\n| **Docker** | Containerization |\n\n### Frameworks & Libraries\n| Category | Technologies |\n|----------|--------------|\n| **Web Frameworks** | Gin (Go), FastAPI (Python), Chi (Go) |\n| **ORM** | GORM, SQLAlchemy, Tortoise-ORM |\n| **Data Processing** | pandas, dbt, Dask |\n| **ML/NLP** | TensorFlow, scikit-learn, RapidFuzz |\n| **Orchestration** | Apache Airflow, Prefect |\n\n---\n\n## Core Competencies Demonstrated\n\n### 1. Backend Engineering\n- RESTful API design with consistent patterns\n- gRPC for high-performance RPC\n- Middleware pipeline architecture\n- Connection pooling and resource management\n\n### 2. Distributed Systems\n- Worker pool patterns with concurrency control\n- Message queue integration (RabbitMQ, SQS, Kinesis)\n- Event-driven architecture\n- Horizontal scaling strategies\n\n### 3. Data Engineering\n- ETL/ELT pipeline design\n- Data warehouse modeling (Star Schema)\n- Incremental processing with partitioning\n- Cross-database synchronization\n\n### 4. Cloud Architecture\n- Serverless computing (AWS Lambda)\n- Container orchestration (Docker, ECR)\n- Distributed caching (Redis Cluster)\n- Multi-environment deployment\n\n### 5. Machine Learning\n- Ensemble model design\n- NLP and text processing\n- Feature engineering\n- Model deployment at scale\n\n### 6. DevOps & Observability\n- CI/CD pipeline design (GitLab)\n- Prometheus metrics\n- Sentry error tracking\n- Structured logging\n\n---\n\n## Project Complexity Comparison\n\n| Project | LOC | Files | Complexity | Production Scale |\n|---------|-----|-------|------------|------------------|\n| event-grpc | 5,000+ | 50+ | High | 10K+ events/sec |\n| coffee | 8,500+ | 80+ | High | Multi-tenant SaaS |\n| beat | 2,000+ | 50+ | Medium-High | 150+ workers |\n| fake_follower_analysis | 955 | 10 | Medium | Serverless |\n| stir | 17,500+ | 180+ | Very High | Billions of records |\n| saas-gateway | 2,500+ | 30+ | Medium | 12 services |\n\n---\n\n## Interview Preparation: Key Talking Points\n\n### System Design Questions\n\n1. **\"Design a real-time event processing system\"**\n   - Reference: event-grpc architecture\n   - Worker pools, message queues, buffered sinkers\n   - Multi-destination routing\n\n2. **\"Design a multi-tenant SaaS platform\"**\n   - Reference: coffee architecture\n   - Plan-based feature gating\n   - Dual database strategy\n\n3. **\"Design a data pipeline for analytics\"**\n   - Reference: stir architecture\n   - Airflow + dbt + ClickHouse\n   - Incremental processing\n\n4. **\"Design an API gateway\"**\n   - Reference: saas-gateway architecture\n   - JWT auth, session caching\n   - Reverse proxy pattern\n\n### Behavioral Questions\n\n1. **\"Tell me about a complex system you built\"**\n   - Any of the 6 projects with specific metrics\n   - Architecture decisions and trade-offs\n\n2. **\"How do you handle scale?\"**\n   - Worker pools, caching, message queues\n   - Horizontal scaling strategies\n\n3. **\"Describe a challenging debugging experience\"**\n   - Distributed tracing, logging strategies\n   - Error handling patterns\n\n---\n\n## Files in This Portfolio\n\n\\`\\`\\`\n/work_ex/\n├── MASTER_PORTFOLIO_SUMMARY.md     ← You are here\n├── ANALYSIS_event_grpc.md\n├── ANALYSIS_coffee.md\n├── ANALYSIS_beat.md\n├── ANALYSIS_fake_follower_analysis.md\n├── ANALYSIS_stir.md\n├── ANALYSIS_saas_gateway.md\n├── event-grpc/                      (Source code)\n├── coffee/                          (Source code)\n├── beat/                            (Source code)\n├── fake_follower_analysis/          (Source code)\n├── stir/                            (Source code)\n└── saas-gateway/                    (Source code)\n\\`\\`\\`\n\n---\n\n## Contact & Next Steps\n\nThis portfolio demonstrates production-grade software engineering across:\n- **3 Go projects** (event-grpc, coffee, saas-gateway)\n- **3 Python projects** (beat, fake_follower_analysis, stir)\n- **Multiple domains**: Event processing, SaaS, Data Engineering, ML\n\nEach project includes detailed analysis with:\n- Architecture diagrams\n- Technology stack breakdown\n- Code quality assessment\n- Business impact analysis\n- Interview talking points\n\n---\n\n*Generated with comprehensive code analysis covering ~35,000+ lines of code across 6 production projects.*\n"
  },
  {
    "id": "ANALYSIS_beat",
    "title": "Beat Analysis",
    "category": "analysis",
    "badge": null,
    "content": "# ULTRA-DEEP ANALYSIS: BEAT PROJECT\n\n## PROJECT OVERVIEW\n\n| Attribute | Value |\n|-----------|-------|\n| **Project Name** | Beat |\n| **Purpose** | Multi-platform social media data aggregation and scraping service for enterprise-scale analytics |\n| **Architecture** | Distributed task queue system with async worker pools |\n| **Platforms Supported** | Instagram, YouTube, Shopify |\n| **Language** | Python 3.11 |\n| **Total Lines of Code** | ~15,000+ |\n| **Port** | 8000 (FastAPI) |\n\n---\n\n## 1. COMPLETE DIRECTORY STRUCTURE\n\n\\`\\`\\`\nbeat/\n├── Core Entry Points\n│   ├── main.py (14 KB)                # Worker pool system - 73 flows configured\n│   ├── server.py (43 KB)              # FastAPI REST API server\n│   ├── main_assets.py (7.3 KB)        # Asset upload worker pool\n│   ├── config.py                      # Pydantic configuration\n│   └── requirements.txt               # 128 dependencies\n│\n├── core/                              # Core framework\n│   ├── models/models.py               # Pydantic & SQLAlchemy models\n│   ├── entities/entities.py           # SQLAlchemy ORM entities\n│   ├── amqp/\n│   │   ├── amqp.py                    # aio-pika message listener\n│   │   └── models.py                  # AmqpListener configuration\n│   ├── enums/enums.py                 # Platform & status enums\n│   ├── flows/scraper.py               # Flow dispatcher (75+ flows)\n│   ├── helpers/\n│   │   ├── session.py                 # Async session management\n│   │   └── task.py                    # Task utilities\n│   └── client/upload_assets.py        # S3/CDN upload flows\n│\n├── instagram/                         # Instagram module\n│   ├── entities/entities.py           # InstagramAccount, InstagramPost\n│   ├── models/models.py               # InstagramProfileLog, PostLog\n│   ├── functions/retriever/\n│   │   ├── interface.py               # InstagramCrawlerInterface\n│   │   ├── graphapi/                  # Facebook Graph API\n│   │   │   ├── graphapi.py (20 KB)\n│   │   │   └── graphapi_parser.py\n│   │   ├── lama/                      # Lama API (fallback)\n│   │   │   ├── lama.py\n│   │   │   └── lama_parser.py\n│   │   ├── rapidapi/\n│   │   │   ├── igapi/                 # RapidAPI IGData\n│   │   │   ├── jotucker/              # RapidAPI Instagram Scraper\n│   │   │   ├── neotank/               # RapidAPI NeoTank\n│   │   │   ├── arraybobo/             # RapidAPI Instagram 2022\n│   │   │   ├── bestsolns/             # RapidAPI Best Performance\n│   │   │   └── rocketapi/             # RapidAPI Rocket\n│   │   └── crawler.py\n│   ├── flows/\n│   │   ├── refresh_profile.py (21 KB) # Main flow orchestration\n│   │   ├── profile_extra.py           # Followers, following, comments\n│   │   └── schedule.py\n│   ├── tasks/\n│   │   ├── ingestion.py               # Parse raw responses\n│   │   ├── retrieval.py               # Fetch from APIs\n│   │   ├── processing.py              # Upsert to database\n│   │   └── transformer.py             # Transform to entity models\n│   ├── metric_dim_store.py            # Dimension/metric constants\n│   └── helper.py                      # Engagement calculations\n│\n├── youtube/                           # YouTube module\n│   ├── entities/entities.py           # YoutubeAccount, YoutubePost\n│   ├── models/models.py               # YoutubeProfileLog, PostLog\n│   ├── functions/retriever/\n│   │   ├── interface.py               # YoutubeCrawlerInterface\n│   │   ├── ytapi/                     # Official YouTube Data API v3\n│   │   │   ├── ytapi.py\n│   │   │   └── ytapi_parser.py\n│   │   └── rapidapi/\n│   │       ├── yt_v31/                # RapidAPI YouTube v31\n│   │       ├── rapidapi_youtube/\n│   │       └── rapidapi_youtube_search/\n│   ├── flows/\n│   │   ├── refresh_profile.py (18 KB)\n│   │   ├── profile_extra.py\n│   │   └── csv_jobs.py (12 KB)        # CSV export flows\n│   └── tasks/\n│       ├── ingestion.py\n│       ├── retrieval.py\n│       ├── processing.py\n│       └── csv_report_generation.py\n│\n├── shopify/                           # Shopify module\n│   ├── entities/entities.py\n│   ├── flows/refresh_orders.py\n│   └── tasks/\n│\n├── gpt/                               # OpenAI/GPT module\n│   ├── functions/retriever/\n│   │   ├── interface.py               # GptCrawlerInterface\n│   │   └── openai/\n│   │       ├── openai_extractor.py    # OpenAI API integration\n│   │       └── openai_parser.py\n│   ├── flows/fetch_gpt_data.py\n│   ├── prompts/                       # 13 YAML prompt versions\n│   │   └── profile_info_v*.yaml\n│   └── helper.py\n│\n├── credentials/                       # Credential management\n│   ├── manager.py                     # Credential lifecycle\n│   ├── validator.py                   # Token validation\n│   ├── listener.py (5.2 KB)           # AMQP message handlers\n│   └── identity.py\n│\n├── utils/                             # Utilities\n│   ├── db.py                          # SQLAlchemy helpers\n│   ├── request.py                     # HTTP & rate limiting\n│   ├── exceptions.py                  # Custom exceptions\n│   ├── sentiment_analysis.py\n│   └── extracted_keyword.py           # YAKE keyword extraction\n│\n├── keyword_collection/                # Keyword analysis\n├── collection/                        # Collection management\n├── clients/identity.py                # Identity service client\n│\n├── Configuration\n│   ├── .env, .env.local, .env.prod, .env.stage\n│   ├── schema.sql (13 KB)             # Database DDL\n│   ├── .gitlab-ci.yml                 # GitLab CI/CD\n│   └── scripts/start.sh               # Deployment script\n└── requirements.txt                   # 128 dependencies\n\\`\\`\\`\n\n---\n\n## 2. ARCHITECTURE DIAGRAM\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                           BEAT SERVICE ARCHITECTURE                          │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              ENTRY POINTS                                    │\n├───────────────────┬─────────────────────┬───────────────────────────────────┤\n│   server.py       │     main.py         │    main_assets.py                 │\n│   FastAPI API     │   Worker Pool       │    Asset Upload Workers           │\n│   Port: 8000      │   73 Flows Config   │    S3/CDN Upload                  │\n│   REST Endpoints  │   Multiprocessing   │    Media Caching                  │\n└───────────┬───────┴──────────┬──────────┴───────────────┬───────────────────┘\n            │                  │                          │\n            ▼                  ▼                          ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          WORKER POOL SYSTEM                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  Flow Name                    │  Workers  │  Concurrency  │  Description    │\n│  ─────────────────────────────┼───────────┼───────────────┼─────────────────│\n│  refresh_profile_by_handle    │    10     │      5        │  Instagram      │\n│  refresh_yt_profiles          │    10     │      5        │  YouTube        │\n│  asset_upload_flow            │    15     │      5        │  Media upload   │\n│  refresh_post_insights        │     3     │      5        │  Post metrics   │\n│  fetch_post_comments          │     1     │      5        │  Comments       │\n│  ... 68 more flows            │   varies  │    varies     │  Various        │\n└─────────────────────────────────────────────────────────────────────────────┘\n            │\n            ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         RATE LIMITING LAYER                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  Source                │  Requests  │  Per Period  │  Implementation        │\n│  ──────────────────────┼────────────┼──────────────┼────────────────────────│\n│  youtube138            │    850     │   60 sec     │  asyncio-redis-rate    │\n│  insta-best-performance│      2     │    1 sec     │                        │\n│  arraybobo             │    100     │   30 sec     │                        │\n│  youtubev31            │    500     │   60 sec     │                        │\n│  rocketapi             │    100     │   30 sec     │                        │\n│  Global Daily          │  20,000    │   86400 sec  │  Stacked limiters      │\n│  Global Minute         │     60     │   60 sec     │                        │\n└─────────────────────────────────────────────────────────────────────────────┘\n            │\n            ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        API INTEGRATIONS (15+ APIs)                           │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  INSTAGRAM (6 APIs)           │  YOUTUBE (4 APIs)        │  OTHER           │\n│  ─────────────────────────────┼──────────────────────────┼──────────────────│\n│  • Facebook Graph API v15     │  • YouTube Data API v3   │  • OpenAI GPT    │\n│  • RapidAPI IGData            │  • RapidAPI YouTube v31  │  • Shopify API   │\n│  • RapidAPI JoTucker          │  • RapidAPI YT Search    │  • Identity Svc  │\n│  • RapidAPI NeoTank           │  • YouTube Analytics     │  • S3/CloudFront │\n│  • RapidAPI ArrayBobo         │                          │                  │\n│  • Lama API (fallback)        │                          │                  │\n└─────────────────────────────────────────────────────────────────────────────┘\n            │\n            ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         MESSAGE QUEUE (RabbitMQ/AMQP)                        │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  Listener                 │  Exchange      │  Queue                │  Workers│\n│  ─────────────────────────┼────────────────┼───────────────────────┼─────────│\n│  credentials_validate     │  beat.dx       │  credentials_validate_q│    5   │\n│  identity_token           │  identity.dx   │  identity_token_q      │    5   │\n│  keyword_collection       │  beat.dx       │  keyword_collection_q  │    5   │\n│  sentiment_analysis       │  beat.dx       │  sentiment_analysis_q  │    5   │\n│  sentiment_report         │  beat.dx       │  sentiment_report_q    │    5   │\n└─────────────────────────────────────────────────────────────────────────────┘\n            │\n            ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          DATABASE LAYER                                      │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  PostgreSQL (Async)        │  Redis Cluster            │  S3/CloudFront     │\n│  ──────────────────────────┼───────────────────────────┼────────────────────│\n│  • instagram_account       │  • Rate limit state       │  • Media assets    │\n│  • instagram_post          │  • Cache layer            │  • CDN delivery    │\n│  • youtube_account         │  • Session data           │                    │\n│  • youtube_post            │                           │                    │\n│  • scrape_request_log      │                           │                    │\n│  • credential              │                           │                    │\n│  • profile_log (audit)     │                           │                    │\n│  • post_log (audit)        │                           │                    │\n│  • sentiment_log           │                           │                    │\n│  • asset_log               │                           │                    │\n└─────────────────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n---\n\n## 3. FASTAPI APPLICATION (server.py - 43KB)\n\n### Endpoint Categories\n\n#### Profile Endpoints\n\\`\\`\\`python\nGET  /profiles/{platform}/byhandle/{handle}\n     → Fetch profile by username (Instagram/YouTube)\n     → Rate limited: 1/sec per handle, 60/min global, 20K/day\n\nGET  /profiles/{platform}/byprofileid/{profile_id}\n     → Fetch by platform-specific ID\n\nGET  /profiles/{platform}/byid/{id}\n     → Fetch by internal database ID\n\nGET  /profiles/INSTAGRAM/byhandle/{handle}/insights\n     → Instagram profile insights (business accounts)\n\nGET  /profiles/INSTAGRAM/byhandle/{handle}/audienceinsights\n     → Audience demographics\n\\`\\`\\`\n\n#### Post Endpoints\n\\`\\`\\`python\nGET  /posts/{platform}/byshortcode/{shortcode}\n     → Single post details\n\nGET  /posts/{platform}/{post_type}/{shortcode}\n     → Post with type (image, carousel, reels, story)\n\nGET  /recent/posts/{platform}/byprofileid/{profile_id}\n     → Recent posts with pagination\n\\`\\`\\`\n\n#### Task Management\n\\`\\`\\`python\nPOST /scrape_request_log/flow/{flow}\n     → Create new scrape task (75+ flow types)\n     → Body: {\"params\": {...}, \"priority\": 1}\n\nPOST /scrape_request_log/flow/update/{scrape_id}\n     → Update task status\n\nGET  /scrape_request_log/flow/{id}\n     → Get task result\n\nGET  /list_scrape_data/{account_id}\n     → List tasks by account\n\\`\\`\\`\n\n#### Token Management\n\\`\\`\\`python\nPOST /tokens\n     → Insert/update API credentials\n\nGET  /token/validate\n     → Validate token scopes and expiry\n\\`\\`\\`\n\n#### Health\n\\`\\`\\`python\nGET  /heartbeat\n     → Health check for load balancer\n\nPUT  /heartbeat\n     → Set health status (graceful shutdown)\n\\`\\`\\`\n\n### Rate Limiting Implementation\n\n\\`\\`\\`python\n# Stacked rate limiters for multi-level control\nredis = AsyncRedis.from_url(REDIS_URL)\n\n# Level 1: Daily global limit\nglobal_limit_day = RateSpec(requests=20000, seconds=86400)\n\n# Level 2: Per-minute global limit\nglobal_limit_minute = RateSpec(requests=60, seconds=60)\n\n# Level 3: Per-handle limit\nhandle_limit = RateSpec(requests=1, seconds=1)\n\nasync with RateLimiter(\n    unique_key=f\"refresh_profile_insta_daily\",\n    backend=redis,\n    cache_prefix=\"beat_server_\",\n    rate_spec=global_limit_day\n):\n    async with RateLimiter(\n        unique_key=f\"refresh_profile_insta_minute\",\n        backend=redis,\n        rate_spec=global_limit_minute\n    ):\n        async with RateLimiter(\n            unique_key=f\"refresh_profile_{handle}\",\n            backend=redis,\n            rate_spec=handle_limit\n        ):\n            # Execute API call\n            result = await refresh_profile(handle)\n\\`\\`\\`\n\n---\n\n## 4. WORKER POOL SYSTEM (main.py - 14KB)\n\n### Flow Configuration (73 Flows)\n\n\\`\\`\\`python\n_whitelist = {\n    # Instagram Profile Flows\n    'refresh_profile_custom': {'no_of_workers': 1, 'no_of_concurrency': 2},\n    'refresh_profile_by_handle': {'no_of_workers': 10, 'no_of_concurrency': 5},\n    'refresh_profile_by_profile_id': {'no_of_workers': 5, 'no_of_concurrency': 5},\n    'refresh_profile_basic': {'no_of_workers': 3, 'no_of_concurrency': 5},\n\n    # Instagram Post Flows\n    'refresh_post_by_shortcode': {'no_of_workers': 3, 'no_of_concurrency': 5},\n    'refresh_post_insights': {'no_of_workers': 3, 'no_of_concurrency': 5},\n    'refresh_stories_posts': {'no_of_workers': 1, 'no_of_concurrency': 5},\n    'refresh_story_insights': {'no_of_workers': 1, 'no_of_concurrency': 5},\n\n    # Instagram Extra Flows\n    'fetch_profile_followers': {'no_of_workers': 1, 'no_of_concurrency': 3},\n    'fetch_profile_following': {'no_of_workers': 1, 'no_of_concurrency': 3},\n    'fetch_post_comments': {'no_of_workers': 1, 'no_of_concurrency': 5},\n    'fetch_post_likes': {'no_of_workers': 1, 'no_of_concurrency': 3},\n    'fetch_hashtag_posts': {'no_of_workers': 1, 'no_of_concurrency': 3},\n\n    # YouTube Flows\n    'refresh_yt_profiles': {'no_of_workers': 10, 'no_of_concurrency': 5},\n    'refresh_yt_posts': {'no_of_workers': 5, 'no_of_concurrency': 5},\n    'refresh_yt_posts_by_channel_id': {'no_of_workers': 3, 'no_of_concurrency': 5},\n    'fetch_yt_post_comments': {'no_of_workers': 1, 'no_of_concurrency': 3},\n\n    # YouTube CSV Export Flows\n    'fetch_channels_csv': {'no_of_workers': 2, 'no_of_concurrency': 3},\n    'fetch_channel_videos_csv': {'no_of_workers': 2, 'no_of_concurrency': 3},\n    'fetch_channel_demographic_csv': {'no_of_workers': 2, 'no_of_concurrency': 3},\n\n    # GPT Enrichment Flows\n    'refresh_instagram_gpt_data_base_gender': {'no_of_workers': 2, 'no_of_concurrency': 5},\n    'refresh_instagram_gpt_data_base_location': {'no_of_workers': 2, 'no_of_concurrency': 5},\n    'refresh_instagram_gpt_data_audience_age_gender': {'no_of_workers': 2, 'no_of_concurrency': 5},\n\n    # Asset Upload\n    'asset_upload_flow': {'no_of_workers': 15, 'no_of_concurrency': 5},\n    'asset_upload_flow_stories': {'no_of_workers': 5, 'no_of_concurrency': 5},\n\n    # Shopify\n    'refresh_orders_by_store': {'no_of_workers': 2, 'no_of_concurrency': 3},\n\n    # ... 40+ more flows\n}\n\\`\\`\\`\n\n### Worker Architecture\n\n\\`\\`\\`python\ndef main():\n    \"\"\"Main entry point - spawns worker processes\"\"\"\n    for flow_name, config in _whitelist.items():\n        for i in range(config['no_of_workers']):\n            process = multiprocessing.Process(\n                target=looper,\n                args=(flow_name, config['no_of_concurrency'])\n            )\n            process.start()\n            workers.append(process)\n\n    # Start AMQP listeners\n    start_amqp_listeners()\n\ndef looper(flow_name: str, concurrency: int):\n    \"\"\"Worker process entry point\"\"\"\n    uvloop.install()  # High-performance event loop\n    asyncio.run(poller(flow_name, concurrency))\n\nasync def poller(flow_name: str, concurrency: int):\n    \"\"\"Async polling loop\"\"\"\n    semaphore = asyncio.Semaphore(concurrency)\n\n    while True:\n        task = await poll(flow_name)  # SQL-based task queue\n        if task:\n            asyncio.create_task(\n                perform_task(task, semaphore)\n            )\n        await asyncio.sleep(0.1)\n\nasync def perform_task(task: ScrapeRequestLog, semaphore: Semaphore):\n    \"\"\"Execute task with concurrency control\"\"\"\n    async with semaphore:\n        try:\n            async with asyncio.timeout(600):  # 10-minute timeout\n                result = await execute(task.flow, task.params)\n                await update_task_status(task.id, 'COMPLETE', result)\n        except asyncio.TimeoutError:\n            await update_task_status(task.id, 'TIMEOUT')\n        except Exception as e:\n            await update_task_status(task.id, 'FAILED', str(e))\n\\`\\`\\`\n\n### SQL-Based Task Queue\n\n\\`\\`\\`python\nasync def poll(flow_name: str) -> Optional[ScrapeRequestLog]:\n    \"\"\"Pick task from database with FOR UPDATE SKIP LOCKED\"\"\"\n    query = \"\"\"\n        UPDATE scrape_request_log\n        SET status = 'PROCESSING', picked_at = NOW()\n        WHERE id = (\n            SELECT id FROM scrape_request_log\n            WHERE flow = :flow\n              AND status = 'PENDING'\n              AND (expires_at IS NULL OR expires_at > NOW())\n            ORDER BY priority DESC, created_at ASC\n            FOR UPDATE SKIP LOCKED\n            LIMIT 1\n        )\n        RETURNING *\n    \"\"\"\n    return await session.execute(query, {'flow': flow_name})\n\\`\\`\\`\n\n---\n\n## 5. DATA COLLECTION FLOWS (75+ Flows)\n\n### Instagram Flows (25+)\n\n| Flow | Purpose | Workers | Concurrency |\n|------|---------|---------|-------------|\n| refresh_profile_by_handle | Profile lookup by username | 10 | 5 |\n| refresh_profile_by_profile_id | Profile lookup by ID | 5 | 5 |\n| refresh_profile_basic | Lightweight profile | 3 | 5 |\n| refresh_profile_insights | Business insights | 3 | 5 |\n| refresh_post_by_shortcode | Single post details | 3 | 5 |\n| refresh_post_insights | Post metrics | 3 | 5 |\n| refresh_stories_posts | Story content | 1 | 5 |\n| refresh_story_insights | Story metrics | 1 | 5 |\n| fetch_profile_followers | Follower list (paginated) | 1 | 3 |\n| fetch_profile_following | Following list | 1 | 3 |\n| fetch_post_comments | Post comments | 1 | 5 |\n| fetch_post_likes | Post likers | 1 | 3 |\n| fetch_hashtag_posts | Posts by hashtag | 1 | 3 |\n| fetch_tagged_posts | Tagged posts | 1 | 3 |\n| asset_upload_flow | Media CDN upload | 15 | 5 |\n\n### YouTube Flows (20+)\n\n| Flow | Purpose | Workers | Concurrency |\n|------|---------|---------|-------------|\n| refresh_yt_profiles | Channel info | 10 | 5 |\n| refresh_yt_posts | Video list | 5 | 5 |\n| refresh_yt_posts_by_channel_id | Videos by channel | 3 | 5 |\n| refresh_yt_posts_by_playlist_id | Playlist videos | 2 | 3 |\n| refresh_yt_profile_insights | Channel analytics | 2 | 3 |\n| fetch_yt_post_comments | Video comments | 1 | 3 |\n| fetch_channels_csv | Channel export | 2 | 3 |\n| fetch_channel_videos_csv | Video export | 2 | 3 |\n| fetch_channel_demographic_csv | Demographics export | 2 | 3 |\n| fetch_channel_daily_stats_csv | Daily stats | 2 | 3 |\n| fetch_channel_engagement_csv | Engagement metrics | 2 | 3 |\n\n### GPT Enrichment Flows (6)\n\n| Flow | Purpose | Workers | Concurrency |\n|------|---------|---------|-------------|\n| refresh_instagram_gpt_data_base_gender | Infer gender | 2 | 5 |\n| refresh_instagram_gpt_data_base_location | Infer location | 2 | 5 |\n| refresh_instagram_gpt_data_base_categ_lang_topics | Content analysis | 2 | 5 |\n| refresh_instagram_gpt_data_audience_age_gender | Audience demographics | 2 | 5 |\n| refresh_instagram_gpt_data_audience_cities | Geographic distribution | 2 | 5 |\n| refresh_instagram_gpt_data_gender_location_lang | Combined analysis | 2 | 5 |\n\n---\n\n## 6. INSTAGRAM SCRAPING IMPLEMENTATIONS\n\n### API Integration Architecture\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────┐\n│                 INSTAGRAM CRAWLER INTERFACE                      │\n│                 (instagram/functions/retriever/interface.py)     │\n├─────────────────────────────────────────────────────────────────┤\n│  Abstract Methods:                                               │\n│  - fetch_profile_by_handle(handle) → dict                       │\n│  - fetch_profile_posts_by_handle(handle, limit) → list          │\n│  - fetch_post_by_shortcode(shortcode) → dict                    │\n│  - fetch_post_insights(post_id) → dict                          │\n│  - fetch_profile_insights(user_id) → dict                       │\n│  - fetch_stories_posts(user_id) → list                          │\n│  - fetch_story_insights(story_id) → dict                        │\n│  - fetch_followers(user_id, cursor) → (list, cursor)            │\n│  - fetch_following(user_id, cursor) → (list, cursor)            │\n│  - fetch_comments(post_id, cursor) → (list, cursor)             │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n           ┌──────────────────┼──────────────────┐\n           ▼                  ▼                  ▼\n┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n│   GraphAPI      │  │   RapidAPI      │  │   Lama API      │\n│   (Primary)     │  │   (Secondary)   │  │   (Fallback)    │\n├─────────────────┤  ├─────────────────┤  ├─────────────────┤\n│ • Official API  │  │ • IGData        │  │ • Post lookup   │\n│ • Business accts│  │ • JoTucker      │  │ • No auth req   │\n│ • Insights      │  │ • NeoTank       │  │ • Rate limited  │\n│ • Stories       │  │ • ArrayBobo     │  │                 │\n│                 │  │ • BestSolns     │  │                 │\n│                 │  │ • RocketAPI     │  │                 │\n└─────────────────┘  └─────────────────┘  └─────────────────┘\n\\`\\`\\`\n\n### GraphAPI Implementation (graphapi.py - 20KB)\n\n\\`\\`\\`python\nclass GraphApi(InstagramCrawlerInterface):\n    \"\"\"Facebook Graph API implementation for Instagram Business accounts\"\"\"\n\n    BASE_URL = \"https://graph.facebook.com/v15.0\"\n\n    async def fetch_profile_by_handle(self, handle: str) -> dict:\n        \"\"\"\n        Endpoint: /{user_id}/business_discovery.username(handle)\n        Fields: biography, followers_count, follows_count, media_count,\n                profile_picture_url, name, username, is_verified, etc.\n        \"\"\"\n        fields = \"biography,followers_count,follows_count,media_count,...\"\n        url = f\"{self.BASE_URL}/{self.user_id}?fields=business_discovery.username({handle}){{{fields}}}\"\n        return await self._request(url)\n\n    async def fetch_post_insights(self, post_id: str) -> dict:\n        \"\"\"\n        Endpoint: /{post_id}/insights\n        Metrics: impressions, reach, engagement, saved, video_views\n        \"\"\"\n        metrics = \"impressions,reach,engagement,saved,video_views\"\n        url = f\"{self.BASE_URL}/{post_id}/insights?metric={metrics}\"\n        return await self._request(url)\n\n    async def validate_token(self) -> bool:\n        \"\"\"\n        Validate token scopes and expiry via debug_token endpoint\n        Required scopes: instagram_basic, instagram_manage_insights,\n                         pages_read_engagement, pages_show_list\n        \"\"\"\n        url = f\"{self.BASE_URL}/debug_token?input_token={self.token}\"\n        response = await self._request(url)\n        scopes = response['data']['scopes']\n        return all(s in scopes for s in REQUIRED_SCOPES)\n\\`\\`\\`\n\n### Data Flow Pipeline\n\n\\`\\`\\`\n┌──────────────────────────────────────────────────────────────────────┐\n│                         DATA FLOW PIPELINE                            │\n└──────────────────────────────────────────────────────────────────────┘\n\nStage 1: RETRIEVAL (instagram/tasks/retrieval.py)\n┌──────────────────────────────────────────────────────────────────────┐\n│  retrieve_profile_data_by_handle(handle, source)                     │\n│    ↓                                                                 │\n│  Select crawler based on source → Execute API call → Raw dict       │\n└──────────────────────────────────────────────────────────────────────┘\n                              ↓\nStage 2: PARSING (instagram/tasks/ingestion.py)\n┌──────────────────────────────────────────────────────────────────────┐\n│  parse_profile_data(raw_data, source)                                │\n│    ↓                                                                 │\n│  Extract fields → Create InstagramProfileLog                         │\n│    - dimensions: [Dimension(key, value), ...]                        │\n│    - metrics: [Metric(key, value), ...]                              │\n└──────────────────────────────────────────────────────────────────────┘\n                              ↓\nStage 3: PROCESSING (instagram/tasks/processing.py)\n┌──────────────────────────────────────────────────────────────────────┐\n│  upsert_profile(profile_log: InstagramProfileLog)                    │\n│    ↓                                                                 │\n│  Transform to InstagramAccount (ORM) → Upsert to PostgreSQL          │\n│    ↓                                                                 │\n│  Create ProfileLog entry (audit trail)                               │\n│    ↓                                                                 │\n│  Publish event to AMQP                                               │\n└──────────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n---\n\n## 7. DATABASE MODELS\n\n### Core ORM Entities (SQLAlchemy)\n\n\\`\\`\\`python\n# core/entities/entities.py\n\nclass Credential(Base):\n    \"\"\"API credential storage\"\"\"\n    __tablename__ = 'credential'\n\n    id = Column(BigInteger, primary_key=True)\n    idempotency_key = Column(String, unique=True)\n    source = Column(String)  # graphapi, ytapi, rapidapi-igapi, etc.\n    credentials = Column(JSONB)  # {token, user_id, key, refresh_token, ...}\n    handle = Column(String)\n    enabled = Column(Boolean, default=True)\n    data_access_expired = Column(Boolean, default=False)\n    disabled_till = Column(DateTime)  # TTL for rate limit backoff\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, onupdate=func.now())\n\n\nclass ScrapeRequestLog(Base):\n    \"\"\"Task queue table\"\"\"\n    __tablename__ = 'scrape_request_log'\n\n    id = Column(BigInteger, primary_key=True)\n    idempotency_key = Column(String, unique=True)\n    platform = Column(String)  # INSTAGRAM, YOUTUBE, SHOPIFY\n    flow = Column(String)  # 75+ flow names\n    status = Column(String)  # PENDING, PROCESSING, COMPLETE, FAILED\n    params = Column(JSONB)  # Flow-specific parameters\n    data = Column(Text)  # Result or error message\n    priority = Column(Integer, default=1)\n    retry_count = Column(Integer, default=0)\n    account_id = Column(String)\n    created_at = Column(DateTime, default=func.now())\n    picked_at = Column(DateTime)\n    scraped_at = Column(DateTime)\n    expires_at = Column(DateTime)\n\n\nclass ProfileLog(Base):\n    \"\"\"Audit log for profile snapshots\"\"\"\n    __tablename__ = 'profile_log'\n\n    id = Column(BigInteger, primary_key=True)\n    platform = Column(String)\n    profile_id = Column(String)\n    dimensions = Column(JSONB)  # [{key, value}, ...]\n    metrics = Column(JSONB)  # [{key, value}, ...]\n    source = Column(String)\n    timestamp = Column(DateTime, default=func.now())\n\\`\\`\\`\n\n### Instagram ORM Models\n\n\\`\\`\\`python\n# instagram/entities/entities.py\n\nclass InstagramAccount(Base):\n    \"\"\"Instagram profile data\"\"\"\n    __tablename__ = 'instagram_account'\n\n    id = Column(BigInteger, primary_key=True)\n    profile_id = Column(String, unique=True)  # Instagram user ID\n    handle = Column(String, index=True)\n    full_name = Column(String)\n    biography = Column(Text)\n\n    # Metrics\n    followers = Column(BigInteger)\n    following = Column(BigInteger)\n    media_count = Column(BigInteger)\n\n    # Calculated metrics\n    avg_likes = Column(Float)\n    avg_comments = Column(Float)\n    avg_reach = Column(Float)\n    avg_engagement = Column(Float)\n    avg_reels_plays = Column(Float)\n\n    # Profile attributes\n    profile_pic_url = Column(String)\n    profile_type = Column(String)  # personal, business, creator\n    is_private = Column(Boolean)\n    is_verified = Column(Boolean)\n    is_business_or_creator = Column(Boolean)\n    category = Column(String)\n    fbid = Column(String)  # Facebook page ID\n\n    # Timestamps\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, onupdate=func.now())\n    refreshed_at = Column(DateTime)\n\n\nclass InstagramPost(Base):\n    \"\"\"Instagram post data\"\"\"\n    __tablename__ = 'instagram_post'\n\n    id = Column(BigInteger, primary_key=True)\n    post_id = Column(String, unique=True)  # Instagram media ID\n    shortcode = Column(String, unique=True, index=True)\n    profile_id = Column(String, ForeignKey('instagram_account.profile_id'))\n    handle = Column(String)\n\n    # Content\n    post_type = Column(String)  # image, carousel, reels, story\n    caption = Column(Text)\n    thumbnail_url = Column(String)\n    display_url = Column(String)\n\n    # Metrics\n    likes = Column(BigInteger)\n    comments = Column(BigInteger)\n    plays = Column(BigInteger)  # For reels/videos\n    reach = Column(BigInteger)\n    views = Column(BigInteger)\n    shares = Column(BigInteger)\n    impressions = Column(BigInteger)\n    saved = Column(BigInteger)\n\n    # Timestamps\n    publish_time = Column(DateTime)\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, onupdate=func.now())\n\\`\\`\\`\n\n---\n\n## 8. GPT/OPENAI INTEGRATION\n\n### Architecture\n\n\\`\\`\\`python\n# gpt/functions/retriever/openai/openai_extractor.py\n\nclass OpenAi(GptCrawlerInterface):\n    \"\"\"OpenAI API integration for data enrichment\"\"\"\n\n    def __init__(self):\n        openai.api_type = \"azure\"\n        openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n    async def fetch_instagram_gpt_data_base_gender(\n        self, handle: str, bio: str\n    ) -> dict:\n        \"\"\"Infer audience gender from bio and handle\"\"\"\n        prompt = self._load_prompt(\"profile_info_v0.12.yaml\")\n        response = await openai.ChatCompletion.acreate(\n            engine=\"gpt-35-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": prompt['system']},\n                {\"role\": \"user\", \"content\": prompt['user'].format(\n                    handle=handle, bio=bio\n                )}\n            ],\n            temperature=0  # Deterministic output\n        )\n        return self._parse_response(response)\n\n    async def fetch_instagram_gpt_data_audience_age_gender(\n        self, handle: str, bio: str, recent_posts: list\n    ) -> dict:\n        \"\"\"Infer audience demographics from content\"\"\"\n        # Analyze bio + recent post captions\n        content = f\"{bio}\\\\n\\\\n\" + \"\\\\n\".join([p['caption'] for p in recent_posts])\n        # ... similar implementation\n\\`\\`\\`\n\n### Prompt Management\n\n\\`\\`\\`yaml\n# gpt/prompts/profile_info_v0.12.yaml\n\nsystem: |\n  You are an AI assistant that analyzes Instagram profiles.\n  Given a username and bio, infer the following:\n  - Primary audience gender (male/female/mixed)\n  - Confidence score (0-1)\n\nuser: |\n  Username: {handle}\n  Bio: {bio}\n\n  Analyze and respond in JSON format:\n  {\n    \"gender\": \"male|female|mixed\",\n    \"confidence\": 0.85,\n    \"reasoning\": \"...\"\n  }\n\nmodel: gpt-35-turbo\ntemperature: 0\n\\`\\`\\`\n\n### Use Cases\n\n| Flow | Input | Output |\n|------|-------|--------|\n| base_gender | handle, bio | {gender, confidence} |\n| base_location | handle, bio | {country, city, confidence} |\n| categ_lang_topics | handle, bio, posts | {category, language, topics[]} |\n| audience_age_gender | handle, bio, posts | {age_range, gender_dist} |\n| audience_cities | handle, bio, posts | {cities: [{name, percentage}]} |\n\n---\n\n## 9. MESSAGE QUEUE INTEGRATION (RabbitMQ/AMQP)\n\n### aio-pika Implementation\n\n\\`\\`\\`python\n# core/amqp/amqp.py\n\n@dataclass\nclass AmqpListener:\n    \"\"\"AMQP listener configuration\"\"\"\n    exchange: str\n    routing_key: str\n    queue: str\n    workers: int\n    prefetch: int\n    fn: Callable  # Handler function\n\n\nasync def async_listener(config: AmqpListener):\n    \"\"\"Start AMQP listener with connection recovery\"\"\"\n    connection = await aio_pika.connect_robust(\n        os.environ[\"RMQ_URL\"],\n        heartbeat=60\n    )\n    channel = await connection.channel()\n    await channel.set_qos(prefetch_count=config.prefetch)\n\n    exchange = await channel.declare_exchange(\n        config.exchange, ExchangeType.DIRECT, durable=True\n    )\n    queue = await channel.declare_queue(config.queue, durable=True)\n    await queue.bind(exchange, routing_key=config.routing_key)\n\n    async with queue.iterator() as queue_iter:\n        async for message in queue_iter:\n            async with message.process():\n                try:\n                    await config.fn(message.body)\n                except Exception as e:\n                    logger.error(f\"Message processing failed: {e}\")\n                    # Message will be requeued\n                    raise\n\\`\\`\\`\n\n### Configured Listeners\n\n\\`\\`\\`python\n# main.py\n\namqp_listeners = [\n    AmqpListener(\n        exchange=\"beat.dx\",\n        routing_key=\"credentials_validate_rk\",\n        queue=\"credentials_validate_q\",\n        workers=5,\n        prefetch=10,\n        fn=credential_validate\n    ),\n    AmqpListener(\n        exchange=\"identity.dx\",\n        routing_key=\"new_access_token_rk\",\n        queue=\"identity_token_q\",\n        workers=5,\n        prefetch=10,\n        fn=upsert_credential_from_identity\n    ),\n    AmqpListener(\n        exchange=\"beat.dx\",\n        routing_key=\"keyword_collection_rk\",\n        queue=\"keyword_collection_q\",\n        workers=5,\n        prefetch=1,\n        fn=fetch_keyword_collection\n    ),\n    AmqpListener(\n        exchange=\"beat.dx\",\n        routing_key=\"post_activity_log_bulk_rk\",\n        queue=\"sentiment_analysis_q\",\n        workers=5,\n        prefetch=1,\n        fn=sentiment_extraction\n    ),\n    AmqpListener(\n        exchange=\"beat.dx\",\n        routing_key=\"sentiment_collection_report_in_rk\",\n        queue=\"sentiment_report_q\",\n        workers=5,\n        prefetch=1,\n        fn=fetch_sentiment_report\n    ),\n]\n\\`\\`\\`\n\n---\n\n## 10. CREDENTIAL MANAGEMENT\n\n### Credential Manager\n\n\\`\\`\\`python\n# credentials/manager.py\n\nclass CredentialManager:\n    \"\"\"Manage API credentials lifecycle\"\"\"\n\n    async def insert_creds(\n        self, source: str, credentials: dict, handle: str = None\n    ) -> Credential:\n        \"\"\"Upsert credential by idempotency key\"\"\"\n        key = f\"{source}:{credentials.get('user_id', credentials.get('key'))}\"\n        return await get_or_create(\n            session,\n            Credential,\n            idempotency_key=key,\n            defaults={\n                'source': source,\n                'credentials': credentials,\n                'handle': handle,\n                'enabled': True\n            }\n        )\n\n    async def disable_creds(\n        self, cred_id: int, disable_duration: int = 3600\n    ) -> None:\n        \"\"\"Disable credential with TTL (rate limit backoff)\"\"\"\n        await session.execute(\n            update(Credential)\n            .where(Credential.id == cred_id)\n            .values(\n                enabled=False,\n                disabled_till=func.now() + timedelta(seconds=disable_duration)\n            )\n        )\n\n    async def get_enabled_cred(self, source: str) -> Optional[Credential]:\n        \"\"\"Get random enabled credential for source\"\"\"\n        creds = await session.execute(\n            select(Credential)\n            .where(Credential.source == source)\n            .where(Credential.enabled == True)\n            .where(\n                or_(\n                    Credential.disabled_till.is_(None),\n                    Credential.disabled_till < func.now()\n                )\n            )\n        )\n        enabled_creds = creds.scalars().all()\n        return random.choice(enabled_creds) if enabled_creds else None\n\\`\\`\\`\n\n### Credential Validator\n\n\\`\\`\\`python\n# credentials/validator.py\n\nREQUIRED_SCOPES = [\n    'instagram_basic',\n    'instagram_manage_insights',\n    'pages_read_engagement',\n    'pages_show_list'\n]\n\nclass CredentialValidator:\n    \"\"\"Validate API credentials\"\"\"\n\n    async def validate(self, credential: Credential) -> ValidationResult:\n        \"\"\"Validate token and check required scopes\"\"\"\n        if credential.source == 'graphapi':\n            return await self._validate_graphapi(credential)\n        elif credential.source == 'ytapi':\n            return await self._validate_youtube(credential)\n        # ... other sources\n\n    async def _validate_graphapi(self, cred: Credential) -> ValidationResult:\n        token = cred.credentials.get('token')\n        response = await self._debug_token(token)\n\n        if not response['data']['is_valid']:\n            raise TokenValidationFailed(\"Token is invalid\")\n\n        scopes = response['data']['scopes']\n        missing = [s for s in REQUIRED_SCOPES if s not in scopes]\n        if missing:\n            raise TokenValidationFailed(f\"Missing scopes: {missing}\")\n\n        if response['data'].get('data_access_expires_at', 0) < time.time():\n            raise DataAccessExpired(\"Data access has expired\")\n\n        return ValidationResult(valid=True, scopes=scopes)\n\\`\\`\\`\n\n---\n\n## 11. ENGAGEMENT CALCULATIONS\n\n### Formula-Based Analytics\n\n\\`\\`\\`python\n# instagram/helper.py\n\ndef calculate_engagement_rate(\n    likes: int, comments: int, followers: int\n) -> float:\n    \"\"\"Standard engagement rate formula\"\"\"\n    if followers == 0:\n        return 0.0\n    return ((likes + comments) / followers) * 100\n\n\ndef estimate_reach_reels(plays: int, followers: int) -> float:\n    \"\"\"Estimate reach for Reels based on plays and followers\"\"\"\n    # Empirical formula from platform data analysis\n    factor = 0.94 - (math.log2(followers) * 0.001)\n    return plays * factor\n\n\ndef estimate_reach_posts(likes: int) -> float:\n    \"\"\"Estimate reach for static posts based on likes\"\"\"\n    if likes == 0:\n        return 0.0\n    factor = (7.6 - (math.log10(likes) * 0.7)) * 0.85\n    return factor * likes\n\n\ndef estimate_story_reach(\n    followers: int, avg_engagement: float\n) -> float:\n    \"\"\"Estimate story reach based on followers and engagement\"\"\"\n    base = -0.000025017 * followers\n    engagement_factor = 1.11 * (\n        followers * abs(math.log2(avg_engagement + 2)) * 2 / 100\n    )\n    return base + engagement_factor\n\n\ndef calculate_avg_metrics(posts: list, exclude_outliers: bool = True) -> dict:\n    \"\"\"Calculate average metrics with optional outlier removal\"\"\"\n    if exclude_outliers and len(posts) > 4:\n        # Remove top 2 and bottom 2 by engagement\n        posts = sorted(posts, key=lambda p: p['likes'] + p['comments'])\n        posts = posts[2:-2]\n\n    return {\n        'avg_likes': mean([p['likes'] for p in posts]),\n        'avg_comments': mean([p['comments'] for p in posts]),\n        'avg_reach': mean([p.get('reach', 0) for p in posts]),\n        'avg_engagement': mean([\n            calculate_engagement_rate(p['likes'], p['comments'], p['followers'])\n            for p in posts\n        ])\n    }\n\\`\\`\\`\n\n---\n\n## 12. CI/CD PIPELINE\n\n### GitLab CI Configuration\n\n\\`\\`\\`yaml\n# .gitlab-ci.yml\n\nstages:\n  - deploy_stage\n  - deploy_prod\n\ndeploy_stage:\n  stage: deploy_stage\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - stage-aafat\n  only:\n    - master\n    - dev\n\ndeploy_prod_1:\n  stage: deploy_prod\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - beat-deployer-1\n  when: manual\n  only:\n    - master\n\ndeploy_prod_2:\n  stage: deploy_prod\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - beat-deployer-2\n  when: manual\n  only:\n    - master\n\\`\\`\\`\n\n### Deployment Script\n\n\\`\\`\\`bash\n#!/bin/bash\n# scripts/start.sh\n\n# Setup Python environment\npython3.11 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Copy environment config\ncp .env.\\${ENV} .env\n\n# Graceful shutdown\ncurl -XPUT http://localhost:8000/heartbeat?beat=false\nsleep 10\n\n# Kill existing processes\npkill -f \"python main.py\" || true\npkill -f \"python server.py\" || true\nsleep 5\n\n# Start workers\nnohup python main.py > logs/main.log 2>&1 &\n\n# Start API server\nnohup python server.py > logs/server.log 2>&1 &\n\n# Wait for startup\nsleep 10\n\n# Enable health check\ncurl -XPUT http://localhost:8000/heartbeat?beat=true\n\\`\\`\\`\n\n---\n\n## 13. KEY METRICS & STATISTICS\n\n| Metric | Value |\n|--------|-------|\n| **Total Lines of Code** | ~15,000+ |\n| **Python Modules** | 130+ |\n| **Data Collection Flows** | 75+ |\n| **API Integrations** | 15+ |\n| **Dependencies** | 128 packages |\n| **Worker Processes** | 20-180+ (configurable) |\n| **Concurrency per Worker** | 2-15 (configurable) |\n| **AMQP Listeners** | 5 |\n| **Rate Limit Rules** | 7+ source-specific |\n| **Database Tables** | 30+ |\n\n---\n\n## 14. SKILLS DEMONSTRATED\n\n### Technical Skills\n\n| Skill | Evidence |\n|-------|----------|\n| **Async Python** | FastAPI + uvloop + aio-pika + asyncpg |\n| **Distributed Systems** | Multiprocessing workers + message queues |\n| **API Integration** | 15+ external APIs with fallback strategies |\n| **Rate Limiting** | Redis-backed multi-level rate limiting |\n| **Database Design** | PostgreSQL with JSONB, async sessions |\n| **ML Integration** | OpenAI GPT for data enrichment |\n| **Task Queues** | SQL-based with FOR UPDATE SKIP LOCKED |\n| **Data Pipelines** | 3-stage ETL (Retrieval → Parsing → Processing) |\n\n### Architecture Patterns\n\n1. **Worker Pool Pattern** - Multiprocessing with async I/O\n2. **Semaphore Pattern** - Concurrency control per worker\n3. **Decorator Pattern** - @sessionize for session injection\n4. **Strategy Pattern** - Multiple API implementations per interface\n5. **Circuit Breaker** - Credential disable with TTL backoff\n6. **Fallback Pattern** - Secondary APIs when primary fails\n\n---\n\n## 15. INTERVIEW TALKING POINTS\n\n### System Design Questions\n\n**\"Design a distributed data scraping system\"**\n- Worker pool with 73 configurable flows\n- SQL-based task queue with FOR UPDATE SKIP LOCKED\n- Multi-level rate limiting (global daily, per-minute, per-resource)\n- 15+ API integrations with fallback strategies\n- Credential rotation for load balancing\n\n**\"How do you handle rate limits from external APIs?\"**\n- Redis-backed asyncio-redis-rate-limit\n- Source-specific rate specs (2-850 requests/period)\n- Stacked limiters for multi-level control\n- Credential disable with TTL backoff\n- Multiple API sources for redundancy\n\n**\"Explain your async Python architecture\"**\n- FastAPI + uvloop for high-performance event loop\n- aio-pika for async RabbitMQ consumption\n- asyncpg for async PostgreSQL\n- Semaphore-based concurrency control\n- 10-minute task timeout with auto-cancellation\n\n### Behavioral Questions\n\n**\"Tell me about a complex data pipeline you built\"**\n- Beat: 15K+ LOC, 75+ flows, 15+ API integrations\n- 3-stage pipeline: Retrieval → Parsing → Processing\n- GPT integration for data enrichment\n- Real-time event publishing to AMQP\n\n**\"How do you handle API failures?\"**\n- Fallback APIs (e.g., Lama for Instagram when GraphAPI fails)\n- Retry with exponential backoff\n- Credential rotation on rate limit\n- Circuit breaker with TTL disable\n\n---\n\n*Generated through comprehensive source code analysis of the beat project.*\n"
  },
  {
    "id": "ANALYSIS_stir",
    "title": "Stir Analysis",
    "category": "analysis",
    "badge": null,
    "content": "# ULTRA-DEEP ANALYSIS: STIR DATA PLATFORM\n\n## PROJECT OVERVIEW\n\n| Attribute | Value |\n|-----------|-------|\n| **Project Name** | Stir |\n| **Purpose** | Enterprise Data Platform for Social Media Analytics - Influencer Discovery, Leaderboards, Collections |\n| **Architecture** | Modern Data Stack (ELT) with Apache Airflow + dbt + ClickHouse |\n| **Git Commits** | 1,476 (mature production project) |\n| **Total Lines of Code** | ~17,500+ |\n| **Total DAGs** | 76 |\n| **Total dbt Models** | 112 (29 staging + 83 marts) |\n\n---\n\n## 1. COMPLETE DIRECTORY STRUCTURE\n\n\\`\\`\\`\n/stir/\n├── .git/                              # Git repository (1,476 commits)\n├── .gitignore                         # Git ignore rules\n├── .gitlab-ci.yml                     # CI/CD pipeline configuration\n│\n├── dags/                              # Airflow DAGs (76 files)\n│   ├── __pycache__/                   # Python cache\n│   │\n│   │ # dbt Orchestration DAGs (11)\n│   ├── dbt_core.py                    # Core models (*/15 min)\n│   ├── dbt_hourly.py                  # Hourly transforms (every 2h)\n│   ├── dbt_daily.py                   # Daily batch (19:00 UTC)\n│   ├── dbt_weekly.py                  # Weekly aggregates\n│   ├── dbt_collections.py             # Collection processing (*/30 min)\n│   ├── dbt_staging_collections.py     # Staging collections (*/30 min)\n│   ├── dbt_recent_scl.py              # Recent scrape logging (*/5 min)\n│   ├── dbt_gcc_orders.py              # Order processing (daily)\n│   ├── dbt_refresh_account_tracker_stats.py  # Account stats (hourly)\n│   ├── post_ranker.py                 # Post ranking (*/5 hours)\n│   ├── post_ranker_partial.py         # Partial ranking (*/15 min)\n│   │\n│   │ # Instagram Sync DAGs (17)\n│   ├── sync_insta_collection_posts.py\n│   ├── sync_insta_collection_stories.py\n│   ├── sync_insta_post_comments.py\n│   ├── sync_insta_post_insights.py\n│   ├── sync_insta_profile_followers.py\n│   ├── sync_insta_profile_following.py\n│   ├── sync_insta_profile_insights.py\n│   ├── sync_insta_profiles_by_handle.py\n│   ├── sync_insta_stories.py\n│   ├── sync_insta_stories_explicitly.py\n│   ├── sync_insta_story_insights.py\n│   ├── sync_instagram_gpt_data_audience_age_gender copy.py\n│   ├── sync_instagram_gpt_data_audience_cities.py\n│   ├── sync_instagram_gpt_data_base_categ_lang_topics.py\n│   ├── sync_instagram_gpt_data_base_gender.py\n│   ├── sync_instagram_gpt_data_base_location.py\n│   ├── sync_instagram_gpt_data_gender_location_lang.py\n│   │\n│   │ # YouTube Sync DAGs (12)\n│   ├── sync_yt_channels.py\n│   ├── sync_yt_collection_posts.py\n│   ├── sync_yt_critical_daily.py      # 127 hardcoded channels\n│   ├── sync_yt_genre_videos.py\n│   ├── sync_yt_post_comments.py\n│   ├── sync_yt_post_type.py\n│   ├── sync_yt_profile_insights.py\n│   ├── sync_yt_profile_relationship_by_channel_id.py\n│   ├── sync_yt_profiles_by_handle.py\n│   ├── sync_yt_profiles_videos_by_handle.py\n│   ├── sync_vidooly_es_youtube_channels.py\n│   ├── retry_yt_scrape_events.py\n│   │\n│   │ # Collection & Leaderboard Sync DAGs (15)\n│   ├── sync_collection_post_summary_prod.py\n│   ├── sync_collection_post_summary_staging.py\n│   ├── sync_collection_post_ts_prod.py\n│   ├── sync_collection_post_ts_staging.py\n│   ├── sync_collection_hashtags_prod.py\n│   ├── sync_collection_keywords_prod.py\n│   ├── sync_leaderboard_prod.py\n│   ├── sync_leaderboard_staging.py\n│   ├── sync_time_series_prod.py\n│   ├── sync_time_series_staging.py\n│   ├── sync_trending_content_prod.py\n│   ├── sync_trending_content_staging.py\n│   ├── sync_genre_overview_prod.py\n│   ├── sync_genre_overview_staging.py\n│   ├── sync_hashtags_prod.py\n│   │\n│   │ # Operational & Verification DAGs (9)\n│   ├── sync_g3_collection_posts.py\n│   ├── sync_group_metrics_prod.py\n│   ├── sync_final_post_submitted_for_cp.py\n│   ├── sync_missing_journey.py\n│   ├── sync_post_with_saas_again.py\n│   ├── sync_post_collection_sentiment_report_path.py\n│   ├── sync_keyword_collection_report.py\n│   ├── sync_shopify_orders.py\n│   ├── mark_campaign_completed_on_refer_launch.py\n│   │\n│   │ # Asset Upload DAGs (7)\n│   ├── upload_post_asset.py\n│   ├── upload_post_asset_stories.py\n│   ├── upload_insta_profile_asset.py\n│   ├── upload_profile_relationship_asset.py\n│   ├── upload_content_verification.py\n│   ├── upload_handle_verification.py\n│   ├── uca_su_saas_item_sync.py\n│   │\n│   │ # Utility DAGs (4)\n│   ├── crm_recommendation_invitation.py\n│   ├── create_payout_for_active_su_if_not_present.py\n│   ├── track_hashtags.py\n│   └── slack_connection.py            # Slack notification helper\n│\n├── src/gcc_social/                    # dbt Project Directory\n│   ├── dbt_project.yml                # dbt configuration\n│   ├── dbt_packages/                  # External packages\n│   ├── logs/                          # Execution logs\n│   ├── target/                        # Compiled artifacts\n│   │\n│   └── models/                        # dbt Models (112 total)\n│       │\n│       ├── staging/                   # Staging Layer (29 models)\n│       │   │\n│       │   ├── beat/                  # Beat Source (13 models)\n│       │   │   ├── stg_beat_asset_log.sql\n│       │   │   ├── stg_beat_credential.sql\n│       │   │   ├── stg_beat_instagram_account.sql\n│       │   │   ├── stg_beat_instagram_post.sql\n│       │   │   ├── stg_beat_instagram_profile_insights.sql\n│       │   │   ├── stg_beat_order.sql\n│       │   │   ├── stg_beat_post_activity_log.sql\n│       │   │   ├── stg_beat_post_log.sql\n│       │   │   ├── stg_beat_profile_log.sql\n│       │   │   ├── stg_beat_profile_relationship_log.sql\n│       │   │   ├── stg_beat_recent_scrape_request_log.sql\n│       │   │   ├── stg_beat_scrape_request_log.sql\n│       │   │   ├── stg_beat_youtube_account.sql\n│       │   │   ├── stg_beat_youtube_post.sql\n│       │   │   └── stg_beat_youtube_profile_insights.sql\n│       │   │\n│       │   └── coffee/                # Coffee Source (16 models)\n│       │       ├── stg_coffee_activity_tracker.sql\n│       │       ├── stg_coffee_campaign_profiles.sql\n│       │       ├── stg_coffee_collection_group.sql\n│       │       ├── stg_coffee_keyword_collection.sql\n│       │       ├── stg_coffee_post_collection.sql\n│       │       ├── stg_coffee_post_collection_item.sql\n│       │       ├── stg_coffee_profile_collection.sql\n│       │       ├── stg_coffee_profile_collection_item.sql\n│       │       ├── stg_coffee_stage_view_instagram_account_lite.sql\n│       │       ├── stg_coffee_stage_view_youtube_account_lite.sql\n│       │       ├── stg_coffee_staging_post_collection_item.sql\n│       │       ├── stg_coffee_staging_profile_collection_item.sql\n│       │       └── stg_coffee_view_*_account_lite.sql\n│       │\n│       └── marts/                     # Mart Layer (83 models)\n│           │\n│           ├── audience/              # Audience Analytics (4)\n│           │   ├── mart_audience_info.sql\n│           │   ├── mart_audience_info_follower.sql\n│           │   ├── mart_audience_info_gpt.sql\n│           │   └── mart_audience_info_private.sql\n│           │\n│           ├── collection/            # Collection Management (13)\n│           │   ├── mart_collection_clicks_ts.sql\n│           │   ├── mart_collection_post.sql\n│           │   ├── mart_collection_post_clicks.sql\n│           │   ├── mart_collection_post_ts.sql\n│           │   ├── mart_collection_social_ts.sql\n│           │   ├── mart_fake_events.sql\n│           │   ├── mart_post_collection_*_post_ts.sql\n│           │   └── mart_profile_collection_*_post_ts.sql\n│           │\n│           ├── discovery/             # Discovery & Analytics (16)\n│           │   ├── mart_influencer_perf.sql\n│           │   ├── mart_instagram_account.sql\n│           │   ├── mart_instagram_gpt_basic_data.sql\n│           │   ├── mart_instagram_hashtags.sql\n│           │   ├── mart_instagram_phone.sql\n│           │   ├── mart_instagram_tracked_profiles.sql\n│           │   ├── mart_insta_predicted_*.sql\n│           │   ├── mart_linked_socials.sql\n│           │   ├── mart_manual_data_*.sql\n│           │   ├── mart_primary_group_metrics.sql\n│           │   ├── mart_youtube_account.sql\n│           │   ├── mart_youtube_account_language.sql\n│           │   ├── mart_youtube_profile_relationship.sql\n│           │   └── mart_youtube_tracked_profiles.sql\n│           │\n│           ├── genre/                 # Genre Analysis (7)\n│           │   ├── mart_genre_instagram_trending_content_*.sql\n│           │   ├── mart_genre_overview.sql\n│           │   ├── mart_genre_overview_all.sql\n│           │   ├── mart_genre_trending_content.sql\n│           │   └── mart_genre_youtube_trending_content_*.sql\n│           │\n│           ├── leaderboard/           # Rankings (14)\n│           │   ├── mart_cross_platform_leaderboard.sql\n│           │   ├── mart_insta_account_monthly.sql\n│           │   ├── mart_insta_account_weekly.sql\n│           │   ├── mart_insta_leaderboard_base.sql\n│           │   ├── mart_instagram_leaderboard.sql\n│           │   ├── mart_leaderboard.sql\n│           │   ├── mart_time_series.sql\n│           │   ├── mart_time_series_with_gaps.sql\n│           │   ├── mart_youtube_leaderboard.sql\n│           │   ├── mart_yt_account_*.sql\n│           │   ├── mart_yt_growth.sql\n│           │   └── mart_yt_leaderboard_base*.sql\n│           │\n│           ├── orders/                # Order Processing (1)\n│           │   └── mart_gcc_orders.sql\n│           │\n│           ├── posts/                 # Post Analytics (3)\n│           │   ├── mart_instagram_post_hashtags.sql\n│           │   ├── mart_instagram_post_tagged.sql\n│           │   └── mart_post_location.sql\n│           │\n│           ├── profile_stats_full/    # Full Profile Stats (8)\n│           │   ├── mart_instagram_account_summary.sql\n│           │   ├── mart_instagram_account_summary_via_handles.sql\n│           │   ├── mart_instagram_all_posts.sql\n│           │   ├── mart_instagram_all_posts_with_ranks.sql\n│           │   ├── mart_instagram_creators_followers_fake_analysis.sql\n│           │   ├── mart_instagram_post_ranks.sql\n│           │   ├── mart_instagram_profiles_followers_count.sql\n│           │   └── mart_instagram_recent_post_stats.sql\n│           │\n│           ├── profile_stats_partial/ # Partial Updates (6)\n│           │   └── mart_instagram_*_partial.sql\n│           │\n│           └── staging_collection/    # Staging Collections (9)\n│               └── mart_staging_*.sql\n│\n├── .dbt/                              # dbt Configuration\n│   ├── profiles.yml                   # Database connections\n│   ├── dbt_project.yml\n│   ├── dbt_packages/\n│   ├── logs/\n│   └── target/\n│\n├── scripts/\n│   └── start.sh                       # Deployment startup\n│\n├── requirements.txt                   # Python dependencies (350+)\n├── start.sh                           # Project startup\n└── README.md                          # Documentation\n\\`\\`\\`\n\n---\n\n## 2. TECHNOLOGY STACK\n\n### Core Technologies\n| Category | Technology | Version | Purpose |\n|----------|-----------|---------|---------|\n| **Orchestration** | Apache Airflow | 2.6.3 | Workflow scheduling & monitoring |\n| **Data Transformation** | dbt-core | 1.3.1 | SQL-based transformations |\n| **Analytics DB** | ClickHouse | via dbt-clickhouse 1.3.2 | OLAP queries, real-time analytics |\n| **Transactional DB** | PostgreSQL | 5.4.0 | Operational data storage |\n| **Cloud Storage** | AWS S3 | gcc-social-data bucket | Data staging & backups |\n| **CI/CD** | GitLab CI | -- | Deployment pipeline |\n\n### Python Dependencies (350+)\n\\`\\`\\`\n# Airflow & Extensions\napache-airflow==2.6.3\nairflow-dbt-python==0.15.2\nairflow-clickhouse-plugin==1.0.0\n\n# Database Drivers\nclickhouse-connect==0.5.12\nclickhouse-driver==0.2.5\npsycopg2-binary==2.9.5\nSQLAlchemy==1.4.45\n\n# dbt\ndbt-core==1.3.1\ndbt-clickhouse==1.3.2\ndbt-postgres==1.3.1\n\n# ML/Data Libraries\ntensorflow==2.11.0\ntorch==2.0.1\nscikit-learn==1.0.2\ntransformers==4.29.2\npandas==1.3.5\nnumpy==1.21.6\n\n# Messaging & Monitoring\npika==1.3.2  # RabbitMQ\nslack-sdk==3.21.3\nloguru==0.7.0\n\n# HTTP & Web\nrequests\nparamiko  # SSH\n\\`\\`\\`\n\n---\n\n## 3. APACHE AIRFLOW DAGs (76 TOTAL)\n\n### DAG Category Breakdown\n\n| Category | Count | Description |\n|----------|-------|-------------|\n| dbt Orchestration | 11 | Transformation pipelines |\n| Instagram Sync | 17 | Instagram data collection |\n| YouTube Sync | 12 | YouTube data collection |\n| Collection/Leaderboard Sync | 15 | Analytics sync |\n| Operational/Verification | 9 | Data quality & operations |\n| Asset Upload | 7 | Media asset processing |\n| Utility | 5 | One-off & helper DAGs |\n\n### Scheduling Frequencies\n\n| Frequency | DAGs | Purpose |\n|-----------|------|---------|\n| \\`*/5 * * * *\\` | 2 | Real-time: dbt_recent_scl, post_ranker |\n| \\`*/10 * * * *\\` | 5 | Near real-time: Profile lookups |\n| \\`*/15 * * * *\\` | 2 | Core: dbt_core, post_ranker_partial |\n| \\`*/30 * * * *\\` | 2 | Collections: dbt_collections, staging |\n| \\`0 * * * *\\` | 12 | Hourly: Most sync operations |\n| \\`0 */3 * * *\\` | 8 | Every 3 hours: Heavy syncs |\n| \\`0 0 * * *\\` | 15 | Daily midnight: Full refreshes |\n| \\`0 19 * * *\\` | 1 | Daily 19:00: dbt_daily |\n| \\`15 20 * * *\\` | 2 | Daily 20:15: Leaderboards |\n| \\`0 6 */7 * *\\` | 1 | Weekly: dbt_weekly |\n\n### Operator Distribution\n\n| Operator | Count | Usage |\n|----------|-------|-------|\n| **PythonOperator** | 46 | Data fetching, API calls, processing |\n| **PostgresOperator** | 20 | Data loading, table operations |\n| **ClickHouseOperator** | 19 | Export queries, analytics |\n| **SSHOperator** | 18 | File transfer, remote commands |\n| **DbtRunOperator** | 11 | dbt model execution |\n\n### Connection IDs Used\n\\`\\`\\`python\nconnections = {\n    \"clickhouse_gcc\": \"Primary ClickHouse cluster\",\n    \"prod_pg\": \"Production PostgreSQL\",\n    \"stage_pg\": \"Staging PostgreSQL\",\n    \"ssh_prod_pg\": \"SSH to production server\",\n    \"ssh_stage_pg\": \"SSH to staging server\",\n    \"beat\": \"Beat API service\",\n    \"slack_failure_conn\": \"Slack failure notifications\",\n    \"slack_success_conn\": \"Slack success notifications\"\n}\n\\`\\`\\`\n\n---\n\n## 4. dbt MODELS (112 TOTAL)\n\n### Model Architecture\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────┐\n│                     dbt MODEL LAYERS                             │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  SOURCES                                                         │\n│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │\n│  │ beat_replica │  │   vidooly    │  │    coffee    │          │\n│  │ Instagram/YT │  │ Cross-plat   │  │ Campaigns    │          │\n│  └──────────────┘  └──────────────┘  └──────────────┘          │\n│         ↓                 ↓                 ↓                    │\n│  ┌──────────────────────────────────────────────────────────┐  │\n│  │              STAGING LAYER (29 models)                    │  │\n│  │  stg_beat_*  (13)  │  stg_coffee_*  (16)                 │  │\n│  │  - Raw data extraction with minimal transformation        │  │\n│  │  - Type casting, NULL handling                            │  │\n│  └──────────────────────────────────────────────────────────┘  │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────────────────────┐  │\n│  │               MART LAYER (83 models)                      │  │\n│  │                                                           │  │\n│  │  ┌─────────────────┐  ┌─────────────────┐               │  │\n│  │  │ Audience (4)    │  │ Collection (13) │               │  │\n│  │  │ mart_audience_* │  │ mart_collection*│               │  │\n│  │  └─────────────────┘  └─────────────────┘               │  │\n│  │                                                           │  │\n│  │  ┌─────────────────┐  ┌─────────────────┐               │  │\n│  │  │ Discovery (16)  │  │ Genre (7)       │               │  │\n│  │  │ mart_instagram_ │  │ mart_genre_*    │               │  │\n│  │  │ mart_youtube_*  │  │                 │               │  │\n│  │  └─────────────────┘  └─────────────────┘               │  │\n│  │                                                           │  │\n│  │  ┌─────────────────┐  ┌─────────────────┐               │  │\n│  │  │ Leaderboard(14) │  │ Profile Stats   │               │  │\n│  │  │ mart_leaderboard│  │ Full (8)        │               │  │\n│  │  │ mart_time_series│  │ Partial (6)     │               │  │\n│  │  └─────────────────┘  └─────────────────┘               │  │\n│  │                                                           │  │\n│  │  ┌─────────────────┐  ┌─────────────────┐               │  │\n│  │  │ Posts (3)       │  │ Orders (1)      │               │  │\n│  │  │ Staging Col (9) │  │ mart_gcc_orders │               │  │\n│  │  └─────────────────┘  └─────────────────┘               │  │\n│  └──────────────────────────────────────────────────────────┘  │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### dbt Tags Classification\n\n| Tag | Count | Schedule | Purpose |\n|-----|-------|----------|---------|\n| **core** | 11 | */15 min | Core business metrics |\n| **deprecated** | 12 | Excluded | Legacy/unused models |\n| **post_ranker** | 10 | */5 hours | Post ranking algorithms |\n| **collections** | 2 | */30 min | Collection management |\n| **daily** | 1 | 19:00 UTC | Daily batch processing |\n| **hourly** | 1 | Every 2h | Hourly updates |\n| **weekly** | 1 | Weekly | Weekly aggregates |\n| **staging_collections** | 1 | */30 min | Staging area |\n| **gcc_orders** | 1 | Daily | Order processing |\n| **account_tracker_stats** | 1 | Hourly | Account tracking |\n\n### Materialization Strategies\n\n\\`\\`\\`sql\n-- Table (most common) - Full refresh\n{{ config(materialized='table') }}\n\n-- Incremental - For time-series data\n{{ config(\n    materialized='incremental',\n    engine='ReplacingMergeTree()',\n    order_by='(profile_id, date)',\n    incremental_strategy='append'\n) }}\n\n-- View - For simple transformations (rare)\n{{ config(materialized='view') }}\n\\`\\`\\`\n\n### Staging Models (29)\n\n**Beat Source (13 models):**\n\\`\\`\\`sql\n-- stg_beat_instagram_account\nSELECT\n    profile_id,\n    handle,\n    full_name,\n    biography,\n    followers_count,\n    following_count,\n    posts_count,\n    is_verified,\n    is_business,\n    category,\n    external_url,\n    profile_pic_url,\n    created_at,\n    updated_at\nFROM beat_replica.instagram_account\n\n-- stg_beat_instagram_post\nSELECT\n    post_id,\n    profile_id,\n    short_code,\n    post_type,\n    caption,\n    likes_count,\n    comments_count,\n    views_count,\n    timestamp,\n    location_id,\n    hashtags,\n    mentions\nFROM beat_replica.instagram_post\n\\`\\`\\`\n\n**Coffee Source (16 models):**\n\\`\\`\\`sql\n-- stg_coffee_post_collection\nSELECT\n    collection_id,\n    name,\n    partner_id,\n    is_active,\n    show_in_report,\n    created_at\nFROM coffee.post_collection\n\n-- stg_coffee_campaign_profiles\nSELECT\n    campaign_id,\n    profile_id,\n    status,\n    deliverables_count,\n    completed_deliverables\nFROM coffee.campaign_profiles\n\\`\\`\\`\n\n### Key Mart Models (83)\n\n**mart_instagram_account (Core Discovery Model):**\n\\`\\`\\`sql\n{{ config(\n    materialized='table',\n    tags=['core', 'hourly']\n) }}\n\nWITH base_accounts AS (\n    SELECT * FROM {{ ref('stg_beat_instagram_account') }}\n),\n\npost_stats AS (\n    SELECT\n        profile_id,\n        COUNT(*) as total_posts,\n        AVG(likes_count) as avg_likes,\n        AVG(comments_count) as avg_comments,\n        SUM(likes_count) as total_likes,\n        SUM(comments_count) as total_comments\n    FROM {{ ref('stg_beat_instagram_post') }}\n    WHERE timestamp > now() - INTERVAL 30 DAY\n    GROUP BY profile_id\n),\n\nengagement AS (\n    SELECT\n        profile_id,\n        (avg_likes + avg_comments) / NULLIF(followers_count, 0) * 100 as engagement_rate\n    FROM base_accounts\n    JOIN post_stats USING (profile_id)\n)\n\nSELECT\n    a.*,\n    ps.total_posts,\n    ps.avg_likes,\n    ps.avg_comments,\n    e.engagement_rate,\n    -- Rankings\n    row_number() OVER (ORDER BY followers_count DESC) as followers_rank,\n    row_number() OVER (PARTITION BY category ORDER BY followers_count DESC) as followers_rank_by_cat,\n    row_number() OVER (PARTITION BY language ORDER BY followers_count DESC) as followers_rank_by_lang\nFROM base_accounts a\nLEFT JOIN post_stats ps USING (profile_id)\nLEFT JOIN engagement e USING (profile_id)\n\\`\\`\\`\n\n**mart_leaderboard (Multi-dimensional Rankings):**\n\\`\\`\\`sql\n{{ config(\n    materialized='table',\n    tags=['daily']\n) }}\n\nSELECT\n    profile_id,\n    handle,\n    platform,\n    followers_count,\n    engagement_rate,\n    category,\n    language,\n    country,\n\n    -- Global ranks\n    followers_rank,\n    engagement_rank,\n\n    -- Category ranks\n    followers_rank_by_cat,\n    engagement_rank_by_cat,\n\n    -- Language ranks\n    followers_rank_by_lang,\n    engagement_rank_by_lang,\n\n    -- Combined ranks\n    followers_rank_by_cat_lang,\n    engagement_rank_by_cat_lang,\n\n    -- Rank changes (vs last month)\n    followers_rank - lag(followers_rank) OVER (\n        PARTITION BY profile_id ORDER BY snapshot_date\n    ) as rank_change,\n\n    snapshot_date\nFROM {{ ref('mart_instagram_account') }}\n\\`\\`\\`\n\n**mart_time_series (Incremental):**\n\\`\\`\\`sql\n{{ config(\n    materialized='incremental',\n    engine='ReplacingMergeTree()',\n    order_by='(profile_id, date)',\n    unique_key='(profile_id, date)'\n) }}\n\nSELECT\n    profile_id,\n    toDate(created_at) as date,\n    argMax(followers_count, created_at) as followers,\n    argMax(following_count, created_at) as following,\n    argMax(posts_count, created_at) as posts,\n    max(created_at) as last_updated\nFROM {{ ref('stg_beat_instagram_account') }}\n\n{% if is_incremental() %}\nWHERE created_at > (SELECT max(last_updated) - INTERVAL 4 HOUR FROM {{ this }})\n{% endif %}\n\nGROUP BY profile_id, date\n\\`\\`\\`\n\n---\n\n## 5. DATA FLOW ARCHITECTURE\n\n### Three-Layer Data Flow Pattern\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────┐\n│                    DATA FLOW ARCHITECTURE                        │\n└─────────────────────────────────────────────────────────────────┘\n\nLAYER 1: CLICKHOUSE (Analytics Engine)\n┌─────────────────────────────────────────────────────────────────┐\n│  ClickHouse Database (172.31.28.68:9000)                        │\n│  ├── dbt.stg_* (29 staging tables)                              │\n│  └── dbt.mart_* (83 mart tables)                                │\n│                                                                  │\n│  Features:                                                       │\n│  - ReplacingMergeTree for upserts                               │\n│  - Partitioning by date for query optimization                  │\n│  - ArrayMap for hashtag normalization                           │\n│  - argMax for latest value extraction                           │\n└─────────────────────────────────────────────────────────────────┘\n                            ↓\n                 (ClickHouseOperator)\n                 INSERT INTO FUNCTION s3(...)\n                            ↓\nLAYER 2: AWS S3 (Staging)\n┌─────────────────────────────────────────────────────────────────┐\n│  S3 Bucket: gcc-social-data                                     │\n│  Path: /data-pipeline/tmp/*.json                                │\n│  Format: JSONEachRow                                            │\n│                                                                  │\n│  Settings:                                                       │\n│  - s3_truncate_on_insert=1                                      │\n│  - Automatic compression                                        │\n└─────────────────────────────────────────────────────────────────┘\n                            ↓\n                 (SSHOperator)\n                 aws s3 cp s3://... /tmp/\n                            ↓\nLAYER 3: POSTGRESQL (Operational)\n┌─────────────────────────────────────────────────────────────────┐\n│  PostgreSQL Database (172.31.2.21:5432)                         │\n│  Database: beat                                                 │\n│                                                                  │\n│  Tables:                                                        │\n│  - instagram_account                                            │\n│  - youtube_account                                              │\n│  - collection_post_metrics_summary                              │\n│  - leaderboard_*                                                │\n│                                                                  │\n│  Operations:                                                    │\n│  - COPY from /tmp/*.json                                        │\n│  - JSONB parsing with type casting                              │\n│  - Atomic table swap (RENAME)                                   │\n└─────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### Detailed Data Flow Example (dbt_collections DAG)\n\n\\`\\`\\`python\n# Step 1: dbt transformation in ClickHouse\ndbt_run_task = DbtRunOperator(\n    task_id='dbt_run_collections',\n    models='tag:collections',\n    profiles_dir='/Users/.dbt',\n    target='gcc_warehouse'  # ClickHouse\n)\n\n# Step 2: Export to S3\nclickhouse_export = ClickHouseOperator(\n    task_id='export_to_s3',\n    clickhouse_conn_id='clickhouse_gcc',\n    sql=\"\"\"\n        INSERT INTO FUNCTION s3(\n            's3://gcc-social-data/data-pipeline/tmp/collection_post.json',\n            'AWS_KEY', 'AWS_SECRET',\n            'JSONEachRow'\n        )\n        SELECT * FROM dbt.mart_collection_post\n        SETTINGS s3_truncate_on_insert=1\n    \"\"\"\n)\n\n# Step 3: Download via SSH\nssh_download = SSHOperator(\n    task_id='download_from_s3',\n    ssh_conn_id='ssh_prod_pg',\n    command='aws s3 cp s3://gcc-social-data/data-pipeline/tmp/collection_post.json /tmp/'\n)\n\n# Step 4: Load into PostgreSQL temp table\npg_load = PostgresOperator(\n    task_id='load_temp_table',\n    postgres_conn_id='prod_pg',\n    sql=\"\"\"\n        CREATE TEMP TABLE tmp_collection_post (data JSONB);\n        COPY tmp_collection_post FROM '/tmp/collection_post.json';\n    \"\"\"\n)\n\n# Step 5: Transform and insert\npg_transform = PostgresOperator(\n    task_id='transform_insert',\n    postgres_conn_id='prod_pg',\n    sql=\"\"\"\n        INSERT INTO collection_post_metrics_summary_new\n        SELECT\n            (data->>'collection_id')::bigint,\n            (data->>'post_short_code')::text,\n            (data->>'likes_count')::bigint,\n            (data->>'comments_count')::bigint,\n            (data->>'engagement_rate')::float\n        FROM tmp_collection_post\n    \"\"\"\n)\n\n# Step 6: Atomic table swap\npg_swap = PostgresOperator(\n    task_id='atomic_swap',\n    postgres_conn_id='prod_pg',\n    sql=\"\"\"\n        ALTER TABLE collection_post_metrics_summary\n            RENAME TO collection_post_metrics_summary_old_bkp;\n        ALTER TABLE collection_post_metrics_summary_new\n            RENAME TO collection_post_metrics_summary;\n    \"\"\"\n)\n\n# Task dependencies\ndbt_run_task >> clickhouse_export >> ssh_download >> pg_load >> pg_transform >> pg_swap\n\\`\\`\\`\n\n---\n\n## 6. CLICKHOUSE INTEGRATION\n\n### Connection Configuration\n\\`\\`\\`python\n# Connection details\nconnection = {\n    \"host\": \"172.31.28.68\",\n    \"port\": 9000,\n    \"database\": \"dbt\",\n    \"user\": \"airflow\",\n    \"threads\": 3,\n    \"send_receive_timeout\": 3600\n}\n\\`\\`\\`\n\n### Key ClickHouse Features Used\n\n**1. ReplacingMergeTree Engine:**\n\\`\\`\\`sql\n-- Efficient upsert operations\nENGINE = ReplacingMergeTree(updated_at)\nORDER BY (profile_id, date)\nPARTITION BY toYYYYMM(date)\n\\`\\`\\`\n\n**2. S3 Integration:**\n\\`\\`\\`sql\n-- Export to S3\nINSERT INTO FUNCTION s3(\n    's3://bucket/path/file.json',\n    'KEY', 'SECRET',\n    'JSONEachRow'\n)\nSELECT * FROM dbt.mart_table\nSETTINGS s3_truncate_on_insert=1\n\n-- Import from S3\nSELECT * FROM s3(\n    's3://bucket/path/file.csv',\n    'KEY', 'SECRET',\n    'CSV'\n)\n\\`\\`\\`\n\n**3. Window Functions:**\n\\`\\`\\`sql\n-- Ranking\nrow_number() OVER (ORDER BY followers_count DESC)\nrow_number() OVER (PARTITION BY category ORDER BY followers_count DESC)\n\n-- Latest value\nargMax(followers_count, created_at)\n\n-- Cardinality\nuniqExact(profile_id)\n\\`\\`\\`\n\n**4. Array Operations:**\n\\`\\`\\`sql\n-- Hashtag extraction\narrayMap(x -> lower(trim(x)), splitByChar(',', hashtags))\n\n-- Array aggregation\ngroupArray(hashtag)\n\\`\\`\\`\n\n### Query Patterns\n\n**Incremental Processing:**\n\\`\\`\\`sql\n{% if is_incremental() %}\nWHERE created_at > (\n    SELECT max(created_at) - INTERVAL 4 HOUR\n    FROM {{ this }}\n)\n{% endif %}\n\\`\\`\\`\n\n**Partition Pruning:**\n\\`\\`\\`sql\nWHERE toYYYYMM(date) >= toYYYYMM(now() - INTERVAL 30 DAY)\n\\`\\`\\`\n\n---\n\n## 7. BUSINESS LOGIC & METRICS\n\n### Core Business Problems Solved\n\n**1. Influencer Discovery & Ranking:**\n- Multi-dimensional leaderboards (followers, engagement, growth)\n- Segmentation by category, language, country\n- Monthly snapshots with rank change tracking\n- Cross-platform comparisons (Instagram vs YouTube)\n\n**2. Collection Analytics:**\n- Curated collections of posts/profiles\n- Aggregated engagement metrics\n- Time-series analysis for trending content\n- Sentiment tracking\n\n**3. Campaign Management:**\n- Order tracking from campaigns\n- Content verification for deliverables\n- Referral journey tracking\n- Payout processing\n\n**4. Cross-Platform Linking:**\n- Match Instagram profiles with YouTube channels\n- Unified audience insights\n- Combined reach calculations\n\n### Key Metrics Computed\n\n**Profile-Level Metrics:**\n\\`\\`\\`sql\n-- Engagement calculations\nengagement_rate = (avg_likes + avg_comments) / followers_count * 100\n\n-- Growth metrics\nfollowers_change = current_followers - previous_month_followers\ngrowth_rate = followers_change / previous_month_followers * 100\n\n-- Activity metrics\navg_posts_per_week = total_posts / weeks_active\navg_likes = total_likes / total_posts\navg_comments = total_comments / total_posts\navg_views = total_views / total_videos  -- YouTube\n\\`\\`\\`\n\n**Ranking Dimensions:**\n\\`\\`\\`sql\n-- Global ranks\nfollowers_rank              -- All profiles\nengagement_rank             -- By engagement rate\n\n-- Category ranks\nfollowers_rank_by_cat       -- Within category\nengagement_rank_by_cat      -- Within category\n\n-- Language ranks\nfollowers_rank_by_lang      -- Within language\nengagement_rank_by_lang     -- Within language\n\n-- Combined ranks\nfollowers_rank_by_cat_lang  -- Category + Language\nengagement_rank_by_cat_lang -- Category + Language\n\\`\\`\\`\n\n---\n\n## 8. DAG CONFIGURATIONS\n\n### dbt DAGs Configuration\n\n**dbt_core (Critical Path):**\n\\`\\`\\`python\ndag = DAG(\n    dag_id='dbt_core',\n    default_args={\n        'owner': 'airflow',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5),\n        'on_failure_callback': SlackNotifier.slack_fail_alert\n    },\n    schedule_interval='*/15 * * * *',  # Every 15 minutes\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    max_active_runs=1,\n    concurrency=1,\n    dagrun_timeout=timedelta(minutes=60)\n)\n\ndbt_run = DbtRunOperator(\n    task_id='dbt_run_core',\n    models='tag:core',\n    profiles_dir='/Users/.dbt',\n    target='gcc_warehouse',\n    dag=dag\n)\n\\`\\`\\`\n\n**dbt_daily (Full Refresh):**\n\\`\\`\\`python\ndag = DAG(\n    dag_id='dbt_daily',\n    schedule_interval='0 19 * * *',  # 19:00 UTC daily\n    dagrun_timeout=timedelta(minutes=360),\n    max_active_runs=1\n)\n\ndbt_run = DbtRunOperator(\n    task_id='dbt_run_daily',\n    models='tag:daily',\n    full_refresh=True\n)\n\\`\\`\\`\n\n### Sync DAGs Configuration\n\n**sync_insta_collection_posts:**\n\\`\\`\\`python\ndag = DAG(\n    dag_id='sync_insta_collection_posts',\n    schedule_interval='*/10 * * * *',  # Every 10 minutes\n    max_active_runs=1,\n    concurrency=1\n)\n\ndef create_scrape_requests(**context):\n    \"\"\"Creates scrape requests for collection posts\"\"\"\n    connection = BaseHook.get_connection(\"clickhouse_gcc\")\n    client = clickhouse_connect.get_client(\n        host=connection.host,\n        password=connection.password,\n        username=connection.login\n    )\n\n    # Query posts needing refresh\n    sql = \"\"\"\n        SELECT post_id, short_code\n        FROM dbt.mart_collection_post\n        WHERE last_scraped < now() - INTERVAL 1 HOUR\n        LIMIT 1000\n    \"\"\"\n    result = client.query(sql)\n\n    # Create scrape requests via Beat API\n    for row in result:\n        requests.post(\n            f'{BEAT_URL}/scrape_request_log/flow/instagram_post',\n            json={'short_code': row['short_code']}\n        )\n\ncreate_requests_task = PythonOperator(\n    task_id='create_scrape_requests',\n    python_callable=create_scrape_requests,\n    dag=dag\n)\n\\`\\`\\`\n\n**sync_leaderboard_prod (ClickHouse → S3 → PostgreSQL):**\n\\`\\`\\`python\ndag = DAG(\n    dag_id='sync_leaderboard_prod',\n    schedule_interval='15 20 * * *',  # Daily at 20:15 UTC\n    max_active_runs=1\n)\n\n# Task 1: Export from ClickHouse to S3\nexport_task = ClickHouseOperator(\n    task_id='export_leaderboard',\n    clickhouse_conn_id='clickhouse_gcc',\n    sql=\"\"\"\n        INSERT INTO FUNCTION s3(\n            's3://gcc-social-data/data-pipeline/tmp/leaderboard.json',\n            'KEY', 'SECRET', 'JSONEachRow'\n        )\n        SELECT * FROM dbt.mart_leaderboard\n        SETTINGS s3_truncate_on_insert=1\n    \"\"\"\n)\n\n# Task 2: Download via SSH\ndownload_task = SSHOperator(\n    task_id='download_leaderboard',\n    ssh_conn_id='ssh_prod_pg',\n    command='aws s3 cp s3://gcc-social-data/data-pipeline/tmp/leaderboard.json /tmp/'\n)\n\n# Task 3: Load into PostgreSQL\nload_task = PostgresOperator(\n    task_id='load_leaderboard',\n    postgres_conn_id='prod_pg',\n    sql=\"\"\"\n        -- Create temp table\n        CREATE TEMP TABLE tmp_leaderboard (data JSONB);\n\n        -- Load JSON\n        COPY tmp_leaderboard FROM '/tmp/leaderboard.json';\n\n        -- Insert with transformation\n        INSERT INTO leaderboard_new\n        SELECT\n            (data->>'profile_id')::bigint,\n            (data->>'handle')::text,\n            (data->>'followers_rank')::int,\n            (data->>'engagement_rank')::int,\n            now()\n        FROM tmp_leaderboard;\n    \"\"\",\n    execution_timeout=timedelta(seconds=14400)\n)\n\n# Task 4: Atomic swap\nswap_task = PostgresOperator(\n    task_id='swap_tables',\n    postgres_conn_id='prod_pg',\n    sql=\"\"\"\n        ALTER TABLE leaderboard RENAME TO leaderboard_old;\n        ALTER TABLE leaderboard_new RENAME TO leaderboard;\n        DROP TABLE IF EXISTS leaderboard_old;\n    \"\"\"\n)\n\nexport_task >> download_task >> load_task >> swap_task\n\\`\\`\\`\n\n---\n\n## 9. CI/CD & DEPLOYMENT\n\n### GitLab CI Configuration\n\\`\\`\\`yaml\nstages:\n  - deploy_prod\n\ndeploy_prod:\n  stage: deploy_prod\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - beat-deployer-1\n  environment:\n    name: prod\n  when: manual  # Manual trigger required\n  only:\n    - master\n\\`\\`\\`\n\n### Deployment Script (start.sh)\n\\`\\`\\`bash\n#!/bin/bash\n\n# Copy local configuration\ncp airflow.cfg.local airflow.cfg\n\n# Initialize Airflow database\nairflow db init\n\n# Start Airflow in standalone mode\nairflow standalone\n\\`\\`\\`\n\n### dbt Profiles Configuration\n\\`\\`\\`yaml\n# .dbt/profiles.yml\n\ngcc_social:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      host: 172.31.2.21\n      port: 5432\n      user: airflow\n      password: \"{{ env_var('PG_PASSWORD') }}\"\n      database: beat\n      schema: public\n      threads: 6\n\ngcc_warehouse:\n  target: prod\n  outputs:\n    prod:\n      type: clickhouse\n      host: 172.31.28.68\n      port: 9000\n      user: airflow\n      password: \"{{ env_var('CH_PASSWORD') }}\"\n      database: dbt\n      schema: dbt\n      threads: 3\n\\`\\`\\`\n\n---\n\n## 10. KEY METRICS SUMMARY\n\n| Metric | Value |\n|--------|-------|\n| **Total DAG Files** | 76 |\n| **Lines of Python (DAGs)** | 7,406 |\n| **Staging Models** | 29 |\n| **Mart Models** | 83 |\n| **Total dbt Models** | 112 |\n| **Git Commits** | 1,476 |\n| **Python Dependencies** | 350+ |\n| **ClickHouse Operators** | 19 |\n| **PostgreSQL Operators** | 20 |\n| **SSH Operators** | 18 |\n| **Python Operators** | 46 |\n| **dbt Run Operators** | 11 |\n\n---\n\n## 11. DATA SOURCES INTEGRATED\n\n| Source | Type | Data |\n|--------|------|------|\n| **Instagram API** | Social | Posts, profiles, insights, stories, comments, followers |\n| **YouTube API** | Social | Videos, channels, profiles, insights, comments |\n| **Beat Database** | Internal | Scrape requests, credentials, asset logs |\n| **Coffee Database** | Internal | Campaigns, profiles, collections, deliverables |\n| **Shopify** | E-commerce | Orders |\n| **Vidooly** | External | YouTube channel data |\n| **S3** | Storage | Temporary data staging |\n\n---\n\n## 12. SKILLS DEMONSTRATED\n\n### Data Engineering\n- **ETL/ELT Architecture**: Modern data stack with Airflow + dbt\n- **Workflow Orchestration**: 76 production DAGs with complex dependencies\n- **SQL Optimization**: ClickHouse-specific tuning, window functions\n- **Incremental Processing**: Smart backfill with 4-hour windows\n- **Data Quality**: Validation, atomic operations, backup strategies\n\n### Analytics & Data Modeling\n- **Star Schema Design**: Fact and dimension tables\n- **dbt Mastery**: Staging/mart layers, incremental materializations\n- **Multi-dimensional Analysis**: Rankings across categories, languages, regions\n- **Time-Series Processing**: Trend detection, growth tracking\n\n### Database Administration\n- **ClickHouse**: ReplacingMergeTree, partitioning, S3 integration\n- **PostgreSQL**: JSONB parsing, atomic table swaps, copy operations\n- **Cross-Database Sync**: ClickHouse → S3 → PostgreSQL patterns\n\n### DevOps & Infrastructure\n- **CI/CD**: GitLab pipeline with manual production deploys\n- **Cloud Integration**: AWS S3, SSH tunneling\n- **Monitoring**: Slack notifications, execution timeouts\n- **Configuration Management**: dbt profiles, Airflow connections\n\n---\n\n## 13. INTERVIEW TALKING POINTS\n\n### 1. \"Tell me about a data platform you built\"\n- **Scale**: 76 DAGs processing billions of records daily\n- **Architecture**: Modern data stack with Airflow + dbt + ClickHouse\n- **Dual Database**: ClickHouse for OLAP, PostgreSQL for OLTP\n- **Outcome**: Real-time influencer discovery and campaign analytics\n\n### 2. \"Describe your experience with dbt\"\n- **Models**: Built 112 models (29 staging + 83 marts)\n- **Incremental**: ReplacingMergeTree for efficient upserts\n- **Tags**: Organized execution by core, daily, hourly, collections\n- **Testing**: Data quality validation framework\n\n### 3. \"How do you handle large-scale data processing?\"\n- **ClickHouse**: OLAP engine for billion-record analytics\n- **Partitioning**: Date-based for query optimization\n- **Incremental**: 4-hour lookback windows\n- **Parallelism**: 25 worker threads, concurrent DAGs\n\n### 4. \"Explain a complex data pipeline you built\"\n- **Flow**: ClickHouse → S3 → SSH → PostgreSQL → Atomic swap\n- **Transformation**: JSONB parsing with type casting\n- **Reliability**: Backup tables, atomic operations\n- **Monitoring**: Slack alerts, execution timeouts\n\n### 5. \"How do you ensure data quality?\"\n- **Atomic Operations**: Table rename prevents partial updates\n- **Validation**: Type casting, NULL handling\n- **Monitoring**: Slack notifications on failures\n- **Backups**: Old table preserved before swap\n\n---\n\n## 14. NOTABLE OBSERVATIONS\n\n### Production Hardening\n- \\`max_active_runs=1\\` prevents concurrent execution issues\n- \\`catchup=False\\` avoids backfill on schedule changes\n- \\`dagrun_timeout\\` prevents runaway jobs\n- Slack notifications for all failures\n\n### Performance Optimizations\n- \\`ORDER BY\\` clauses for ClickHouse query efficiency\n- \\`PARTITION BY\\` for date-based pruning\n- \\`argMax\\` for efficient latest value extraction\n- \\`uniqExact\\` for accurate cardinality\n\n### Data Pipeline Patterns\n- Three-layer flow: ClickHouse → S3 → PostgreSQL\n- Atomic table swaps for zero-downtime updates\n- 4-hour incremental windows for freshness vs performance\n\n### Security Notes\n- Credentials in profiles.yml (should use secrets manager)\n- AWS keys in DAG code (should use IAM roles)\n- Hardcoded channel lists (127 YouTube channels)\n\n---\n\n*Analysis covers 17,500+ lines of code across 76 DAGs, 112 dbt models, and complete data infrastructure spanning ClickHouse, PostgreSQL, S3, and multiple APIs.*\n"
  },
  {
    "id": "ANALYSIS_event_grpc",
    "title": "Event-grpc Analysis",
    "category": "analysis",
    "badge": null,
    "content": "# ULTRA-DEEP ANALYSIS: EVENT-GRPC PROJECT\n\n## PROJECT OVERVIEW\n\n| Attribute | Value |\n|-----------|-------|\n| **Project Name** | Event-gRPC |\n| **Purpose** | High-throughput event ingestion & distribution system for real-time analytics |\n| **Architecture** | gRPC Server + RabbitMQ Message Broker + Multi-Database Sinks |\n| **Language** | Go 1.14 |\n| **Project Size** | 11MB, 10,000+ LOC |\n| **Ports** | 8017 (gRPC), 8019 (HTTP/Gin) |\n\n---\n\n## 1. COMPLETE DIRECTORY STRUCTURE\n\n\\`\\`\\`\nevent-grpc/\n├── main.go (573 lines)              # Entry point with 26 consumer configurations\n├── go.mod / go.sum                   # 30+ direct dependencies\n├── .gitlab-ci.yml                    # CI/CD pipeline\n├── .env.*                            # Environment configs (local, stage, production)\n│\n├── proto/                            # Protocol Buffers\n│   ├── eventservice.proto (688 lines) # 60+ event types defined\n│   ├── healthcheck.proto             # gRPC health service\n│   └── go/bulbulgrpc/                # Generated Go code\n│\n├── eventworker/                      # Main gRPC event worker pool\n├── brancheventworker/                # Branch.io events\n├── vidoolyeventworker/               # Vidooly analytics events\n├── webengageeventworker/             # WebEngage marketing events\n├── graphyeventworker/                # Graphy platform events\n├── shopifyeventworker/               # Shopify e-commerce events\n│\n├── sinker/ (20+ files)               # Event persistence layer\n│   ├── eventsinker.go (38KB)         # Main ClickHouse sink\n│   ├── clickeventsinker.go (78KB)    # Click tracking (600+ lines)\n│   ├── brancheventsinker.go          # Branch attribution\n│   ├── webengageeventsinker.go       # WebEngage API sync\n│   └── parser/                       # Event parsers\n│\n├── model/ (19 files)                 # Database models\n│   ├── event.go                      # Core event model\n│   ├── branchevent.go (49 fields)    # Attribution tracking\n│   └── clickevent.go                 # Click analytics\n│\n├── rabbit/rabbit.go (293 lines)      # RabbitMQ client\n├── clickhouse/clickhouse.go          # ClickHouse connection pool\n├── pg/pg.go                          # PostgreSQL connection pool\n├── cache/                            # Redis + Ristretto caching\n│\n├── config/config.go (283 lines)      # 40+ configuration fields\n├── router/router.go                  # Gin HTTP setup\n├── middleware/                       # Request logging, auth, context\n├── context/context.go (199 lines)    # Gateway context management\n├── client/                           # External API clients\n└── scripts/start.sh                  # Graceful deployment script\n\\`\\`\\`\n\n---\n\n## 2. ARCHITECTURE DEEP DIVE\n\n### System Architecture Diagram\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────┐\n│                        CLIENT APPLICATIONS                               │\n│              (Mobile Apps, Web Apps, Backend Services)                  │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │ gRPC (Protobuf)\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                     EVENT-GRPC SERVICE                                   │\n├─────────────────────────────────────────────────────────────────────────┤\n│  ┌─────────────────────┐    ┌─────────────────────────────────────┐    │\n│  │  gRPC Server        │    │  HTTP Server (Gin)                  │    │\n│  │  Port: 8017         │    │  Port: 8019                         │    │\n│  │  - Dispatch RPC     │    │  - /heartbeat (health)              │    │\n│  │  - Health Check     │    │  - /metrics (Prometheus)            │    │\n│  └──────────┬──────────┘    │  - /vidooly/event                   │    │\n│             │               │  - /branch/event                    │    │\n│             │               │  - /webengage/event                 │    │\n│             │               │  - /shopify/event                   │    │\n│             │               └─────────────────────────────────────┘    │\n│             ▼                                                           │\n│  ┌──────────────────────────────────────────────────────────────┐      │\n│  │              WORKER POOL SYSTEM (6 Pools)                    │      │\n│  ├──────────────────────────────────────────────────────────────┤      │\n│  │  Event Worker Pool      │  Configurable size per pool        │      │\n│  │  Branch Worker Pool     │  Buffered channels (1000+ capacity)│      │\n│  │  WebEngage Worker Pool  │  Safe goroutine execution          │      │\n│  │  Vidooly Worker Pool    │  Panic recovery                    │      │\n│  │  Graphy Worker Pool     │  Event grouping by type            │      │\n│  │  Shopify Worker Pool    │  Timestamp correction              │      │\n│  └──────────────────────────────────────────────────────────────┘      │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │ Publish (JSON)\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                     RABBITMQ MESSAGE BROKER                              │\n├─────────────────────────────────────────────────────────────────────────┤\n│  EXCHANGES:                    │  QUEUES (26 Configured):               │\n│  - grpc_event.tx              │  - grpc_clickhouse_event_q (2 workers) │\n│  - grpc_event.dx              │  - clickhouse_click_event_q (2)        │\n│  - grpc_event_error.dx        │  - app_init_event_q (2)                │\n│  - branch_event.tx            │  - branch_event_q (2)                  │\n│  - webengage_event.dx         │  - webengage_event_q (3)               │\n│  - identity.dx                │  - webengage_ch_event_q (5)            │\n│  - beat.dx                    │  - post_log_events_q (20)              │\n│  - coffee.dx                  │  - profile_log_events_q (2)            │\n│  - shopify_event.dx           │  - sentiment_log_events_q (2)          │\n│  - affiliate.dx               │  - scrape_request_log_events_q (2)     │\n│  - bigboss                    │  - activity_tracker_q (2)              │\n│                               │  ... and 15 more queues                │\n├─────────────────────────────────────────────────────────────────────────┤\n│  FEATURES:                                                               │\n│  - Durable queues             - Retry logic (max 2 attempts)            │\n│  - Error queues               - Dead letter routing                     │\n│  - Prefetch QoS (1)           - Auto-reconnect on failure              │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │ Consume\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                        SINKER LAYER (20+ Sinkers)                       │\n├─────────────────────────────────────────────────────────────────────────┤\n│  EVENT SINKERS:                │  BUFFERED SINKERS:                     │\n│  - SinkEventToClickhouse       │  - TraceLogEventsSinker (batch)       │\n│  - SinkErrorEventToClickhouse  │  - AffiliateOrdersSinker (batch)      │\n│  - SinkClickEventToClickhouse  │  - BigBossVotesSinker (batch)         │\n│  - SinkAppInitEvent            │  - PostLogEventsSinker (batch)        │\n│  - SinkLaunchReferEvent        │  - ProfileLogEventsSinker (batch)     │\n│  - SinkBranchEvent             │  - SentimentLogEventsSinker (batch)   │\n│  - SinkGraphyEvent             │  - ScrapeLogEventsSinker (batch)      │\n│  - SinkShopifyEvent            │  - OrderLogEventsSinker (batch)       │\n│  - SinkWebengageToClickhouse   │  - ActivityTrackerSinker (batch)      │\n│  - SinkWebengageToAPI          │                                        │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │\n        ┌────────────────────────┼────────────────────────┐\n        ▼                        ▼                        ▼\n┌───────────────────┐  ┌───────────────────┐  ┌───────────────────────┐\n│    CLICKHOUSE     │  │    POSTGRESQL     │  │    EXTERNAL APIs      │\n│   (Analytics DB)  │  │   (Transactional) │  │                       │\n├───────────────────┤  ├───────────────────┤  ├───────────────────────┤\n│  Tables:          │  │  Tables:          │  │  - WebEngage API      │\n│  - event          │  │  - referral_event │  │  - Identity Service   │\n│  - error_event    │  │  - user_account   │  │  - Social Stream API  │\n│  - branch_event   │  │                   │  │                       │\n│  - click_event    │  │  Features:        │  │  Features:            │\n│  - trace_log      │  │  - Schema-based   │  │  - Resty HTTP client  │\n│  - webengage_event│  │  - Pool: 10/20    │  │  - Retry logic        │\n│  - graphy_event   │  │  - Transactions   │  │  - Error handling     │\n│  - + 10 more      │  │                   │  │                       │\n├───────────────────┤  └───────────────────┘  └───────────────────────┘\n│  Features:        │\n│  - Multi-DB       │\n│  - Auto-reconnect │\n│  - GORM ORM       │\n│  - Batch inserts  │\n└───────────────────┘\n\\`\\`\\`\n\n---\n\n## 3. PROTOCOL BUFFERS & gRPC SERVICE\n\n### eventservice.proto (688 lines)\n\n\\`\\`\\`protobuf\nsyntax = \"proto3\";\noption go_package = \"proto/go/bulbulgrpc\";\noption java_package = \"com.bulbul.grpc.event\";\n\nservice EventService {\n    rpc dispatch(Events) returns (Response) {}\n}\n\nmessage Events {\n    Header header = 1;\n    repeated Event events = 2;\n}\n\nmessage Header {\n    string sessionId = 1;\n    string bbDeviceId = 2;\n    string userId = 3;\n    string deviceId = 4;\n    string androidAdvertisingId = 5;\n    string clientId = 6;\n    string channel = 7;\n    string os = 8;\n    string clientType = 9;\n    string appLanguage = 10;\n    string merchantId = 11;\n    string ppId = 12;\n    string appVersion = 13;\n    string currentURL = 14;\n    string utmReferrer = 15;\n    string utmPlatform = 16;\n    string utmSource = 17;\n    string utmMedium = 18;\n    string utmCampaign = 19;\n    // ... 5 more fields (24 total)\n}\n\\`\\`\\`\n\n### Event Types Defined (60+)\n\n**User Flow Events:**\n- \\`LaunchEvent\\`, \\`LaunchReferEvent\\`\n- \\`PageOpenedEvent\\`, \\`PageLoadedEvent\\`, \\`PageLoadFailedEvent\\`\n\n**Widget Events:**\n- \\`WidgetViewEvent\\`, \\`WidgetCtaClickedEvent\\`\n- \\`WidgetElementViewEvent\\`, \\`WidgetElementClickedEvent\\`\n\n**Commerce Events:**\n- \\`AddToCartEvent\\`, \\`GoToPaymentsEvent\\`\n- \\`InitiatePurchaseEvent\\`, \\`CompletePurchaseEvent\\`, \\`PurchaseFailedEvent\\`\n\n**Streaming Events:**\n- \\`StreamEnterEvent\\`, \\`ChooseProductEvent\\`\n- \\`SocialStreamEnterEvent\\`, \\`SocialStreamActionEvent\\`\n\n**Review Events:**\n- \\`InitiateReview\\`, \\`EnterReviewScreen\\`, \\`SubmitReview\\`\n- \\`ViewAllReviews\\`, \\`ExpandReviewImage\\`\n\n**Authentication Events:**\n- \\`PhoneVerificationInitiateEvent\\`\n- \\`PhoneNumberEntered\\`, \\`OTPVerifiedEvent\\`\n\n**System Events:**\n- \\`AppStartEvent\\`, \\`NotificationActionEvent\\`\n- \\`SessionIdChangeEvent\\`, \\`TestEvent\\`\n\n---\n\n## 4. WORKER POOL IMPLEMENTATION\n\n### Architecture Pattern\n\n\\`\\`\\`go\n// Singleton pattern with lazy initialization\nvar (\n    eventWrapperChannel chan bulbulgrpc.Events\n    channelInit         sync.Once\n)\n\nfunc GetChannel(config config.Config) chan bulbulgrpc.Events {\n    channelInit.Do(func() {\n        // Create buffered channel\n        eventWrapperChannel = make(chan bulbulgrpc.Events,\n            config.EVENT_WORKER_POOL_CONFIG.EVENT_BUFFERED_CHANNEL_SIZE)\n\n        // Initialize worker pool\n        initWorkerPool(config,\n            config.EVENT_WORKER_POOL_CONFIG.EVENT_WORKER_POOL_SIZE,\n            eventWrapperChannel)\n    })\n    return eventWrapperChannel\n}\n\nfunc initWorkerPool(config config.Config, poolSize int,\n    eventChannel <-chan bulbulgrpc.Events) {\n    for i := 0; i < poolSize; i++ {\n        safego.GoNoCtx(func() {\n            worker(config, eventChannel)\n        })\n    }\n}\n\\`\\`\\`\n\n### Worker Processing Logic\n\n\\`\\`\\`go\nfunc worker(config config.Config, eventChannel <-chan bulbulgrpc.Events) {\n    for e := range eventChannel {\n        rabbitConn := rabbit.Rabbit(config)\n\n        // Group events by type\n        eventsGrouped := make(map[string][]*bulbulgrpc.Event)\n        for _, evt := range e.Events {\n            eventName := fmt.Sprintf(\"%v\", reflect.TypeOf(evt.GetEventOf()))\n            eventName = strings.ReplaceAll(eventName, \"*bulbulgrpc.Event_\", \"\")\n\n            // Timestamp correction (future timestamps set to now)\n            et := time.Unix(transformerToInt64(evt.Timestamp)/1000, 0)\n            if et.Unix() > time.Now().Unix() {\n                evt.Timestamp = strconv.FormatInt(time.Now().Unix()*1000, 10)\n            }\n\n            eventsGrouped[eventName] = append(eventsGrouped[eventName], evt)\n        }\n\n        // Publish grouped events to RabbitMQ\n        for eventName, events := range eventsGrouped {\n            e := bulbulgrpc.Events{Header: header, Events: events}\n            if b, err := protojson.Marshal(&e); err == nil {\n                rabbitConn.Publish(\"grpc_event.tx\", eventName, b, map[string]interface{}{})\n            }\n        }\n    }\n}\n\\`\\`\\`\n\n### Worker Pool Configurations\n\n| Worker Pool | Channel Size | Pool Size | Exchange |\n|-------------|--------------|-----------|----------|\n| Event Worker | 1000 | Configurable | grpc_event.tx |\n| Branch Worker | 1000 | Configurable | branch_event.tx |\n| WebEngage Worker | 1000 | Configurable | webengage_event.dx |\n| Vidooly Worker | 1000 | Configurable | vidooly_event.dx |\n| Graphy Worker | 1000 | Configurable | graphy_event.tx |\n| Shopify Worker | 1000 | Configurable | shopify_event.dx |\n\n---\n\n## 5. RABBITMQ INTEGRATION\n\n### Connection Management\n\n\\`\\`\\`go\nvar (\n    singletonRabbit  *RabbitConnection\n    rabbitCloseError chan *amqp.Error\n    rabbitInit       sync.Once\n)\n\nfunc Rabbit(config config.Config) *RabbitConnection {\n    rabbitInit.Do(func() {\n        rabbitConnected := make(chan bool)\n        safego.GoNoCtx(func() {\n            rabbitConnector(config, rabbitConnected)\n        })\n        select {\n        case <-rabbitConnected:\n        case <-time.After(5 * time.Second):\n        }\n    })\n    return singletonRabbit\n}\n\\`\\`\\`\n\n### Consumer Configuration (26 Consumers in main.go)\n\n\\`\\`\\`go\n// Example: High-volume buffered consumer\ntraceLogChan := make(chan interface{}, 10000)\ntraceLogEventConsumerCfg := rabbit.RabbitConsumerConfig{\n    QueueName:            \"trace_log\",\n    Exchange:             \"identity.dx\",\n    RoutingKey:           \"trace_log\",\n    RetryOnError:         true,\n    ErrorExchange:        &errorExchange,\n    ErrorRoutingKey:      &errorRoutingKey,\n    ConsumerCount:        2,\n    BufferChan:           traceLogChan,\n    BufferedConsumerFunc: sinker.BufferTraceLogEvent,\n}\nrabbit.Rabbit(config).InitConsumer(traceLogEventConsumerCfg)\ngo sinker.TraceLogEventsSinker(traceLogChan)  // Batch processor\n\\`\\`\\`\n\n### Retry Logic Implementation\n\n\\`\\`\\`go\nfunc (rabbit *RabbitConnection) Consume(cfg RabbitConsumerConfig) error {\n    for msg := range msgs {\n        listenerResponse := consumerFunc(msg)\n\n        if listenerResponse {\n            msg.Ack(false)\n        } else if !retryOnError {\n            publishToErrorQueue(msg)\n            msg.Ack(false)\n        } else {\n            retryCount := getRetryCount(msg.Headers)\n            if retryCount >= 2 {\n                publishToErrorQueue(msg)\n                msg.Ack(false)\n            } else {\n                // Republish with incremented retry count\n                msg.Headers[\"x-retry-count\"] = retryCount + 1\n                republish(msg)\n                msg.Ack(false)\n            }\n        }\n    }\n}\n\\`\\`\\`\n\n### All 26 Consumer Queues\n\n| Queue | Workers | Purpose |\n|-------|---------|---------|\n| grpc_clickhouse_event_q | 2 | Main events → ClickHouse |\n| grpc_clickhouse_event_error_q | 2 | Error events → ClickHouse |\n| clickhouse_click_event_q | 2 | Click tracking |\n| app_init_event_q | 2 | App initialization |\n| launch_refer_event_q | 2 | Launch referrals |\n| create_user_account_q | 2 | User account creation |\n| event.account_profile_complete | 2 | Profile completion |\n| ab_assignments | 2 | A/B test assignments |\n| branch_event_q | 2 | Branch.io events |\n| graphy_event_q | 2 | Graphy events |\n| trace_log | 2 | Trace logs (buffered) |\n| affiliate_orders_event_q | 2 | Affiliate orders (buffered) |\n| bigboss_votes_log_q | 2 | BigBoss votes (buffered) |\n| post_log_events_q | 20 | Social post logs (buffered) |\n| sentiment_log_events_q | 2 | Sentiment analysis (buffered) |\n| post_activity_log_events_q | 2 | Post activity (buffered) |\n| profile_log_events_q | 2 | Profile logs (buffered) |\n| profile_relationship_log_events_q | 2 | Relationships (buffered) |\n| scrape_request_log_events_q | 2 | Scrape requests (buffered) |\n| order_log_events_q | 2 | Order logs (buffered) |\n| shopify_events_q | 2 | Shopify events (buffered) |\n| webengage_event_q | 3 | WebEngage → API |\n| webengage_ch_event_q | 5 | WebEngage → ClickHouse |\n| webengage_user_event_q | 5 | WebEngage user events |\n| post_log_events_q_bkp | 5 | Backup post logs |\n| activity_tracker_q | 2 | Partner activity (buffered) |\n\n---\n\n## 6. DATABASE LAYER\n\n### ClickHouse Connection (Multi-DB Support)\n\n\\`\\`\\`go\nvar (\n    singletonClickhouseMap map[string]*gorm.DB\n    clickhouseInit         sync.Once\n)\n\nfunc Clickhouse(config config.Config, dbName *string) *gorm.DB {\n    clickhouseInit.Do(func() {\n        connectToClickhouse(config)\n        go clickhouseConnectionCron(config)  // Reconnect every 1 second\n    })\n\n    if dbName != nil {\n        if db, ok := singletonClickhouseMap[*dbName]; ok {\n            return db\n        }\n    }\n    return singletonClickhouseMap[config.CLICKHOUSE_DB_NAME]\n}\n\nfunc connectToClickhouse(config config.Config) {\n    for _, dbName := range config.CLICKHOUSE_DB_NAMES {\n        dsn := \"tcp://\" + host + \":\" + port +\n               \"?database=\" + dbName +\n               \"&read_timeout=10&write_timeout=20\"\n\n        db, _ := gorm.Open(clickhouse.New(clickhouse.Config{\n            DSN: dsn,\n            DisableDatetimePrecision: true,\n        }), &gorm.Config{})\n\n        singletonClickhouseMap[dbName] = db\n    }\n}\n\\`\\`\\`\n\n### PostgreSQL Connection (Schema-Based)\n\n\\`\\`\\`go\nfunc PG(config config.Config) *gorm.DB {\n    pgInit.Do(func() {\n        connectToPG(config)\n        go pgConnectionCron(config)\n    })\n    return singletonPg\n}\n\nfunc connectToPG(config config.Config) {\n    singletonPg, _ = gorm.Open(\"postgres\",\n        \"host=\" + host + \" port=\" + port +\n        \" user=\" + user + \" dbname=\" + dbname +\n        \" password=\" + password)\n\n    singletonPg.DB().SetMaxIdleConns(10)\n    singletonPg.DB().SetMaxOpenConns(20)\n\n    // Schema-based table naming\n    gorm.DefaultTableNameHandler = func(db *gorm.DB, tableName string) string {\n        return config.POSTGRES_EVENT_SCHEMA + \".\" + tableName\n    }\n}\n\\`\\`\\`\n\n### ClickHouse Tables Created\n\n| Table | Purpose | Key Fields |\n|-------|---------|------------|\n| event | Main app events | event_id, event_name, event_params (JSONB) |\n| error_event | Failed event tracking | All event fields + error info |\n| click_event | Click tracking | 600+ lines of click-specific data |\n| branch_event | Deep linking | 49 fields for attribution |\n| webengage_event | Marketing events | userId, eventData (JSONB) |\n| trace_log | Request tracing | hostName, serviceName, timeTaken |\n| graphy_event | Graphy platform | Platform-specific events |\n| app_init_event | App startup | Device, session metrics |\n| launch_refer_event | Launch referrals | Attribution data |\n| ab_assignment | A/B test data | Variant assignments |\n| entity_metric | Generic metrics | Flexible metric storage |\n| post_log_event | Social posts | Post activity data |\n| sentiment_log | Sentiment analysis | Comment sentiment scores |\n| profile_log | Profile activity | Profile metrics over time |\n| profile_relationship_log | Relationships | Follower/following data |\n| order_log | E-commerce orders | Order transaction data |\n| shopify_event | Shopify events | Commerce event data |\n| affiliate_order | Affiliate tracking | Affiliate transaction data |\n\n---\n\n## 7. DATA MODELS\n\n### Core Event Model\n\n\\`\\`\\`go\ntype Event struct {\n    EventId         string    \\`gorm:\"column:event_id\"\\`\n    EventName       string    \\`gorm:\"column:event_name\"\\`\n    EventTimestamp  time.Time \\`gorm:\"column:event_timestamp\"\\`\n    InsertTimestamp time.Time \\`gorm:\"column:insert_timestamp\"\\`\n\n    // Header fields\n    SessionId       string    \\`gorm:\"column:session_id\"\\`\n    BbDeviceId      int64     \\`gorm:\"column:bb_device_id\"\\`\n    UserId          int64     \\`gorm:\"column:user_id\"\\`\n    DeviceId        string    \\`gorm:\"column:device_id\"\\`\n    ClientId        string    \\`gorm:\"column:client_id\"\\`\n    Channel         string    \\`gorm:\"column:channel\"\\`\n    Os              string    \\`gorm:\"column:os\"\\`\n    ClientType      string    \\`gorm:\"column:client_type\"\\`\n    AppLanguage     string    \\`gorm:\"column:app_language\"\\`\n    AppVersion      string    \\`gorm:\"column:app_version\"\\`\n\n    // UTM parameters\n    UtmReferrer     string    \\`gorm:\"column:utm_referrer\"\\`\n    UtmPlatform     string    \\`gorm:\"column:utm_platform\"\\`\n    UtmSource       string    \\`gorm:\"column:utm_source\"\\`\n    UtmMedium       string    \\`gorm:\"column:utm_medium\"\\`\n    UtmCampaign     string    \\`gorm:\"column:utm_campaign\"\\`\n\n    // Dynamic event data\n    EventParams     JSONB     \\`gorm:\"column:event_params;type:String\"\\`\n}\n\\`\\`\\`\n\n### Branch Event Model (49 Fields)\n\n\\`\\`\\`go\ntype BranchEvent struct {\n    // Core identifiers\n    Id                    string \\`gorm:\"column:id\"\\`\n    Name                  string \\`gorm:\"column:name\"\\`\n    Timestamp             int64  \\`gorm:\"column:timestamp\"\\`\n\n    // Attribution data\n    Attributed            bool   \\`gorm:\"column:attributed\"\\`\n    DeepLinked            bool   \\`gorm:\"column:deep_linked\"\\`\n    ExistingUser          bool   \\`gorm:\"column:existing_user\"\\`\n    HasClicked            bool   \\`gorm:\"column:has_clicked\"\\`\n    HasApp                bool   \\`gorm:\"column:has_app\"\\`\n\n    // Timing metrics\n    SecondsFromInstall    int64  \\`gorm:\"column:seconds_from_install\"\\`\n    SecondsFromLastOpen   int64  \\`gorm:\"column:seconds_from_last_attributed_touch_timestamp\"\\`\n\n    // Geo data\n    GeoCountryEn          string \\`gorm:\"column:geo_country_en\"\\`\n    GeoRegionEn           string \\`gorm:\"column:geo_region_en\"\\`\n    GeoCityEn             string \\`gorm:\"column:geo_city_en\"\\`\n\n    // Campaign data\n    Campaign              string \\`gorm:\"column:campaign\"\\`\n    Channel               string \\`gorm:\"column:channel\"\\`\n    Feature               string \\`gorm:\"column:feature\"\\`\n    Tags                  string \\`gorm:\"column:tags\"\\`\n\n    // Complex nested data (JSONB)\n    EventData             JSONB  \\`gorm:\"column:event_data;type:String\"\\`\n    UserData              JSONB  \\`gorm:\"column:user_data;type:String\"\\`\n    LastAttributedTouchData JSONB \\`gorm:\"column:last_attributed_touch_data;type:String\"\\`\n\n    // ... 30 more fields for complete attribution tracking\n}\n\\`\\`\\`\n\n---\n\n## 8. SINKER IMPLEMENTATIONS\n\n### Main Event Sinker\n\n\\`\\`\\`go\nfunc SinkEventToClickhouse(delivery amqp.Delivery) bool {\n    grpcEvent := &bulbulgrpc.Events{}\n    if err := protojson.Unmarshal(delivery.Body, grpcEvent); err != nil {\n        return false\n    }\n\n    if grpcEvent.Events == nil || len(grpcEvent.Events) == 0 {\n        return false\n    }\n\n    events := []model.Event{}\n    for _, e := range grpcEvent.Events {\n        event, err := TransformToEventModel(grpcEvent, e)\n        if err != nil {\n            continue\n        }\n        events = append(events, event)\n    }\n\n    if len(events) > 0 {\n        result := clickhouse.Clickhouse(config.New(), nil).Create(events)\n        return result != nil && result.Error == nil\n    }\n    return false\n}\n\\`\\`\\`\n\n### Buffered Sinker Pattern (High-Volume)\n\n\\`\\`\\`go\n// Buffer function - adds to channel\nfunc BufferTraceLogEvent(delivery amqp.Delivery, c chan interface{}) bool {\n    var traceLog map[string]interface{}\n    if err := json.Unmarshal(delivery.Body, &traceLog); err == nil {\n        c <- traceLog\n        return true\n    }\n    return false\n}\n\n// Batch processor - runs in separate goroutine\nfunc TraceLogEventsSinker(c chan interface{}) {\n    ticker := time.NewTicker(5 * time.Second)\n    batch := []model.TraceLogEvent{}\n\n    for {\n        select {\n        case event := <-c:\n            traceLog := parseTraceLog(event)\n            batch = append(batch, traceLog)\n\n            if len(batch) >= 1000 {\n                flushBatch(batch)\n                batch = []model.TraceLogEvent{}\n            }\n\n        case <-ticker.C:\n            if len(batch) > 0 {\n                flushBatch(batch)\n                batch = []model.TraceLogEvent{}\n            }\n        }\n    }\n}\n\\`\\`\\`\n\n### All Sinker Functions\n\n| Sinker | Type | Destination |\n|--------|------|-------------|\n| SinkEventToClickhouse | Direct | ClickHouse event table |\n| SinkErrorEventToClickhouse | Direct | ClickHouse error_event |\n| SinkClickEventToClickhouse | Direct | ClickHouse click_event |\n| SinkAppInitEvent | Direct | ClickHouse app_init_event |\n| SinkLaunchReferEvent | Direct | ClickHouse + PostgreSQL |\n| SinkBranchEvent | Direct | ClickHouse branch_event |\n| SinkGraphyEvent | Direct | ClickHouse graphy_event |\n| SinkWebengageToClickhouse | Direct | ClickHouse webengage_event |\n| SinkWebengageToAPI | HTTP | WebEngage REST API |\n| SinkShopifyEvent | Buffered | ClickHouse shopify_event |\n| TraceLogEventsSinker | Buffered | ClickHouse trace_log |\n| AffiliateOrdersSinker | Buffered | ClickHouse affiliate_order |\n| BigBossVotesSinker | Buffered | ClickHouse bigboss_votes |\n| PostLogEventsSinker | Buffered | ClickHouse post_log_event |\n| SentimentLogEventsSinker | Buffered | ClickHouse sentiment_log |\n| ProfileLogEventsSinker | Buffered | ClickHouse profile_log |\n| ScrapeLogEventsSinker | Buffered | ClickHouse scrape_request_log |\n| OrderLogEventsSinker | Buffered | ClickHouse order_log |\n| ActivityTrackerSinker | Buffered | ClickHouse activity_tracker |\n\n---\n\n## 9. ERROR HANDLING & RESILIENCE\n\n### Safe Goroutine Execution\n\n\\`\\`\\`go\nfunc GoNoCtx(f func()) {\n    go func() {\n        defer func() {\n            if panicMessage := recover(); panicMessage != nil {\n                stack := debug.Stack()\n                log.Printf(\"RECOVERED FROM PANIC: %v\\\\nSTACK: %s\",\n                    panicMessage, stack)\n            }\n        }()\n        f()\n    }()\n}\n\\`\\`\\`\n\n### Connection Auto-Recovery\n\n\\`\\`\\`go\n// ClickHouse reconnect cron\nfunc clickhouseConnectionCron(config config.Config) {\n    ticker := time.NewTicker(1 * time.Second)\n    for range ticker.C {\n        for dbName, db := range singletonClickhouseMap {\n            if db == nil || db.Error != nil {\n                reconnect(dbName)\n            }\n        }\n    }\n}\n\n// RabbitMQ reconnect on close\nfunc rabbitConnector(config config.Config, connected chan bool) {\n    for {\n        connectRabbit(config)\n        connected <- true\n\n        // Wait for close notification\n        <-rabbitCloseError\n\n        // Reconnect with backoff\n        time.Sleep(2 * time.Second)\n    }\n}\n\\`\\`\\`\n\n### Error Flow\n\n\\`\\`\\`\nEvent Processing\n    ↓\nSuccess? → ACK & Done\n    ↓ (Failure)\nRetry 1 → Republish with x-retry-count=1\n    ↓ (Failure)\nRetry 2 → Republish with x-retry-count=2\n    ↓ (Failure)\nRoute to Error Exchange → Dead Letter Queue\n\\`\\`\\`\n\n---\n\n## 10. CI/CD PIPELINE\n\n### GitLab CI Configuration\n\n\\`\\`\\`yaml\nstages:\n  - build\n  - deploy\n  - publish\n\nbuild:\n  stage: build\n  variables:\n    GOOS: linux\n    GOARCH: amd64\n  script:\n    - protoc --go_out=. --go-grpc_out=. proto/eventservice.proto\n    - env GOOS=$GOOS GOARCH=$GOARCH go build\n  artifacts:\n    paths:\n      - event-grpc\n      - .env*\n      - scripts/start.sh\n    expire_in: 1 week\n\ndeploy_staging:\n  stage: deploy\n  variables:\n    ENV: STAGE\n    GOGC: 250\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - deployer-stage-search\n  only:\n    - master\n    - dev\n\ndeploy_production:\n  stage: deploy\n  when: manual\n  only:\n    - master\n\npublish_web:\n  stage: publish\n  script:\n    - protoc proto/eventservice.proto\n        --js_out=import_style=commonjs:./proto/web/src\n        --grpc-web_out=import_style=commonjs+dts,mode=grpcwebtext:./proto/web/src/\n    - npm publish --registry http://artifactory.bulbul.tv:4873/\n  when: manual\n\npublish_java:\n  stage: publish\n  script:\n    - mvn clean install -U && mvn deploy\n  when: manual\n\\`\\`\\`\n\n### Graceful Deployment Script\n\n\\`\\`\\`bash\n#!/bin/bash\nulimit -n 100000\n\nPID=$(ps aux | grep event-grpc | grep -v grep | awk '{print $2}')\n\nif [ ! -z \"$PID\" ]; then\n    # Remove from load balancer\n    curl -vXPUT http://localhost:$GIN_PORT/heartbeat/?beat=false\n    sleep 15\n\n    # Kill existing process\n    kill -9 $PID\nfi\n\nsleep 10\n\n# Start new process\nENV=$CI_ENVIRONMENT_NAME ./event-grpc >> \"logs/out.log\" 2>&1 &\n\n# Symlink for log access\nln -s $(pwd)/logs /bulbul/services/event-grpc/logs\n\n# Wait for startup\nsleep 20\n\n# Add back to load balancer\ncurl -vXPUT http://localhost:$GIN_PORT/heartbeat/?beat=true\n\\`\\`\\`\n\n---\n\n## 11. CONFIGURATION MANAGEMENT\n\n### Environment-Based Config (40+ Fields)\n\n\\`\\`\\`go\ntype Config struct {\n    // Server\n    Port    string  // 8017 (gRPC)\n    GinPort string  // 8019 (HTTP)\n    Env     string  // PRODUCTION, STAGE, LOCAL\n\n    // External APIs\n    IDENTITY_URL      string\n    SOCIAL_STREAM_URL string\n    WEBENGAGE_API_KEY string\n    WEBENGAGE_URL     string\n    WEBENGAGE_USER_URL string\n    WEBENGAGE_BRAND_API_KEY string\n    WEBENGAGE_BRAND_URL string\n\n    // OAuth Client IDs\n    CUSTOMER_APP_CLIENT_ID string\n    HOST_APP_CLIENT_ID     string\n    GCC_HOST_APP_CLIENT_ID string\n    GCC_BRAND_CLIENT_ID    string\n\n    // Redis\n    RedisClusterAddresses  []string\n    REDIS_CLUSTER_PASSWORD string\n\n    // Worker Pools (6 configurations)\n    EVENT_WORKER_POOL_CONFIG          *EVENT_WORKER_POOL_CONFIG\n    BRANCH_EVENT_WORKER_POOL_CONFIG   *EVENT_WORKER_POOL_CONFIG\n    WEBENGAGE_EVENT_WORKER_POOL_CONFIG *EVENT_WORKER_POOL_CONFIG\n    VIDOOLY_EVENT_WORKER_POOL_CONFIG  *EVENT_WORKER_POOL_CONFIG\n    GRAPHY_EVENT_WORKER_POOL_CONFIG   *EVENT_WORKER_POOL_CONFIG\n    SHOPIFY_EVENT_WORKER_POOL_CONFIG  *EVENT_WORKER_POOL_CONFIG\n\n    // Databases\n    CLICKHOUSE_CONNECTION_CONFIG *CLICKHOUSE_CONNECTION_CONFIG\n    POSTGRES_CONNECTION_CONFIG   *POSTGRES_CONNECTION_CONFIG\n    RABBIT_CONNECTION_CONFIG     *RABBIT_CONNECTION_CONFIG\n\n    // Security\n    HMAC_SECRET []byte\n    LOG_LEVEL   int\n}\n\ntype EVENT_WORKER_POOL_CONFIG struct {\n    EVENT_WORKER_POOL_SIZE      int\n    EVENT_BUFFERED_CHANNEL_SIZE int\n}\n\ntype CLICKHOUSE_CONNECTION_CONFIG struct {\n    CLICKHOUSE_HOST     string\n    CLICKHOUSE_PORT     string\n    CLICKHOUSE_USER     string\n    CLICKHOUSE_DB_NAMES []string  // Comma-separated\n    CLICKHOUSE_PASSWORD string\n}\n\ntype RABBIT_CONNECTION_CONFIG struct {\n    RABBIT_USER      string\n    RABBIT_PASSWORD  string\n    RABBIT_HOST      string\n    RABBIT_PORT      string\n    RABBIT_HEARTBEAT int  // milliseconds\n    RABBIT_VHOST     string\n}\n\\`\\`\\`\n\n---\n\n## 12. OBSERVABILITY\n\n### Prometheus Metrics\n\n\\`\\`\\`go\nrouter.GET(\"/metrics\", gin.WrapH(promhttp.Handler()))\n\\`\\`\\`\n\n### Sentry Integration\n\n\\`\\`\\`go\nsentry.Init(sentry.ClientOptions{\n    Dsn:              \"http://xxx@172.31.14.149:9000/26\",\n    Environment:      config.Env,\n    AttachStacktrace: true,\n})\n\nrouter.Use(sentrygin.New(sentrygin.Options{Repanic: true}))\n\\`\\`\\`\n\n### Request Logging\n\n\\`\\`\\`go\nfunc RequestLogger(config config.Config) gin.HandlerFunc {\n    return func(c *gin.Context) {\n        start := time.Now()\n\n        // Capture request/response bodies\n        buf, _ := ioutil.ReadAll(c.Request.Body)\n        c.Request.Body = ioutil.NopCloser(bytes.NewBuffer(buf))\n\n        c.Next()\n\n        // Log with configurable verbosity\n        if gc.Config.LOG_LEVEL < 3 {\n            gc.Logger.Error().Msg(fmt.Sprintf(\n                \"%s - %s - [%s] \\\\\"%s %s\\\\\" %d %s\\\\n%s\",\n                c.Request.Header.Get(header.RequestID),\n                c.ClientIP(),\n                time.Now().Format(time.RFC1123),\n                c.Request.Method, path,\n                c.Writer.Status(),\n                time.Since(start),\n                body,  // Request body (verbose mode)\n            ))\n        }\n    }\n}\n\\`\\`\\`\n\n### Health Check Endpoint\n\n\\`\\`\\`go\n// GET /heartbeat/ - Returns health status\n// PUT /heartbeat/?beat=false - Remove from LB\n// PUT /heartbeat/?beat=true - Add to LB\n\nfunc (s *healthserver) Check(ctx context.Context, req *grpc_health_v1.HealthCheckRequest)\n    (*grpc_health_v1.HealthCheckResponse, error) {\n    if heartbeat.BEAT {\n        return &grpc_health_v1.HealthCheckResponse{\n            Status: grpc_health_v1.HealthCheckResponse_SERVING,\n        }, nil\n    }\n    return &grpc_health_v1.HealthCheckResponse{\n        Status: grpc_health_v1.HealthCheckResponse_NOT_SERVING,\n    }, nil\n}\n\\`\\`\\`\n\n---\n\n## 13. KEY METRICS & STATISTICS\n\n| Metric | Value |\n|--------|-------|\n| **Total Lines of Code** | 10,000+ |\n| **Go Source Files** | 90+ |\n| **Test Files** | 8 |\n| **Proto Definitions** | 688 lines |\n| **Event Types** | 60+ |\n| **Database Models** | 19 |\n| **Sinker Functions** | 20+ |\n| **RabbitMQ Consumers** | 26 configured |\n| **Total Consumer Workers** | 70+ |\n| **Worker Pools** | 6 |\n| **Buffered Channel Capacity** | 100K+ combined |\n| **Direct Dependencies** | 30+ |\n| **Exchanges** | 11 |\n| **ClickHouse Tables** | 18+ |\n\n---\n\n## 14. SKILLS DEMONSTRATED\n\n### Technical Skills\n\n| Skill | Evidence |\n|-------|----------|\n| **gRPC & Protocol Buffers** | 688-line proto with 60+ event types, Java/JS client publishing |\n| **Go Concurrency** | Worker pools, channels, sync.Once, goroutines, panic recovery |\n| **Message Queues** | RabbitMQ with 26 queues, retry logic, dead letter routing |\n| **Database Design** | ClickHouse (multi-DB), PostgreSQL (schema-based), connection pooling |\n| **Distributed Systems** | Event-driven architecture, fault tolerance, auto-recovery |\n| **API Design** | gRPC + REST hybrid, health checks, metrics endpoints |\n| **DevOps** | GitLab CI/CD, graceful deployments, zero-downtime, client library publishing |\n| **Observability** | Prometheus, Sentry, structured logging, request tracing |\n\n### Architecture Patterns\n\n1. **Worker Pool Pattern** - Configurable concurrency with buffered channels\n2. **Publish-Subscribe** - RabbitMQ exchange-based routing\n3. **Singleton Pattern** - Connection pools with lazy initialization\n4. **Circuit Breaker** - Auto-reconnect on database/queue failures\n5. **Buffered Sinker** - Batch processing for high-volume events\n6. **Gateway Pattern** - Unified entry point with middleware pipeline\n\n---\n\n## 15. INTERVIEW TALKING POINTS\n\n### System Design Questions\n\n**\"Design a real-time event processing system\"**\n- Built gRPC server handling high-throughput events\n- 6 Worker pools with configurable concurrency\n- RabbitMQ for reliable message delivery with retry logic (max 2 retries)\n- Multi-destination routing (ClickHouse, PostgreSQL, External APIs)\n- Buffered sinkers for high-volume batch processing\n\n**\"How do you ensure reliability in distributed systems?\"**\n- Auto-reconnect on connection failures (1-second cron)\n- Message retry with dead letter queues\n- Safe goroutine execution with panic recovery\n- Health checks for load balancer integration\n- Graceful deployment with zero-downtime\n\n**\"Explain your experience with message queues\"**\n- 26 RabbitMQ consumer configurations\n- 11 exchanges for event type distribution\n- Durable queues with prefetch QoS\n- Error queue pattern for debugging\n- Buffered consumption for high-volume events\n\n### Behavioral Questions\n\n**\"Tell me about a complex system you built\"**\n- Event-gRPC: 10K+ LOC, 60+ event types, 26 message queues\n- Handles real-time analytics for mobile/web applications\n- Multi-database strategy (ClickHouse for analytics, PostgreSQL for transactions)\n- Client library publishing (Java via Maven, JavaScript via NPM)\n\n**\"How do you handle scale?\"**\n- Configurable worker pool sizes\n- Buffered channels (100K+ capacity)\n- Batch inserts to databases\n- Horizontal scaling via stateless design\n- 70+ concurrent consumer workers\n\n---\n\n*Generated through comprehensive source code analysis of the event-grpc project.*\n"
  },
  {
    "id": "ANALYSIS_fake_follower",
    "title": "Fake Follower Analysis",
    "category": "analysis",
    "badge": null,
    "content": "# ULTRA-DEEP ANALYSIS: FAKE FOLLOWER DETECTION SYSTEM\n\n## PROJECT OVERVIEW\n\n| Attribute | Value |\n|-----------|-------|\n| **Project Name** | fake_follower_analysis |\n| **Purpose** | ML-powered fake follower detection using NLP, fuzzy matching, and multi-language transliteration |\n| **Architecture** | AWS Lambda + ECR serverless microservice with SQS/Kinesis data pipeline |\n| **Core Algorithm** | Ensemble model combining 5+ detection features |\n| **Total Lines of Code** | 955+ |\n| **Languages Supported** | 10 Indic scripts + English |\n| **Name Database** | 35,183 Indian baby names |\n\n---\n\n## 1. COMPLETE DIRECTORY STRUCTURE\n\n\\`\\`\\`\n/fake_follower_analysis/\n├── .git/                                    # Git repository\n├── Dockerfile                               # Lambda Docker image definition (23 lines)\n├── requirement.txt                          # Python dependencies (5 items)\n├── createDict.py                            # Hindi vowel/consonant mapping generator (91 lines)\n├── fake.py                                  # Core ML detection algorithm (385 lines, 19KB)\n├── pull.py                                  # Kinesis stream data retrieval (131 lines)\n├── push.py                                  # Data pipeline - ClickHouse→S3→SQS (154 lines)\n├── push1.py                                 # Single record test for Kinesis (41 lines)\n├── push_old.py                              # Legacy pipeline version (153 lines)\n│\n└── lambda_ecr_files/                        # ECR deployment package\n    ├── baby_names_.csv                      # 35,183 Indian baby names database\n    ├── svar.csv                             # 24 Hindi vowel transliteration mappings\n    ├── vyanjan.csv                          # 42 Hindi consonant transliteration mappings\n    ├── Dockerfile                           # Duplicate Docker config\n    ├── requirement.txt                      # Duplicate dependencies\n    ├── fake.py                              # Duplicate core algorithm\n    │\n    ├── indic-trans-master/                  # Indic script transliteration library\n    │   ├── indictrans/\n    │   │   ├── __init__.py                  # Package exports (Transliterator, UrduNormalizer, WX)\n    │   │   ├── transliterator.py            # Main Transliterator class\n    │   │   ├── base.py                      # BaseTransliterator with HMM models\n    │   │   ├── script_transliterate.py      # Language-specific transliterators\n    │   │   │\n    │   │   ├── _decode/                     # ML decoding algorithms\n    │   │   │   ├── viterbi.pyx              # Viterbi algorithm (Cython)\n    │   │   │   └── beamsearch.pyx           # Beamsearch decoder (Cython)\n    │   │   │\n    │   │   ├── _utils/                      # Utility functions\n    │   │   │   ├── wx_enc.py                # WX encoding converter\n    │   │   │   ├── one_hot_enc.py           # OneHotEncoder for features\n    │   │   │   └── urdu_normalizer.py       # Urdu script normalizer\n    │   │   │\n    │   │   ├── mappings/                    # Character mapping tables\n    │   │   │\n    │   │   └── models/                      # Pre-trained HMM models (10 languages)\n    │   │       ├── hin-eng/                 # Hindi → English\n    │   │       │   ├── coef_.npy            # HMM coefficient matrix\n    │   │       │   ├── classes.npy          # Output character mapping\n    │   │       │   ├── intercept_init_.npy  # Initial state probabilities\n    │   │       │   ├── intercept_trans_.npy # Transition probabilities\n    │   │       │   ├── intercept_final_.npy # Final state probabilities\n    │   │       │   └── sparse.vec           # Feature vocabulary\n    │   │       ├── ben-eng/                 # Bengali → English\n    │   │       ├── guj-eng/                 # Gujarati → English\n    │   │       ├── kan-eng/                 # Kannada → English\n    │   │       ├── mal-eng/                 # Malayalam → English\n    │   │       ├── ori-eng/                 # Odia → English\n    │   │       ├── pan-eng/                 # Punjabi → English\n    │   │       ├── tam-eng/                 # Tamil → English\n    │   │       ├── tel-eng/                 # Telugu → English\n    │   │       └── urd-eng/                 # Urdu → English\n    │   │\n    │   ├── setup.py                         # Package installation\n    │   ├── setup.cfg                        # Build configuration\n    │   ├── README.rst                       # Documentation\n    │   └── tests/                           # Unit tests\n    │\n    └── new/                                 # Mirrored structure for Docker build\n\\`\\`\\`\n\n---\n\n## 2. TECHNOLOGY STACK\n\n### Core Dependencies (requirement.txt)\n| Library | Version | Purpose |\n|---------|---------|---------|\n| **boto3** | 1.28.57 | AWS SDK (Lambda, SQS, Kinesis, S3) |\n| **pandas** | 2.1.1 | Data manipulation & CSV processing |\n| **numpy** | 1.26.0 | Numerical computing |\n| **rapidfuzz** | 3.3.1 | High-performance fuzzy string matching |\n| **unidecode** | latest | Unicode → ASCII normalization |\n| **indictrans** | custom | Multi-language Indic transliteration (ML-based) |\n| **clickhouse_connect** | implicit | ClickHouse database client |\n| **ijson** | implicit | Streaming JSON parsing |\n\n### AWS Services Architecture\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────┐\n│                     AWS INFRASTRUCTURE                          │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐      │\n│  │   AWS S3     │    │   AWS SQS    │    │ AWS Kinesis  │      │\n│  │ gcc-social-  │ →  │ creator_     │ →  │ creator_out  │      │\n│  │ data bucket  │    │ follower_in  │    │   stream     │      │\n│  └──────────────┘    └──────────────┘    └──────────────┘      │\n│         ↓                   ↓                   ↑               │\n│  ┌──────────────────────────────────────────────────────┐      │\n│  │            AWS Lambda (ECR Container)                 │      │\n│  │  ┌────────────────────────────────────────────────┐  │      │\n│  │  │               fake.handler()                    │  │      │\n│  │  │  - ML-based fake detection                      │  │      │\n│  │  │  - 10 Indic language transliteration           │  │      │\n│  │  │  - 35,183 name database lookup                 │  │      │\n│  │  └────────────────────────────────────────────────┘  │      │\n│  └──────────────────────────────────────────────────────┘      │\n│                                                                 │\n│  ┌──────────────┐                                              │\n│  │   AWS ECR    │  Docker container registry                   │\n│  │  Python 3.10 │  with pre-trained ML models                  │\n│  └──────────────┘                                              │\n│                                                                 │\n└─────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### Container Configuration (Dockerfile)\n\\`\\`\\`dockerfile\nFROM public.ecr.aws/lambda/python:3.10\n\n# System dependencies for indictrans compilation\nRUN yum install -y gcc-c++ pkgconfig poppler-cpp-devel\n\n# Install Python dependencies\nCOPY requirement.txt ./\nCOPY indic-trans-master ./\nRUN pip install -r requirements.txt\nRUN pip install .  # Installs indictrans from setup.py\n\n# Copy ML models and mappings to site-packages\nRUN cp -r indictrans/models /var/lang/lib/python3.10/site-packages/indictrans/\nRUN cp -r indictrans/mappings /var/lang/lib/python3.10/site-packages/indictrans/\n\n# Copy Hindi transliteration mappings\nCOPY svar.csv ./      # 24 vowel mappings\nCOPY vyanjan.csv ./   # 42 consonant mappings\n\n# Final dependency installation\nRUN pip install -r requirement.txt && pip install --upgrade numpy\n\n# Copy application code and data\nCOPY fake.py ./\nCOPY baby_names_.csv ./baby_names.csv\n\n# Cleanup\nRUN rm -r indictrans\n\n# Lambda entry point\nCMD [ \"fake.handler\" ]\n\\`\\`\\`\n\n---\n\n## 3. CORE ML ALGORITHM - COMPLETE BREAKDOWN\n\n### Detection Pipeline Flow\n\\`\\`\\`\nINPUT: {follower_handle, follower_full_name}\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 1: SYMBOL CONVERSION                                     │\n│ symbol_name_convert() - 13 Unicode symbol variants → ASCII    │\n│ Example: \"𝓐𝓵𝓲𝓬𝓮\" → \"Alice\"                                   │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 2: LANGUAGE DETECTION                                    │\n│ check_lang_other_than_indic() - Regex for non-Indic scripts   │\n│ Pattern: r'[Α-Ωα-ωԱ-Ֆა-ჰ一-鿿가-힣]+'                         │\n│ Detects: Greek, Armenian, Georgian, Chinese, Korean           │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 3: INDIC SCRIPT TRANSLITERATION                          │\n│ detect_language() + Transliterator()                          │\n│ Converts: \"राहुल\" → \"Rahul\" (Hindi → English)                 │\n│ Uses HMM-based ML models for 10 languages                     │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 4: UNICODE DECODING                                      │\n│ uni_decode() - unidecode(name, errors='preserve')             │\n│ Final ASCII normalization                                     │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 5: HANDLE CLEANING                                       │\n│ clean_handle() - Multi-stage normalization:                   │\n│   [_\\\\-.] → space                                              │\n│   [^\\\\w\\\\s] → removed                                           │\n│   [\\\\d] → removed                                              │\n│   [^a-zA-Z\\\\s] → removed                                       │\n│   → lowercase + strip                                         │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 6: FEATURE EXTRACTION (5 Independent Features)           │\n│                                                               │\n│ Feature 1: fake_real_based_on_lang (0/1)                      │\n│ Feature 2: number_more_than_4_handle (0/1)                    │\n│ Feature 3: chhitij_logic (0/1/2)                              │\n│ Feature 4: similarity_score (0-100)                           │\n│ Feature 5: indian_name_score (0-100)                          │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\n┌───────────────────────────────────────────────────────────────┐\n│ STEP 7: ENSEMBLE SCORING                                      │\n│ process1() → Binary classification (0/1/2)                    │\n│ final() → Weighted score (0.0 / 0.33 / 1.0)                   │\n└───────────────────────────────────────────────────────────────┘\n                    ↓\nOUTPUT: 19-field response with all features + final score\n\\`\\`\\`\n\n### Feature 1: Non-Indic Language Detection\n\\`\\`\\`python\ndef check_lang_other_than_indic(symbolic_name):\n    \"\"\"\n    Detects non-Indic scripts that indicate bot/fake accounts\n\n    Regex: r'[Α-Ωα-ωԱ-Ֆა-ჰ一-鿿가-힣]+'\n\n    Detects:\n    - Greek:    Α-Ω (uppercase), α-ω (lowercase)\n    - Armenian: Ա-Ֆ\n    - Georgian: ა-ჰ\n    - Chinese:  一-鿿 (CJK Unified Ideographs)\n    - Korean:   가-힣 (Hangul Syllables)\n\n    Returns: 1 (FAKE) if non-Indic detected, 0 (REAL) otherwise\n\n    Rationale: Real Indian users rarely use foreign scripts in names\n    \"\"\"\n    pattern = r'[Α-Ωα-ωԱ-Ֆა-ჰ一-鿿가-힣]+'\n    if re.search(pattern, symbolic_name):\n        return 1  # FAKE\n    return 0  # REAL\n\\`\\`\\`\n\n### Feature 2: Numerical Digit Count\n\\`\\`\\`python\ndef count_numerical_digits(text):\n    \"\"\"Count digits in handle\"\"\"\n    return sum(c.isdigit() for c in text)\n\ndef fake_real_more_than_4_digit(number):\n    \"\"\"\n    Threshold: 4 digits\n\n    Examples:\n    - \"rahul_27\" → 2 digits → REAL (0)\n    - \"rahul_12345\" → 5 digits → FAKE (1)\n    - \"user_999999\" → 6 digits → FAKE (1)\n\n    Rationale: Real users rarely add >4 random digits to handles\n    \"\"\"\n    return 1 if number > 4 else 0\n\\`\\`\\`\n\n### Feature 3: Handle-Name Special Character Logic\n\\`\\`\\`python\ndef process(follower_handle, cleaned_handle, cleaned_name):\n    \"\"\"\n    Analyzes correlation between special characters and name matching\n\n    SPECIAL_CHARS = ('_', '-', '.')\n\n    Decision Tree:\n    ├── Has special chars?\n    │   ├── YES → Single word name?\n    │   │   ├── YES → Similarity > 80?\n    │   │   │   ├── YES → Return 0 (REAL)\n    │   │   │   └── NO  → Return 1 (FAKE)\n    │   │   └── NO (multi-word) → Return 0 (REAL)\n    │   └── NO → Return 2 (INCONCLUSIVE)\n\n    Rationale:\n    - Users with special chars typically include their real name\n    - Single-word names with special chars but poor match = likely fake\n    \"\"\"\n    SPECIAL_CHARS = ('_', '-', '.')\n\n    if any(char in follower_handle for char in SPECIAL_CHARS):\n        if not ' ' in cleaned_name:  # Single word name\n            if generate_similarity_score(cleaned_handle, cleaned_name) > 80:\n                return 0  # REAL\n            else:\n                return 1  # FAKE\n        else:\n            return 0  # Multi-word name = REAL\n    else:\n        return 2  # No special chars = INCONCLUSIVE\n\\`\\`\\`\n\n### Feature 4: Fuzzy Similarity Scoring\n\\`\\`\\`python\ndef generate_similarity_score(handle, name):\n    \"\"\"\n    RapidFuzz-based similarity with weighted ensemble\n\n    Algorithm:\n    1. Generate all permutations of name words (max 4 words = 24 permutations)\n    2. For each permutation, calculate 3 fuzzy metrics:\n       - partial_ratio: Substring matching (weight: 2x)\n       - token_sort_ratio: Order-invariant matching\n       - token_set_ratio: Subset matching with deduplication\n    3. Combine: (2×partial + sort + set) / 4\n    4. Return maximum score across all permutations\n\n    Range: 0-100 (higher = more similar)\n\n    Example:\n    - handle=\"john_doe\", name=\"John Doe\" → ~95\n    - handle=\"xyz123\", name=\"Rahul Kumar\" → ~15\n    \"\"\"\n    from itertools import permutations\n    from rapidfuzz import fuzz as fuzzz\n\n    name_parts = name.split()\n    if len(name_parts) <= 4:\n        name_permutations = [' '.join(p) for p in permutations(name_parts)]\n    else:\n        name_permutations = [name]\n\n    similarity_score = -1\n    for name_variant in name_permutations:\n        partial_ratio = fuzzz.partial_ratio(handle, name_variant)\n        token_sort_ratio = fuzzz.token_sort_ratio(handle, name_variant)\n        token_set_ratio = fuzzz.token_set_ratio(handle, name_variant)\n\n        score = (2 * partial_ratio + token_sort_ratio + token_set_ratio) / 4\n        similarity_score = max(similarity_score, score)\n\n    return similarity_score\n\ndef based_on_partial_ratio(similarity_score):\n    \"\"\"\n    Threshold: 90\n    Returns: 0 (REAL) if > 90, 1 (FAKE) otherwise\n    \"\"\"\n    return 0 if similarity_score > 90 else 1\n\\`\\`\\`\n\n### Feature 5: Indian Name Database Matching\n\\`\\`\\`python\ndef check_indian_names(name):\n    \"\"\"\n    Matches against 35,183 Indian baby names database\n\n    Algorithm:\n    1. Split name into first_name + optional last_name\n    2. For each part, fuzzy match against entire database\n    3. Use same weighted formula: (2×ratio + sort + set) / 4\n    4. Return maximum score found\n\n    Special handling:\n    - Name < 2 chars → Return 1 (FAKE indicator)\n    - Last name < 2 chars → Set to 1 (FAKE indicator)\n\n    Range: 0-100 (higher = more likely real Indian name)\n    \"\"\"\n    global namess  # 35,183 names loaded from baby_names_.csv\n\n    if len(name) < 2:\n        return 1  # Too short\n\n    name_parts = name.split()\n    first_name = name_parts[0]\n    last_name = name_parts[1] if len(name_parts) >= 2 else None\n\n    similarity_score = 0\n\n    # Match first name\n    for db_name in namess:\n        score = (2 * fuzzz.ratio(db_name, first_name) +\n                 fuzzz.token_sort_ratio(db_name, first_name) +\n                 fuzzz.token_set_ratio(db_name, first_name)) / 4\n        similarity_score = max(similarity_score, score)\n\n    # Match last name if present\n    if last_name and len(last_name) >= 2:\n        for db_name in namess:\n            score = (2 * fuzzz.ratio(db_name, last_name) +\n                     fuzzz.token_sort_ratio(db_name, last_name) +\n                     fuzzz.token_set_ratio(db_name, last_name)) / 4\n            similarity_score = max(similarity_score, score)\n\n    return similarity_score\n\\`\\`\\`\n\n### Ensemble Scoring Functions\n\\`\\`\\`python\ndef process1(fake_real_based_on_lang, number_more_than_4_handle, chhitij_logic):\n    \"\"\"\n    Binary feature combination classifier\n\n    Decision Logic:\n    ├── Non-Indic language? → 1 (FAKE)\n    ├── >4 digits in handle? → 1 (FAKE)\n    ├── Special char mismatch (chhitij=1)? → 1 (FAKE)\n    ├── No special chars (chhitij=2)? → 2 (INCONCLUSIVE)\n    └── Otherwise → 0 (REAL)\n    \"\"\"\n    if fake_real_based_on_lang:\n        return 1  # FAKE\n    if number_more_than_4_handle:\n        return 1  # FAKE\n    if chhitij_logic == 1:\n        return 1  # FAKE\n    elif chhitij_logic == 2:\n        return 2  # INCONCLUSIVE\n    return 0  # REAL\n\ndef final(fake_real_based_on_lang, similarity_score,\n          number_more_than_4_handle, chhitij_logic):\n    \"\"\"\n    Weighted final score (0.0 to 1.0)\n\n    Scoring Rules:\n    ├── Non-Indic language? → 1.0 (100% FAKE)\n    ├── Similarity 0-40? → 0.33 (33% confidence FAKE)\n    ├── >4 digits? → 1.0 (100% FAKE)\n    ├── Special char mismatch (chhitij=1)? → 1.0 (100% FAKE)\n    ├── No special chars (chhitij=2)? → 0.0 (REAL)\n    └── Otherwise → 0.0 (REAL)\n\n    Output Range:\n    - 0.0  = Definitely REAL\n    - 0.33 = Weak FAKE indicator\n    - 1.0  = Definitely FAKE\n    \"\"\"\n    if fake_real_based_on_lang:\n        return 1.0  # 100% FAKE\n\n    if 0 < similarity_score <= 40:\n        return 0.33  # Weak FAKE signal\n\n    if number_more_than_4_handle:\n        return 1.0  # 100% FAKE\n\n    if chhitij_logic == 1:\n        return 1.0  # 100% FAKE\n    elif chhitij_logic == 2:\n        return 0.0  # REAL\n\n    return 0.0  # Default: REAL\n\\`\\`\\`\n\n---\n\n## 4. NLP & TRANSLITERATION SYSTEM\n\n### Supported Languages (10 Indic Scripts + Derivatives)\n| Language | Code | Script | Character Range | ML Model |\n|----------|------|--------|-----------------|----------|\n| Hindi | hin | Devanagari | 77 chars | hin-eng/ |\n| Bengali | ben | Bengali | 65 chars | ben-eng/ |\n| Gujarati | guj | Gujarati | 82 chars | guj-eng/ |\n| Kannada | kan | Kannada | 65 chars | kan-eng/ |\n| Malayalam | mal | Malayalam | 43 chars | mal-eng/ |\n| Odia | ori | Odia | 63 chars | ori-eng/ |\n| Punjabi | pan | Gurmukhi | 61 chars | pan-eng/ |\n| Tamil | tam | Tamil | 62 chars | tam-eng/ |\n| Telugu | tel | Telugu | 65 chars | tel-eng/ |\n| Urdu | urd | Perso-Arabic | 41 chars | urd-eng/ |\n| Marathi | mar | Devanagari | → hin-eng | (uses Hindi) |\n| Nepali | nep | Devanagari | → hin-eng | (uses Hindi) |\n| Konkani | kok | Devanagari | → hin-eng | (uses Hindi) |\n| Assamese | asm | Bengali | → ben-eng | (uses Bengali) |\n\n### Language Detection Algorithm\n\\`\\`\\`python\n# Character-to-language mapping\ndata = {\n    'hin': [अ, आ, इ, ई, उ, ऊ, ए, ऐ, ओ, औ, क, ख, ग, घ, ...],  # 77 chars\n    'pan': [ਅ, ਆ, ਇ, ਈ, ਉ, ਊ, ਏ, ਐ, ਓ, ਔ, ਕ, ਖ, ਗ, ਘ, ...],  # 61 chars\n    'guj': [અ, આ, ઇ, ઈ, ઉ, ઊ, એ, ઐ, ઓ, ઔ, ક, ખ, ગ, ઘ, ...],  # 82 chars\n    'ben': [অ, আ, ই, ঈ, উ, ঊ, এ, ঐ, ও, ঔ, ক, খ, গ, ঘ, ...],  # 65 chars\n    'urd': [ء, آ, أ, ؤ, إ, ئ, ا, ب, ت, ث, ج, ح, خ, د, ...],  # 41 chars\n    'tam': [அ, ஆ, இ, ஈ, உ, ஊ, எ, ஏ, ஐ, ஒ, ஓ, ஔ, க, ...],  # 62 chars\n    'mal': [അ, ആ, ഇ, ഈ, ഉ, ഊ, എ, ഏ, ഐ, ഒ, ഓ, ഔ, ക, ...],  # 43 chars\n    'kan': [ಅ, ಆ, ಇ, ಈ, ಉ, ಊ, ಎ, ಏ, ಐ, ಒ, ಓ, ಔ, ಕ, ...],  # 65 chars\n    'ori': [ଅ, ଆ, ଇ, ଈ, ଉ, ଊ, ଏ, ଐ, ଓ, ଔ, କ, ଖ, ଗ, ଘ, ...],  # 63 chars\n    'tel': [అ, ఆ, ఇ, ఈ, ఉ, ఊ, ఎ, ఏ, ఐ, ఒ, ఓ, ఔ, క, ...],  # 65 chars\n}\n\n# Build reverse lookup\nchar_to_lang = {}\nfor lang, chars in data.items():\n    for char in chars:\n        char_to_lang[char] = lang\n\ndef detect_language(word):\n    \"\"\"\n    Character-by-character language identification\n\n    Process:\n    1. For each char, lookup char_to_lang[char]\n    2. Get language code (hin, ben, etc.)\n    3. For Hindi: Use custom process_word() with svar/vyanjan CSVs\n    4. For others: Use Transliterator(source→eng)\n    5. Call trn.transform(word) for ML-based transliteration\n    \"\"\"\n\\`\\`\\`\n\n### Hindi-Specific Processing (svar.csv + vyanjan.csv)\n\\`\\`\\`python\n# svar.csv - 24 Hindi Vowel Mappings\nvowels = {\n    'ँ': 'n',   # Chandrabindu (nasal)\n    'ं': 'n',   # Anusvara\n    'ः': 'a',   # Visarga\n    'अ': 'a',   # A\n    'आ': 'aa',  # Aa\n    'इ': 'i',   # I\n    'ई': 'ee',  # Ii\n    'उ': 'u',   # U\n    'ऊ': 'oo',  # Uu\n    'ऋ': 'ri',  # Vocalic R\n    'ए': 'e',   # E\n    'ऐ': 'ai',  # Ai\n    'ओ': 'o',   # O\n    'औ': 'au',  # Au\n    'ा': 'a',   # Aa matra\n    'ि': 'i',   # I matra\n    'ी': 'ee',  # Ii matra\n    'ु': 'u',   # U matra\n    'ू': 'oo',  # Uu matra\n    'े': 'e',   # E matra\n    'ै': 'ai',  # Ai matra\n    'ो': 'o',   # O matra\n    'ौ': 'au',  # Au matra\n    '्': '',    # Halant (suppresses inherent vowel)\n}\n\n# vyanjan.csv - 42 Hindi Consonant Mappings\nconsonants = {\n    # Velar\n    'क': 'k',   'ख': 'kh',  'ग': 'g',   'घ': 'gh',  'ङ': 'ng',\n    # Palatal\n    'च': 'ch',  'छ': 'chh', 'ज': 'j',   'झ': 'jh',  'ञ': 'nj',\n    # Retroflex\n    'ट': 't',   'ठ': 'th',  'ड': 'd',   'ढ': 'dh',  'ण': 'n',\n    # Dental\n    'त': 't',   'थ': 'th',  'द': 'd',   'ध': 'dh',  'न': 'n',\n    # Labial\n    'प': 'p',   'फ': 'ph',  'ब': 'b',   'भ': 'bh',  'म': 'm',\n    # Semi-vowels\n    'य': 'y',   'र': 'r',   'ल': 'l',   'व': 'v',\n    # Sibilants\n    'श': 'sh',  'ष': 'sh',  'स': 's',\n    # Glottal\n    'ह': 'h',\n    # Complex\n    'क्ष': 'ksh', 'त्र': 'tr', 'ज्ञ': 'gy',\n    # Nukta variants\n    'क़': 'q',   'ख़': 'kh',  'ग़': 'gh',  'ज़': 'z',\n    'ड़': 'r',   'ढ़': 'rh',  'फ़': 'f',\n}\n\ndef process_word(word):\n    \"\"\"\n    Custom Hindi → English transliteration\n\n    Handles Devanagari diacritics (matra) combination:\n    1. Detect nukta (़) diacritics\n    2. Process consonant + matra combinations\n    3. Handle consonant clusters (halant sequences)\n    4. Return romanized form\n\n    Example: \"राहुल\" → \"raahul\"\n    \"\"\"\n\\`\\`\\`\n\n### ML-Based Transliteration (indictrans)\n\\`\\`\\`python\nfrom indictrans import Transliterator\n\nclass Transliterator:\n    \"\"\"\n    HMM-based sequence labeling for transliteration\n\n    Supports:\n    - Indic → English (ML models)\n    - English → Indic (ML models)\n    - Indic → Indic (Rule-based or ML)\n    - Urdu normalization\n\n    Model files per language pair:\n    - coef_.npy: HMM coefficient matrix\n    - classes.npy: Output character mapping\n    - intercept_init_.npy: Initial state probabilities\n    - intercept_trans_.npy: Transition probabilities\n    - intercept_final_.npy: Final state probabilities\n    - sparse.vec: Feature vocabulary\n    \"\"\"\n\n    def __init__(self, source, target, decode='viterbi',\n                 build_lookup=False, rb=True):\n        \"\"\"\n        Args:\n            source: Source language code (hin, ben, etc.)\n            target: Target language code (eng, hin, etc.)\n            decode: 'viterbi' (single best) or 'beamsearch' (top-k)\n            build_lookup: Cache repeated words\n            rb: Use rule-based for Indic-to-Indic\n        \"\"\"\n\n    def transform(self, text):\n        \"\"\"\n        ML Pipeline:\n        1. UTF-8 → WX notation (ISO 15919)\n        2. Feature extraction: n-gram context\n        3. HMM prediction: Linear classifier + decoder\n        4. WX → UTF-8 (target script)\n        \"\"\"\n\\`\\`\\`\n\n### Symbol Normalization (13 Unicode Variants)\n\\`\\`\\`python\ndef symbol_name_convert(name):\n    \"\"\"\n    Converts fancy Unicode text to standard ASCII\n\n    Supported variants (13 sets):\n    1. Circled Letters: 🅐🅑🅒🅓🅔... → ABCDE...\n    2. Mathematical Bold: 𝐀𝐁𝐂𝐃𝐄... → ABCDE...\n    3. Mathematical Italic: 𝐴𝐵𝐶𝐷𝐸... → ABCDE...\n    4. Mathematical Bold Italic: 𝑨𝑩𝑪𝑫𝑬... → ABCDE...\n    5. Mathematical Script: 𝒜𝒝𝒞𝒟𝒠... → ABCDE...\n    6. Mathematical Bold Script: 𝓐𝓑𝓒𝓓𝓔... → ABCDE...\n    7. Mathematical Fraktur: 𝔄𝔅ℭ𝔇𝔈... → ABCDE...\n    8. Mathematical Double-Struck: 𝔸𝔹ℂ𝔻𝔼... → ABCDE...\n    9. Mathematical Bold Fraktur: 𝕬𝕭𝕮𝕯𝕰... → ABCDE...\n    10. Mathematical Sans-Serif: 𝖠𝖡𝖢𝖣𝖤... → ABCDE...\n    11. Mathematical Sans-Serif Bold: 𝗔𝗕𝗖𝗗𝗘... → ABCDE...\n    12. Mathematical Monospace: 𝙰𝙱𝙲𝙳𝙴... → ABCDE...\n    13. Full-width: ＡＢＣＤＥ... → ABCDE...\n\n    Output: Standard ASCII A-Z, a-z, 0-9\n    \"\"\"\n\\`\\`\\`\n\n---\n\n## 5. AWS DATA PIPELINE ARCHITECTURE\n\n### Complete Data Flow\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────┐\n│                    DAILY BATCH PROCESSING PIPELINE                  │\n└─────────────────────────────────────────────────────────────────────┘\n\n1. DATA EXTRACTION (push.py)\n   ┌──────────────────────────────────────────────────────────────┐\n   │  ClickHouse Database (ec2-52-66-200-31.ap-south-1)          │\n   │  ├── dbt.mart_instagram_account (Creator metadata)          │\n   │  ├── dbt.stg_beat_profile_relationship_log (Historical)     │\n   │  └── _e.profile_relationship_log_events (Real-time)         │\n   └──────────────────────────────────────────────────────────────┘\n                              ↓ SQL Query\n   ┌──────────────────────────────────────────────────────────────┐\n   │  S3: gcc-social-data/temp/{date}_creator_followers.json     │\n   │  Format: JSONEachRow (one JSON object per line)             │\n   │  Fields: {handle, follower_handle, follower_full_name}      │\n   └──────────────────────────────────────────────────────────────┘\n\n2. MESSAGE DISTRIBUTION (push.py)\n                              ↓ Download to local\n   ┌──────────────────────────────────────────────────────────────┐\n   │  Batch Processing:                                          │\n   │  ├── Read 10,000 lines at a time                            │\n   │  ├── Divide into 8 buckets (round-robin)                    │\n   │  └── 8 parallel workers (multiprocessing.Pool)              │\n   └──────────────────────────────────────────────────────────────┘\n                              ↓ SQS batch send (10 msgs/call)\n   ┌──────────────────────────────────────────────────────────────┐\n   │  SQS Queue: creator_follower_in (eu-north-1)                │\n   │  ├── MaximumMessageSize: 256 KB                             │\n   │  ├── MessageRetentionPeriod: 4 days (345,600s)              │\n   │  └── VisibilityTimeout: 30 seconds                          │\n   └──────────────────────────────────────────────────────────────┘\n\n3. LAMBDA PROCESSING (fake.py)\n                              ↓ Event trigger\n   ┌──────────────────────────────────────────────────────────────┐\n   │  AWS Lambda (ECR Container)                                 │\n   │  ├── Handler: fake.handler(event, context)                  │\n   │  ├── Runtime: Python 3.10                                   │\n   │  ├── Processing: model(event) → 19 features                 │\n   │  └── Output: SQS send to output_queue                       │\n   └──────────────────────────────────────────────────────────────┘\n\n4. RESULTS STREAMING\n                              ↓ Kinesis put_record\n   ┌──────────────────────────────────────────────────────────────┐\n   │  Kinesis Stream: creator_out                                │\n   │  ├── Mode: ON_DEMAND (auto-scaling)                         │\n   │  ├── PartitionKey: follower_handle                          │\n   │  └── Region: ap-south-1                                     │\n   └──────────────────────────────────────────────────────────────┘\n\n5. OUTPUT AGGREGATION (pull.py)\n                              ↓ Multi-shard parallel read\n   ┌──────────────────────────────────────────────────────────────┐\n   │  Local Output File:                                         │\n   │  {date}_creator_followers_final_fake_analysis.json          │\n   │  ├── 19 columns per record                                  │\n   │  └── Used for downstream analytics                          │\n   └──────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### ClickHouse Query Structure\n\\`\\`\\`sql\n-- push.py SQL Query (Complex CTE structure)\nINSERT INTO FUNCTION s3(\n    'https://gcc-social-data.s3.ap-south-1.amazonaws.com/temp/{filename}.json',\n    'AKIAKEY...', 'SECRET...',\n    'JSONEachRow'\n)\nWITH\n    handles AS (\n        -- Load creator handles from S3 CSV\n        SELECT Names as handle\n        FROM s3('https://gcc-social-data.s3.ap-south-1.amazonaws.com/temp/creators_handles.csv',\n                'AKIAKEY...', 'SECRET...', 'CSV')\n    ),\n\n    profile_ids AS (\n        -- Map handles to Instagram profile IDs\n        SELECT profile_id\n        FROM dbt.mart_instagram_account mia\n        WHERE handle IN (SELECT handle FROM handles)\n    ),\n\n    follower_data AS (\n        -- Historical follower data (dbt staging table)\n        SELECT\n            log.target_profile_id,\n            JSONExtractString(source_dimensions, 'handle') as follower_handle,\n            JSONExtractString(source_dimensions, 'full_name') as follower_full_name\n        FROM dbt.stg_beat_profile_relationship_log log\n        WHERE target_profile_id IN (SELECT profile_id FROM profile_ids)\n          AND follower_handle IS NOT NULL AND follower_handle != ''\n          AND follower_full_name IS NOT NULL AND follower_full_name != ''\n    ),\n\n    follower_events_data AS (\n        -- Real-time follower events\n        SELECT\n            log.target_profile_id,\n            JSONExtractString(source_dimensions, 'handle') as follower_handle,\n            JSONExtractString(source_dimensions, 'full_name') as follower_full_name\n        FROM _e.profile_relationship_log_events log\n        WHERE target_profile_id IN (SELECT profile_id FROM profile_ids)\n          AND follower_handle IS NOT NULL AND follower_handle != ''\n    ),\n\n    data AS (\n        -- Combine historical and real-time\n        SELECT * FROM follower_data\n        UNION ALL\n        SELECT * FROM follower_events_data\n    )\n\nSELECT\n    mia.handle,\n    d.follower_handle,\n    d.follower_full_name\nFROM data d\nINNER JOIN dbt.mart_instagram_account mia\n    ON d.target_profile_id = mia.profile_id\nGROUP BY mia.handle, d.follower_handle, d.follower_full_name\n\\`\\`\\`\n\n### SQS Configuration\n\\`\\`\\`python\n# Queue creation (push.py)\nqueue = sqs.create_queue(\n    QueueName='creator_follower_in',\n    Attributes={\n        'MaximumMessageSize': '262144',      # 256 KB max\n        'MessageRetentionPeriod': '345600',  # 4 days\n        'VisibilityTimeout': '30'            # 30 seconds\n    }\n)\n\n# Batch message sending\ndef final(messages):\n    \"\"\"Send batch of 10 messages to SQS\"\"\"\n    response = queue.send_message_batch(\n        QueueUrl=queue_url,\n        Entries=messages  # Max 10 per API call\n    )\n\\`\\`\\`\n\n### Kinesis Configuration\n\\`\\`\\`python\n# Stream creation\nkinesis_client.create_stream(\n    StreamName='creator_out',\n    StreamModeDetails={'StreamMode': 'ON_DEMAND'}  # Auto-scaling\n)\n\n# Record sending (from Lambda)\nresponse = kinesis.put_record(\n    StreamName='creator_out',\n    Data=json.dumps(response_data),\n    PartitionKey='follower_handle',\n    StreamARN='arn:aws:kinesis:ap-south-1:495506833699:stream/creator_out'\n)\n\\`\\`\\`\n\n---\n\n## 6. OUTPUT SCHEMA (19 Fields)\n\n\\`\\`\\`python\nresponse = {\n    # Input Processing\n    1. \"symbolic_name\": str,\n       # Name after Unicode symbol normalization\n       # Example: \"𝓐𝓵𝓲𝓬𝓮\" → \"Alice\"\n\n    2. \"transliterated_follower_name\": str,\n       # Name transliterated from Indic to English\n       # Example: \"राहुल\" → \"Rahul\"\n\n    3. \"decoded_name\": str,\n       # Final normalized ASCII form\n       # Example: \"Ràhul\" → \"Rahul\"\n\n    4. \"cleaned_handle\": str,\n       # Handle normalized: special chars removed, lowercase\n       # Example: \"rahul_prasad27\" → \"rahul prasad\"\n\n    5. \"cleaned_name\": str,\n       # Decoded name normalized same way\n       # Example: \"Rahul Prasad\" → \"rahul prasad\"\n\n    # Feature Flags\n    6. \"fake_real_based_on_lang\": int (0/1),\n       # 1 = Non-Indic language detected (FAKE)\n       # 0 = Valid language\n\n    7. \"chhitij_logic\": int (0/1/2),\n       # 0 = Handle matches name well (REAL)\n       # 1 = Special chars but poor match (FAKE)\n       # 2 = No special chars (INCONCLUSIVE)\n\n    8. \"number_handle\": int,\n       # Count of digits in original handle\n       # Example: \"user123\" → 3\n\n    9. \"number_more_than_4_handle\": int (0/1),\n       # 1 = More than 4 digits (FAKE indicator)\n       # 0 = 4 or fewer (acceptable)\n\n    10. \"numeric_handle\": int (0/1),\n        # 1 = Purely numeric handle (FAKE indicator)\n        # 0 = Contains letters\n\n    # Similarity Scores\n    11. \"similarity_score\": float (0-100),\n        # Fuzzy match between handle and name\n        # Higher = more similar\n\n    12. \"fake_real_based_on_fuzzy_score_90\": int (0/1),\n        # 0 = Score > 90 (REAL)\n        # 1 = Score ≤ 90 (FAKE)\n\n    13. \"indian_name_score\": float (0-100),\n        # Match against 35,183 Indian names\n        # Higher = more likely real Indian name\n\n    14. \"score_80\": int (0/1),\n        # 1 = indian_name_score > 80 (REAL)\n        # 0 = Score ≤ 80 (FAKE indicator)\n\n    # Ensemble Outputs\n    15. \"process1_\": int (0/1/2),\n        # Binary feature combination\n        # 0 = Likely REAL\n        # 1 = Multiple FAKE indicators\n        # 2 = INCONCLUSIVE\n\n    16. \"final_\": float (0.0/0.33/1.0),\n        # Final fake probability\n        # 0.0 = Definitely REAL\n        # 0.33 = Weak FAKE signal\n        # 1.0 = Definitely FAKE\n}\n\\`\\`\\`\n\n---\n\n## 7. NAME DATABASE ANALYSIS\n\n### baby_names_.csv Statistics\n\\`\\`\\`\nTotal Records: 35,183 names + 1 header = 35,184 lines\nFile Size: ~287 KB\nFormat: Single column CSV\nHeader: \"Baby Names\"\n\nSample Names:\n- Chokku, Kulprem, Omal, Sparsh, Kullin\n- Nikil, Hara, Sanyakta, Sarajanya, Shrihan\n- (35,173 more names...)\n\nCharacteristics:\n- Predominantly Indian-origin names\n- Covers multiple regional languages\n- Phonetically normalized for matching\n- All converted to lowercase during comparison\n\nUsage:\nnamess = pd.read_csv('baby_names_.csv')['Baby Names'].str.lower()\n# Loaded once at module import for O(1) subsequent access\n\\`\\`\\`\n\n---\n\n## 8. CONFIGURATION & THRESHOLDS\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| **Fuzzy Score Threshold** | >90 | Handle-name similarity for \"REAL\" |\n| **Digit Count Threshold** | >4 | FAKE indicator in handle |\n| **Indian Name Threshold** | >80 | Name database match for \"REAL\" |\n| **Weak Similarity Range** | 0-40 | Assigns 0.33 confidence |\n| **Special Characters** | \\`_ - .\\` | Indicates intentional handle |\n| **Name Length Min** | 2 chars | Below = FAKE indicator |\n| **Permutation Limit** | 4 words | Max for permutation generation |\n| **SQS Batch Size** | 10,000 | Messages per ClickHouse export |\n| **SQS Queue Workers** | 8 | Parallel processing threads |\n| **SQS Message Max** | 256 KB | MaximumMessageSize |\n| **SQS Retention** | 4 days | MessageRetentionPeriod |\n| **SQS Visibility** | 30 sec | VisibilityTimeout |\n| **Kinesis Mode** | ON_DEMAND | Auto-scaling stream |\n| **Kinesis Shard Limit** | 10,000 | Records per get_records call |\n\n---\n\n## 9. PERFORMANCE ANALYSIS\n\n### Algorithm Complexity\n\\`\\`\\`\ngenerate_similarity_score():\n  Time: O(p × s) where p = permutations (max 24), s = string length\n  Practical: O(m × n) for string comparison\n\ncheck_indian_names():\n  Time: O(d × n) where d = 35,183 names, n = name tokens\n  Practical: O(35,183) per name = linear scan\n\nTotal per record:\n  Symbol conversion: 1-5ms\n  Language detection: 1-2ms\n  Transliteration: 5-10ms (ML inference)\n  Fuzzy scoring: 5-15ms\n  Indian name check: 10-50ms (full database scan)\n  ─────────────────────────────\n  TOTAL: 50-100ms per follower\n\\`\\`\\`\n\n### Throughput Estimates\n\\`\\`\\`\nSingle Lambda: 10-20 records/second\n8 parallel workers: 80-160 records/second\nDaily batch (100K followers): ~10-20 minutes\nMonthly scale (3M followers): ~5-10 hours\n\\`\\`\\`\n\n---\n\n## 10. WHAT MAKES A FOLLOWER \"FAKE\"\n\n### Strong FAKE Indicators (score = 1.0)\n1. **Non-Indic Script Characters**\n   - Greek, Armenian, Georgian, Chinese, Korean\n   - Bots often use foreign characters to evade filters\n\n2. **>4 Numerical Digits in Handle**\n   - Examples: user_12345, rahul_999999\n   - Real users rarely add that many random digits\n\n3. **Special Character Mismatch**\n   - Has \\`_\\`, \\`-\\`, \\`.\\` but handle doesn't match name\n   - Intentional separators should relate to real name\n\n### Weak FAKE Indicator (score = 0.33)\n4. **Low Handle-Name Similarity (0-40%)**\n   - Handle bears little resemblance to displayed name\n   - Could be nickname, but suspicious\n\n### REAL Indicators (score = 0.0)\n5. **High Handle-Name Similarity (>90%)**\n   - Handle clearly derived from real name\n\n6. **No Special Characters**\n   - Simple handles without separators = inconclusive but default REAL\n\n7. **High Indian Name Match (>80%)**\n   - Name matches known Indian name database\n\n---\n\n## 11. KEY METRICS SUMMARY\n\n| Metric | Value |\n|--------|-------|\n| **Total Lines of Code** | 955 |\n| **Core Model File** | 385 lines (fake.py) |\n| **Python Files** | 6 |\n| **Data Files** | 4 (CSV + ML models) |\n| **Container Dependencies** | 5 major |\n| **Supported Languages** | 10 Indic + 4 derivative scripts |\n| **Name Database** | 35,183 entries |\n| **AWS Services** | 5 (Lambda, SQS, Kinesis, S3, ECR) |\n| **Detection Features** | 5 independent heuristics |\n| **Output Fields** | 16 (per follower analysis) |\n| **Confidence Levels** | 3 (0.0, 0.33, 1.0) |\n| **Throughput** | 10-20 records/sec per Lambda |\n| **HMM Models** | 10 language pairs |\n| **Vowel Mappings** | 24 (Hindi) |\n| **Consonant Mappings** | 42 (Hindi) |\n\n---\n\n## 12. SKILLS DEMONSTRATED\n\n### Machine Learning & NLP\n- **Ensemble Model Design**: 5 independent feature combination\n- **HMM-based Transliteration**: Pre-trained models for 10 languages\n- **Fuzzy String Matching**: RapidFuzz with weighted scoring\n- **Feature Engineering**: Multi-stage text normalization pipeline\n- **Unicode Processing**: 13 symbol variants normalization\n\n### Cloud Architecture (AWS)\n- **Serverless Computing**: Lambda with ECR containerization\n- **Message Queuing**: SQS for batch job distribution\n- **Stream Processing**: Kinesis for real-time results\n- **Data Lake Integration**: S3 for intermediate storage\n- **Database Integration**: ClickHouse analytical queries\n\n### Software Engineering\n- **Python Multiprocessing**: 8-worker parallel batch processing\n- **Docker Containerization**: Lambda-optimized images\n- **Data Pipeline Design**: ETL with ClickHouse → S3 → SQS → Lambda → Kinesis\n- **Algorithm Optimization**: Permutation limiting, database caching\n\n### Domain Knowledge\n- **Linguistics**: 10 Indic scripts + character mapping systems\n- **Social Media Analytics**: Fake account detection patterns\n- **Indian Market Specialization**: Regional language support\n\n---\n\n## 13. INTERVIEW TALKING POINTS\n\n### 1. \"Tell me about an ML system you built\"\n- **Context**: Fake follower detection for Instagram analytics platform\n- **Approach**: Ensemble model with 5 independent features\n- **NLP Challenge**: 10 Indic script transliteration using HMM models\n- **Scale**: 35,183 name database, serverless Lambda processing\n- **Outcome**: Real-time fake detection with 3 confidence levels\n\n### 2. \"Describe your AWS experience\"\n- **Architecture**: S3 → SQS → Lambda → Kinesis pipeline\n- **Containerization**: ECR with Python 3.10 + ML models\n- **Scaling**: ON_DEMAND Kinesis, 8 parallel SQS workers\n- **Integration**: ClickHouse → AWS data extraction\n\n### 3. \"How do you handle multilingual text?\"\n- **Challenge**: Indian users write names in 10+ scripts\n- **Solution**: indictrans library with ML-based transliteration\n- **Custom Work**: Hindi vowel/consonant mappings (66 characters)\n- **Normalization**: 13 Unicode symbol variant handling\n\n### 4. \"Explain your approach to text similarity\"\n- **Algorithm**: RapidFuzz with weighted ensemble\n- **Metrics**: partial_ratio (2×), token_sort_ratio, token_set_ratio\n- **Optimization**: Permutation limiting (max 4 words = 24 variants)\n- **Database**: 35,183 Indian names for validation\n\n### 5. \"How do you design data pipelines?\"\n- **Extraction**: Complex ClickHouse CTEs with S3 export\n- **Distribution**: Batch processing with multiprocessing.Pool\n- **Processing**: Event-driven Lambda with SQS triggers\n- **Output**: Kinesis streaming for real-time consumption\n\n---\n\n## 14. SECURITY CONSIDERATIONS\n\n### Issues Identified\n1. **Hardcoded AWS Credentials** (3 separate key pairs in source code)\n2. **Hardcoded ClickHouse Password**\n3. **No Input Validation** on event data\n4. **No Error Handling** beyond basic try/except\n5. **Unencrypted Data Transfer** to SQS/Kinesis\n\n### Recommended Improvements\n- Use AWS Secrets Manager or environment variables\n- Add input sanitization for follower_handle and follower_full_name\n- Implement proper error handling with Sentry/CloudWatch\n- Enable SQS/Kinesis encryption at rest\n\n---\n\n*Analysis covers 955+ lines of code across 6 Python files, 4 data files, 10 pre-trained ML models, and complete AWS infrastructure integration.*\n"
  },
  {
    "id": "ANALYSIS_coffee",
    "title": "Coffee Analysis",
    "category": "analysis",
    "badge": null,
    "content": "# ULTRA-DEEP ANALYSIS: COFFEE PROJECT\n\n## PROJECT OVERVIEW\n\n| Attribute | Value |\n|-----------|-------|\n| **Project Name** | Coffee |\n| **Purpose** | Multi-tenant SaaS platform for influencer discovery, profile collections, and social media analytics |\n| **Architecture** | 4-Layer REST microservice with PostgreSQL + ClickHouse dual database strategy |\n| **Language** | Go 1.18 |\n| **Total Lines of Code** | ~8,500+ |\n| **Port** | 7179 (main), 9292 (debug/metrics) |\n\n---\n\n## 1. COMPLETE DIRECTORY STRUCTURE\n\n\\`\\`\\`\ncoffee/\n├── main.go                           # Application entry point\n├── go.mod / go.sum                    # Dependencies (30+ packages)\n├── schema.sql (918 lines)             # Database schema (27+ tables)\n├── .gitlab-ci.yml                     # CI/CD pipeline\n├── .env.*                             # Environment configs\n│\n├── server/\n│   ├── server.go                      # HTTP server setup with Chi\n│   └── middlewares/\n│       ├── context.go                 # Application context extraction\n│       ├── session.go                 # Transaction management\n│       ├── errorhandling.go           # Sentry error integration\n│       └── requestinterceptor.go      # Request/response capture\n│\n├── core/\n│   ├── rest/\n│   │   ├── api.go                     # Generic API handler interface\n│   │   ├── service.go                 # Generic Service layer\n│   │   ├── manager.go                 # Generic Manager layer\n│   │   └── dao.go                     # Generic DAO interface\n│   ├── persistence/\n│   │   ├── postgres/db.go             # PostgreSQL connection\n│   │   ├── clickhouse/db.go           # ClickHouse connection\n│   │   └── redis/redis.go             # Redis cluster client\n│   ├── appcontext/\n│   │   └── requestcontext.go          # Multi-tenant request context\n│   └── domain/\n│       └── domain.go                  # Core types (Entry, Entity, Response)\n│\n├── app/\n│   └── app.go                         # Service container & DI\n│\n├── routes/\n│   └── services.go                    # Route registration\n│\n├── discovery/                         # Profile discovery module\n│   ├── api/api.go\n│   ├── service/service.go\n│   ├── manager/\n│   │   ├── manager.go\n│   │   ├── searchmanager.go\n│   │   ├── timeseriesmanager.go\n│   │   ├── hashtagsmanager.go\n│   │   └── audiencemanager.go\n│   ├── dao/dao.go\n│   ├── domain/\n│   └── listeners/\n│\n├── profilecollection/                 # Profile collections\n├── postcollection/                    # Post collections\n├── leaderboard/                       # Rankings system\n├── collectionanalytics/               # Collection analytics\n├── genreinsights/                     # Genre insights\n├── keywordcollection/                 # Keyword collections\n├── campaignprofiles/                  # Campaign profiles\n├── collectiongroup/                   # Collection grouping\n├── partnerusage/                      # Partner usage tracking\n├── content/                           # Content management\n│\n├── listeners/\n│   └── listeners.go                   # Watermill event listeners\n├── publishers/\n│   └── publisher.go                   # Event publishing\n├── amqp/\n│   └── config.go                      # AMQP configuration\n│\n├── client/                            # External service clients\n│   ├── partner/                       # Partner service client\n│   ├── dam/                           # Digital asset management\n│   ├── winkl/                         # Legacy Winkl integration\n│   └── beat/                          # Beat service client\n│\n├── helpers/                           # Utility functions\n├── constants/constants.go             # Constants (languages, categories)\n├── config/config.go                   # Viper configuration\n├── logger/                            # Logrus setup\n├── sentry/                            # Sentry error tracking\n└── scripts/\n    └── start.sh                       # Deployment script\n\\`\\`\\`\n\n---\n\n## 2. FOUR-LAYER REST ARCHITECTURE\n\n### Architecture Diagram\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────────┐\n│                           HTTP REQUEST                                   │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                     MIDDLEWARE PIPELINE (10 Layers)                      │\n├─────────────────────────────────────────────────────────────────────────┤\n│  1. RequestInterceptor     │  Capture request/response for errors       │\n│  2. SlowQueryLogger        │  Log slow queries                          │\n│  3. SentryMiddleware       │  Error tracking integration                │\n│  4. ApplicationContext     │  Extract headers → RequestContext          │\n│  5. ServiceSession         │  Transaction + timeout management          │\n│  6. chi/Logger             │  HTTP request logging                      │\n│  7. chi/RequestID          │  Request ID generation                     │\n│  8. chi/RealIP             │  Client IP extraction                      │\n│  9. chi-prometheus         │  Metrics (300ms-30s buckets)               │\n│ 10. Recoverer              │  Panic recovery                            │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                         API LAYER (Handler)                              │\n├─────────────────────────────────────────────────────────────────────────┤\n│  Interface: ApiWrapper                                                   │\n│  Methods:                                                                │\n│    - AttachRoutes(r *chi.Mux)  // Register HTTP routes                  │\n│    - GetPrefix() string         // Service path prefix                  │\n│                                                                          │\n│  Responsibilities:                                                       │\n│    - Parse HTTP request (body, params, headers)                         │\n│    - Validate input                                                      │\n│    - Call Service layer                                                  │\n│    - Render JSON response                                                │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                        SERVICE LAYER                                     │\n├─────────────────────────────────────────────────────────────────────────┤\n│  Generic Type: Service[RES Response, EX Entry, EN Entity, I ID]         │\n│                                                                          │\n│  Methods:                                                                │\n│    - Create(ctx, entry) (*RES, error)                                   │\n│    - FindById(ctx, id) (*RES, error)                                    │\n│    - FindByIds(ctx, ids) ([]RES, error)                                 │\n│    - Update(ctx, id, entry) (*RES, error)                               │\n│    - Search(ctx, query, sort, page, size) ([]RES, int64, error)         │\n│    - InitializeSession(ctx) error                                        │\n│    - Close(ctx) error                                                    │\n│    - Rollback(ctx) error                                                 │\n│                                                                          │\n│  Responsibilities:                                                       │\n│    - Transaction management                                              │\n│    - Orchestrate Manager calls                                           │\n│    - Response transformation                                             │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                        MANAGER LAYER                                     │\n├─────────────────────────────────────────────────────────────────────────┤\n│  Generic Type: Manager[EX Entry, EN Entity, I ID]                        │\n│                                                                          │\n│  Methods:                                                                │\n│    - toEntity(*EX) (*EN, error)    // Entry → Entity                    │\n│    - toEntry(*EN) (*EX, error)     // Entity → Entry                    │\n│    - CRUD operations via DAO                                             │\n│                                                                          │\n│  Responsibilities:                                                       │\n│    - Business logic enforcement                                          │\n│    - Data transformation                                                 │\n│    - Validation rules                                                    │\n│    - Domain-specific operations                                          │\n└────────────────────────────────┬────────────────────────────────────────┘\n                                 │\n                                 ▼\n┌─────────────────────────────────────────────────────────────────────────┐\n│                          DAO LAYER                                       │\n├─────────────────────────────────────────────────────────────────────────┤\n│  Interface: DaoProvider[EN Entity, I ID]                                 │\n│                                                                          │\n│  Methods:                                                                │\n│    - FindById(ctx, id) (*EN, error)                                     │\n│    - FindByIds(ctx, ids) ([]EN, error)                                  │\n│    - Create(ctx, entity) (*EN, error)                                   │\n│    - Update(ctx, id, entity) (*EN, error)                               │\n│    - Search(ctx, query, sort, page, size) ([]EN, int64, error)          │\n│    - SearchJoins(ctx, query, sort, page, size, joins) ([]EN, int64)     │\n│    - GetSession(ctx) *gorm.DB                                           │\n│    - AddPredicateForSearchJoins(req, filter) *gorm.DB                   │\n│                                                                          │\n│  Implementations:                                                        │\n│    - PostgreSQL DAO (OLTP - transactional)                              │\n│    - ClickHouse DAO (OLAP - analytics)                                  │\n└─────────────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### Generic Type Implementation\n\n\\`\\`\\`go\n// core/rest/service.go\ntype Service[RES domain.Response, EX domain.Entry, EN domain.Entity, I domain.ID] struct {\n    Manager    *Manager[EX, EN, I]\n    Repository DaoProvider[EN, I]\n}\n\n// core/rest/manager.go\ntype Manager[EX domain.Entry, EN domain.Entity, I domain.ID] struct {\n    Dao       DaoProvider[EN, I]\n    ToEntity  func(*EX) (*EN, error)\n    ToEntry   func(*EN) (*EX, error)\n}\n\n// core/rest/dao.go\ntype DaoProvider[EN domain.Entity, I domain.ID] interface {\n    FindById(ctx context.Context, id I) (*EN, error)\n    Create(ctx context.Context, entity *EN) (*EN, error)\n    Update(ctx context.Context, id I, entity *EN) (*EN, error)\n    Search(ctx context.Context, query interface{}, sortBy, sortDir string, page, size int) ([]EN, int64, error)\n    GetSession(ctx context.Context) *gorm.DB\n}\n\\`\\`\\`\n\n---\n\n## 3. DATABASE SCHEMA (27+ Tables)\n\n### schema.sql Structure (918 lines)\n\n#### Social Profile Tables\n\n\\`\\`\\`sql\n-- Instagram profiles with 100+ columns\nCREATE TABLE instagram_account (\n    id SERIAL PRIMARY KEY,\n    profile_id VARCHAR(255) UNIQUE,\n    handle VARCHAR(255),\n    full_name VARCHAR(255),\n    bio TEXT,\n\n    -- Metrics\n    followers BIGINT,\n    following BIGINT,\n    engagement_rate DECIMAL(10,6),\n    avg_likes DECIMAL(10,2),\n    avg_comments DECIMAL(10,2),\n    avg_views DECIMAL(10,2),\n    total_posts INTEGER,\n\n    -- Audience Demographics (JSONB)\n    audience_gender JSONB,\n    audience_age JSONB,\n    audience_location JSONB,\n\n    -- Grades\n    engagement_rate_grade VARCHAR(10),\n    followers_grade VARCHAR(10),\n\n    -- Linked Accounts\n    youtube_account_id INTEGER REFERENCES youtube_account(id),\n    winkl_profile_id VARCHAR(255),\n    gcc_profile_id VARCHAR(255),\n\n    -- Timestamps\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- YouTube channels\nCREATE TABLE youtube_account (\n    id SERIAL PRIMARY KEY,\n    channel_id VARCHAR(255) UNIQUE,\n    title VARCHAR(255),\n    subscribers BIGINT,\n    total_views BIGINT,\n    avg_views DECIMAL(10,2),\n    plays BIGINT,\n    shorts_reach BIGINT,\n    -- Similar structure to Instagram\n);\n\\`\\`\\`\n\n#### Collection Tables\n\n\\`\\`\\`sql\n-- Profile collections\nCREATE TABLE profile_collection (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    partner_id BIGINT NOT NULL,\n    account_id BIGINT,\n    user_id BIGINT,\n    share_id VARCHAR(255) UNIQUE,\n    source VARCHAR(50),  -- SAAS, SAAS-AT, GCC_CAMPAIGN\n    is_public BOOLEAN DEFAULT FALSE,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Collection items (profiles in collection)\nCREATE TABLE profile_collection_item (\n    id SERIAL PRIMARY KEY,\n    collection_id INTEGER REFERENCES profile_collection(id),\n    profile_id INTEGER,\n    platform VARCHAR(50),\n    platform_profile_id VARCHAR(255),\n    added_by BIGINT,\n    custom_data JSONB,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Post collections\nCREATE TABLE post_collection (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255),\n    partner_id BIGINT NOT NULL,\n    account_id BIGINT,\n    ingestion_frequency VARCHAR(50),\n    show_in_report BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\\`\\`\\`\n\n#### Analytics Tables\n\n\\`\\`\\`sql\n-- Leaderboard rankings\nCREATE TABLE leaderboard (\n    id SERIAL PRIMARY KEY,\n    platform VARCHAR(50),\n    profile_id VARCHAR(255),\n    month DATE,\n\n    -- Rankings\n    followers_rank INTEGER,\n    followers_change_rank INTEGER,\n    engagement_rate_rank INTEGER,\n    views_rank INTEGER,\n    plays_rank INTEGER,\n\n    -- Category/Language rankings\n    followers_rank_by_cat INTEGER,\n    followers_rank_by_lang INTEGER,\n    followers_rank_by_cat_lang INTEGER,\n\n    category VARCHAR(100),\n    language VARCHAR(50),\n\n    UNIQUE(platform, profile_id, month)\n);\nCREATE INDEX idx_leaderboard_month ON leaderboard(month);\nCREATE INDEX idx_leaderboard_platform ON leaderboard(platform);\n\n-- Time-series profile data\nCREATE TABLE social_profile_time_series (\n    id SERIAL PRIMARY KEY,\n    platform VARCHAR(50),\n    platform_profile_id VARCHAR(255),\n    date DATE,\n    followers BIGINT,\n    following BIGINT,\n    engagement_rate DECIMAL(10,6),\n    posts_count INTEGER\n);\nCREATE INDEX idx_ts_platform_profile ON social_profile_time_series(platform, platform_profile_id, date);\n\n-- Post metrics summary\nCREATE TABLE collection_post_metrics_summary (\n    id SERIAL PRIMARY KEY,\n    collection_id INTEGER,\n    post_id VARCHAR(255),\n    platform VARCHAR(50),\n    likes BIGINT,\n    comments BIGINT,\n    views BIGINT,\n    shares BIGINT,\n    engagement_rate DECIMAL(10,6),\n    last_updated TIMESTAMP\n);\n\\`\\`\\`\n\n#### Business Tables\n\n\\`\\`\\`sql\n-- Partner usage tracking\nCREATE TABLE partner_usage (\n    id SERIAL PRIMARY KEY,\n    partner_id BIGINT NOT NULL,\n    module VARCHAR(100),\n    usage_count INTEGER DEFAULT 0,\n    limit_count INTEGER,\n    plan_type VARCHAR(50),\n    valid_from DATE,\n    valid_to DATE\n);\n\n-- Activity tracking\nCREATE TABLE activity_tracker (\n    id SERIAL PRIMARY KEY,\n    partner_id BIGINT,\n    account_id BIGINT,\n    user_id BIGINT,\n    activity_type VARCHAR(100),\n    entity_type VARCHAR(100),\n    entity_id VARCHAR(255),\n    metadata JSONB,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Profile page access tracking (for paid plans)\nCREATE TABLE partner_profile_page_track (\n    id SERIAL PRIMARY KEY,\n    partner_id BIGINT,\n    platform VARCHAR(50),\n    platform_profile_id VARCHAR(255),\n    access_count INTEGER DEFAULT 1,\n    first_accessed TIMESTAMP,\n    last_accessed TIMESTAMP,\n    UNIQUE(partner_id, platform, platform_profile_id)\n);\n\\`\\`\\`\n\n---\n\n## 4. MULTI-TENANT IMPLEMENTATION\n\n### Request Context Structure\n\n\\`\\`\\`go\n// core/appcontext/requestcontext.go\ntype RequestContext struct {\n    Ctx             context.Context\n    Mutex           sync.Mutex\n    Properties      map[string]interface{}\n\n    // Tenant Identification\n    PartnerId       *int64           // Primary tenant ID\n    AccountId       *int64           // Sub-tenant/account\n    UserId          *int64           // Individual user\n    UserName        *string\n\n    // Authentication\n    Authorization   string           // Bearer token\n    DeviceId        *string\n    IsLoggedIn      bool\n\n    // Plan & Access Control\n    IsPremiumMember bool\n    IsBasicMember   bool\n    PlanType        *constants.PlanType  // FREE, SAAS, PAID\n\n    // Database Sessions\n    Session         persistence.Session  // PostgreSQL transaction\n    CHSession       persistence.Session  // ClickHouse transaction\n}\n\\`\\`\\`\n\n### Header Extraction\n\n\\`\\`\\`go\n// server/middlewares/context.go\nfunc ApplicationContext(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        ctx := appcontext.NewRequestContext(r.Context())\n\n        // Extract tenant headers\n        if partnerId := r.Header.Get(\"x-bb-partner-id\"); partnerId != \"\" {\n            pid, _ := strconv.ParseInt(partnerId, 10, 64)\n            ctx.PartnerId = &pid\n        }\n\n        if accountId := r.Header.Get(\"x-bb-account-id\"); accountId != \"\" {\n            aid, _ := strconv.ParseInt(accountId, 10, 64)\n            ctx.AccountId = &aid\n        }\n\n        // Plan type\n        if planType := r.Header.Get(\"x-bb-plan-type\"); planType != \"\" {\n            pt := constants.PlanType(planType)\n            ctx.PlanType = &pt\n        }\n\n        // Authorization token parsing\n        if auth := r.Header.Get(\"Authorization\"); auth != \"\" {\n            // Format: userId~userName~isLoggedIn~accountId:password (base64)\n            ctx.ParseAuthorization(auth)\n        }\n\n        next.ServeHTTP(w, r.WithContext(ctx.ToContext()))\n    })\n}\n\\`\\`\\`\n\n### Plan-Based Feature Gating\n\n\\`\\`\\`go\n// constants/constants.go\nconst (\n    FreePlan PlanType = \"FREE\"    // Limited features\n    SaasPlan PlanType = \"SAAS\"    // Standard SaaS\n    PaidPlan PlanType = \"PAID\"    // Enterprise\n)\n\nvar PartnerLimitModules = map[string]bool{\n    \"DISCOVERY\":        true,\n    \"CAMPAIGN_REPORT\":  true,\n    \"PROFILE_PAGE\":     true,\n    \"ACCOUNT_TRACKING\": true,\n}\n\n// Security checks for free users\nfunc blockProfileDataForFreeUsers(ctx *RequestContext, profile *Profile) {\n    if ctx.PlanType != nil && *ctx.PlanType == FreePlan {\n        // Nullify premium fields\n        profile.Email = nil\n        profile.Phone = nil\n        profile.AudienceDetails = nil\n    }\n}\n\\`\\`\\`\n\n---\n\n## 5. MESSAGE QUEUE INTEGRATION (Watermill + AMQP)\n\n### AMQP Configuration\n\n\\`\\`\\`go\n// amqp/config.go\nfunc GetAMQPConfig() amqp.Config {\n    return amqp.Config{\n        Connection: amqp.ConnectionConfig{\n            AmqpURI: viper.GetString(\"AMQP_URI\"),\n        },\n        Marshaler: amqp.DefaultMarshaler{},\n        Exchange: amqp.ExchangeConfig{\n            GenerateName: func(topic string) string {\n                // Pattern: {exchange}___{queue}\n                parts := strings.Split(topic, \"___\")\n                return parts[0]\n            },\n            Type:    \"topic\",\n            Durable: true,\n        },\n        Queue: amqp.QueueConfig{\n            GenerateName: func(topic string) string {\n                parts := strings.Split(topic, \"___\")\n                if len(parts) > 1 {\n                    return parts[1]\n                }\n                return topic\n            },\n            Durable: true,\n        },\n        QueueBind: amqp.QueueBindConfig{\n            GenerateRoutingKey: func(topic string) string {\n                parts := strings.Split(topic, \"___\")\n                if len(parts) > 1 {\n                    return parts[1]\n                }\n                return topic\n            },\n        },\n    }\n}\n\\`\\`\\`\n\n### Event Listeners Setup\n\n\\`\\`\\`go\n// listeners/listeners.go\nfunc SetupListeners(container *app.ApplicationContainer) {\n    subscriber, _ := amqp.NewSubscriber(GetAMQPConfig(), logger)\n\n    router, _ := message.NewRouter(message.RouterConfig{}, logger)\n\n    // Middleware chain\n    router.AddMiddleware(\n        middleware.MessageApplicationContext,      // Extract context\n        middleware.TransactionSessionHandler,      // Transaction management\n        middleware.Retry{\n            MaxRetries:      3,\n            InitialInterval: 100 * time.Millisecond,\n        },\n        middleware.Recoverer,                      // Panic recovery\n    )\n\n    // Register handlers for each module\n    router.AddNoPublisherHandler(\n        \"discovery_handler\",\n        \"coffee.dx___discovery_events_q\",\n        subscriber,\n        container.Discovery.Listeners.HandleEvent,\n    )\n\n    router.AddNoPublisherHandler(\n        \"profile_collection_handler\",\n        \"coffee.dx___profile_collection_q\",\n        subscriber,\n        container.ProfileCollection.Listeners.HandleEvent,\n    )\n\n    // ... more handlers for each module\n\n    router.Run(context.Background())\n}\n\\`\\`\\`\n\n### Event Publishing\n\n\\`\\`\\`go\n// publishers/publisher.go\ntype Publisher struct {\n    pub *amqp.Publisher\n}\n\nfunc (p *Publisher) PublishMessage(jsonBytes []byte, topic string) error {\n    msg := message.NewMessage(watermill.NewUUID(), jsonBytes)\n    return p.pub.Publish(topic, msg)\n}\n\n// Usage in service layer\nfunc (s *Service) Create(ctx context.Context, entry *Entry) (*Response, error) {\n    // ... create logic ...\n\n    // After-commit callback for event publishing\n    session.PerformAfterCommit(ctx, func() {\n        eventData, _ := json.Marshal(entry)\n        publisher.PublishMessage(eventData, \"coffee.dx___profile_collection_q\")\n    })\n\n    return response, nil\n}\n\\`\\`\\`\n\n---\n\n## 6. DUAL DATABASE STRATEGY\n\n### PostgreSQL (OLTP - Transactional)\n\n\\`\\`\\`go\n// core/persistence/postgres/db.go\nvar (\n    db     *gorm.DB\n    pgOnce sync.Once\n)\n\nfunc GetDB() *gorm.DB {\n    pgOnce.Do(func() {\n        dsn := fmt.Sprintf(\n            \"host=%s user=%s password=%s dbname=%s port=%s sslmode=disable TimeZone=Asia/Kolkata\",\n            viper.GetString(\"PG_HOST\"),\n            viper.GetString(\"PG_USER\"),\n            viper.GetString(\"PG_PASS\"),\n            viper.GetString(\"PG_DB\"),\n            viper.GetString(\"PG_PORT\"),\n        )\n\n        db, _ = gorm.Open(postgres.Open(dsn), &gorm.Config{\n            Logger: logger.Default.LogMode(logger.Info),\n        })\n\n        sqlDB, _ := db.DB()\n        sqlDB.SetMaxIdleConns(viper.GetInt(\"PG_MAX_IDLE_CONN\"))  // 5\n        sqlDB.SetMaxOpenConns(viper.GetInt(\"PG_MAX_OPEN_CONN\"))  // 5\n    })\n    return db\n}\n\\`\\`\\`\n\n### ClickHouse (OLAP - Analytics)\n\n\\`\\`\\`go\n// core/persistence/clickhouse/db.go\nvar (\n    chDB     *gorm.DB\n    chOnce   sync.Once\n)\n\nfunc GetDB() *gorm.DB {\n    chOnce.Do(func() {\n        dsn := fmt.Sprintf(\n            \"tcp://%s:%s?database=%s&username=%s&password=%s&read_timeout=10&write_timeout=20\",\n            viper.GetString(\"CH_HOST\"),\n            viper.GetString(\"CH_PORT\"),\n            viper.GetString(\"CH_DB\"),\n            viper.GetString(\"CH_USER\"),\n            viper.GetString(\"CH_PASS\"),\n        )\n\n        chDB, _ = gorm.Open(clickhouse.Open(dsn), &gorm.Config{\n            Logger: logger.Default.LogMode(logger.Info),\n        })\n\n        sqlDB, _ := chDB.DB()\n        sqlDB.SetMaxIdleConns(viper.GetInt(\"CH_MAX_IDLE_CONN\"))  // 1\n        sqlDB.SetMaxOpenConns(viper.GetInt(\"CH_MAX_OPEN_CONN\"))  // 1\n    })\n    return chDB\n}\n\\`\\`\\`\n\n### Session Management\n\n\\`\\`\\`go\n// server/middlewares/session.go\nfunc ServiceSessionMiddlewares(serviceWrappers map[ServiceWrapper]ApiWrapper) func(next http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            ctx := appcontext.GetRequestContext(r.Context())\n\n            // Initialize PostgreSQL session\n            pgSession := postgres.NewSession()\n            ctx.Session = pgSession\n\n            // Initialize ClickHouse session (if needed)\n            if needsClickHouse(r.URL.Path) {\n                chSession := clickhouse.NewSession()\n                ctx.CHSession = chSession\n            }\n\n            // Timeout context\n            timeoutCtx, cancel := context.WithTimeout(r.Context(),\n                time.Duration(viper.GetInt(\"SERVER_TIMEOUT\")) * time.Second)\n            defer cancel()\n\n            // Execute handler\n            done := make(chan bool)\n            go func() {\n                next.ServeHTTP(w, r.WithContext(ctx.ToContext()))\n                done <- true\n            }()\n\n            select {\n            case <-done:\n                // Commit on success\n                pgSession.Commit(ctx)\n                if ctx.CHSession != nil {\n                    ctx.CHSession.Commit(ctx)\n                }\n                // Execute after-commit callbacks\n                pgSession.ExecuteAfterCommitCallbacks()\n\n            case <-timeoutCtx.Done():\n                // Rollback on timeout\n                pgSession.Rollback(ctx)\n                if ctx.CHSession != nil {\n                    ctx.CHSession.Rollback(ctx)\n                }\n                http.Error(w, \"Request timeout\", http.StatusGatewayTimeout)\n            }\n        })\n    }\n}\n\\`\\`\\`\n\n---\n\n## 7. BUSINESS MODULES (12 Modules)\n\n### Module Structure Pattern\n\nEach module follows the same structure:\n\\`\\`\\`\n{module}/\n├── api/api.go              # HTTP handlers\n├── service/service.go      # Business orchestration\n├── manager/manager.go      # Domain logic\n├── dao/dao.go              # Data access\n├── domain/\n│   ├── entry.go            # Input DTOs\n│   ├── entity.go           # Database entities\n│   └── response.go         # Output DTOs\n└── listeners/listeners.go  # Event handlers\n\\`\\`\\`\n\n### 1. Discovery Module\n\n\\`\\`\\`\nEndpoints:\n  GET  /discovery-service/api/profile/{profileId}\n  GET  /discovery-service/api/profile/{platform}/{platformProfileId}\n  GET  /discovery-service/api/profile/{platform}/{platformProfileId}/audience\n  POST /discovery-service/api/profile/{platform}/{platformProfileId}/timeseries\n  POST /discovery-service/api/profile/{platform}/{platformProfileId}/content\n  GET  /discovery-service/api/profile/{platform}/{platformProfileId}/hashtags\n  POST /discovery-service/api/profile/{platform}/{platformProfileId}/similar_accounts\n  POST /discovery-service/api/profile/{platform}/search\n  POST /discovery-service/api/profile/locations\n\nManagers:\n  - SearchManager: Profile search with filters\n  - TimeSeriesManager: Growth data over time\n  - HashtagsManager: Hashtag analytics\n  - AudienceManager: Demographic data\n  - LocationManager: Location-based search\n\\`\\`\\`\n\n### 2. Profile Collection Module\n\n\\`\\`\\`\nEndpoints:\n  POST   /profile-collection-service/api/collection/\n  GET    /profile-collection-service/api/collection/{id}\n  PUT    /profile-collection-service/api/collection/{id}\n  DELETE /profile-collection-service/api/collection/{id}\n  POST   /profile-collection-service/api/collection/search\n  GET    /profile-collection-service/api/collection/byshareid/{shareId}\n  POST   /profile-collection-service/api/collection/{id}/link/renew\n  GET    /profile-collection-service/api/collection/recent\n\n  POST   /profile-collection-service/api/collection/{collectionId}/item/\n  DELETE /profile-collection-service/api/collection/{collectionId}/item\n  PUT    /profile-collection-service/api/collection/{collectionId}/item/bulk\n  POST   /profile-collection-service/api/collection/item/search\n\\`\\`\\`\n\n### 3. Leaderboard Module\n\n\\`\\`\\`\nEndpoints:\n  POST /leaderboard-service/api/leaderboard/platform/{platform}\n  POST /leaderboard-service/api/leaderboard/platform/{platform}/category/{category}\n  POST /leaderboard-service/api/leaderboard/platform/{platform}/language/{language}\n  POST /leaderboard-service/api/leaderboard/cross-platform\n\\`\\`\\`\n\n### 4-12. Other Modules\n\n| Module | Purpose | Key Features |\n|--------|---------|--------------|\n| Post Collection | Curated post collections | Ingestion frequency, metrics |\n| Collection Analytics | Hashtag & keyword tracking | Post metrics summary |\n| Genre Insights | Genre-based analytics | Trending content |\n| Keyword Collection | Keyword-based collections | Search optimization |\n| Campaign Profiles | Campaign management | Profile associations |\n| Collection Group | Grouped collections | Hierarchical organization |\n| Partner Usage | Usage tracking | Plan limits, feature access |\n| Content | Content management | Media handling |\n\n---\n\n## 8. API RESPONSE FORMAT\n\n### Standard Response Structure\n\n\\`\\`\\`go\n// domain/response.go\ntype StandardResponse struct {\n    Data       interface{} \\`json:\"data\"\\`\n    Count      int         \\`json:\"count\"\\`\n    TotalCount int64       \\`json:\"totalCount\"\\`\n    Status     Status      \\`json:\"status\"\\`\n}\n\ntype Status struct {\n    Status     string \\`json:\"status\"\\`      // SUCCESS, ERROR\n    Message    string \\`json:\"message\"\\`\n    NextCursor string \\`json:\"nextCursor,omitempty\"\\`\n}\n\n// Example response\n{\n    \"data\": [...],\n    \"count\": 10,\n    \"totalCount\": 1000,\n    \"status\": {\n        \"status\": \"SUCCESS\",\n        \"message\": \"Records retrieved successfully\",\n        \"nextCursor\": \"11\"\n    }\n}\n\\`\\`\\`\n\n### Request Headers\n\n\\`\\`\\`\nAuthorization: Bearer <base64_encoded_token>\nx-bb-partner-id: {int64}\nx-bb-account-id: {int64}\nx-bb-plan-type: FREE|SAAS|PAID\nx-bb-uid: {string}\nx-bb-clientid: {string}\nx-bb-requestid: {string}\n\\`\\`\\`\n\n---\n\n## 9. CI/CD PIPELINE\n\n### GitLab CI Configuration\n\n\\`\\`\\`yaml\nstages:\n  - test\n  - build\n  - deploy_staging\n  - deploy_prod\n\nbuild:\n  stage: build\n  variables:\n    GOOS: linux\n    GOARCH: amd64\n  script:\n    - go build -o bin/coffee\n    - tar -czvf coffee.tar.gz .\n  artifacts:\n    paths:\n      - bin/coffee\n      - .env*\n      - scripts/\n      - coffee.tar.gz\n    expire_in: 1 week\n  only:\n    - master\n    - dev\n\ndeploy_staging:\n  stage: deploy_staging\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - deployer-stage-search\n  when: manual\n  only:\n    - master\n    - dev\n\ndeploy_prod_1:\n  stage: deploy_prod\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - cb1-1\n  when: manual\n  only:\n    - master\n\ndeploy_prod_2:\n  stage: deploy_prod\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - cb2-1\n  when: manual\n  only:\n    - master\n\\`\\`\\`\n\n---\n\n## 10. OBSERVABILITY\n\n### Prometheus Metrics\n\n\\`\\`\\`go\n// Chi-Prometheus integration\nchiprometheus.NewMiddleware(\n    \"coffee\",\n    chiprometheus.WithBuckets([]float64{300, 500, 1000, 5000, 10000, 20000, 30000}),\n)\n\n// Exposed at /metrics\n\\`\\`\\`\n\n### Sentry Error Tracking\n\n\\`\\`\\`go\n// sentry/sentry.go\nfunc Setup() {\n    sentry.Init(sentry.ClientOptions{\n        Dsn:              viper.GetString(\"SENTRY_DSN\"),\n        Environment:      viper.GetString(\"ENV\"),\n        AttachStacktrace: true,\n    })\n}\n\n// Middleware captures errors with request context\nfunc SentryErrorLoggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        hub := sentry.GetHubFromContext(r.Context())\n        hub.Scope().SetRequest(r)\n\n        defer func() {\n            if err := recover(); err != nil {\n                hub.RecoverWithContext(r.Context(), err)\n            }\n        }()\n\n        next.ServeHTTP(w, r)\n    })\n}\n\\`\\`\\`\n\n### Logging\n\n\\`\\`\\`go\n// logger/logger.go\nfunc Setup() {\n    logrus.SetFormatter(&logrus.JSONFormatter{})\n\n    level, _ := logrus.ParseLevel(viper.GetString(\"LOG_LEVEL\"))\n    logrus.SetLevel(level)\n\n    // Sentry hook for errors\n    hook, _ := logrus_sentry.NewSentryHook(viper.GetString(\"SENTRY_DSN\"), []logrus.Level{\n        logrus.PanicLevel,\n        logrus.FatalLevel,\n        logrus.ErrorLevel,\n    })\n    logrus.AddHook(hook)\n}\n\\`\\`\\`\n\n---\n\n## 11. KEY METRICS & STATISTICS\n\n| Metric | Value |\n|--------|-------|\n| **Total Lines of Code** | 8,500+ |\n| **Go Source Files** | 100+ |\n| **Database Tables** | 27+ |\n| **API Endpoints** | 50+ |\n| **Business Modules** | 12 |\n| **Middleware Layers** | 10 |\n| **Go Version** | 1.18 |\n| **Schema SQL Lines** | 918 |\n\n---\n\n## 12. SKILLS DEMONSTRATED\n\n### Technical Skills\n\n| Skill | Evidence |\n|-------|----------|\n| **Go Generics** | Type-safe Service/Manager/DAO with generic constraints |\n| **REST API Design** | 4-layer architecture with consistent patterns |\n| **Multi-Tenancy** | Partner/Account isolation with plan-based gating |\n| **Dual Database** | PostgreSQL (OLTP) + ClickHouse (OLAP) strategy |\n| **Message Queues** | Watermill + AMQP for event-driven architecture |\n| **Transaction Management** | Request-scoped transactions with auto-commit/rollback |\n| **Middleware Pipeline** | 10-layer middleware chain with timeout handling |\n| **Observability** | Prometheus, Sentry, structured logging |\n\n### Architecture Patterns\n\n1. **4-Layer REST Architecture** - API → Service → Manager → DAO\n2. **Generic Repository Pattern** - Type-safe CRUD with Go generics\n3. **Service Container** - Dependency injection container\n4. **Event-Driven** - Watermill pub/sub with after-commit callbacks\n5. **Multi-Tenant** - Header-based tenant isolation\n6. **Feature Flags** - Plan-based feature gating\n\n---\n\n## 13. INTERVIEW TALKING POINTS\n\n### System Design Questions\n\n**\"Describe a multi-tenant SaaS architecture you built\"**\n- 4-layer REST architecture with Go generics for type safety\n- Partner/Account/User hierarchy with header-based isolation\n- Plan-based feature gating (FREE/SAAS/PAID)\n- Activity tracking and usage limits per tenant\n\n**\"How do you handle different database needs?\"**\n- PostgreSQL for OLTP (collections, profiles, relationships)\n- ClickHouse for OLAP (time-series, analytics, aggregations)\n- Request-scoped transactions with dual session management\n- Automatic commit/rollback with timeout handling\n\n**\"Explain your event-driven architecture\"**\n- Watermill framework with AMQP transport\n- After-commit callbacks for guaranteed event publishing\n- Retry middleware (3 attempts, 100ms interval)\n- Separate queues per business module\n\n### Behavioral Questions\n\n**\"Tell me about a complex backend system you built\"**\n- Coffee: 8,500+ LOC, 12 business modules, 50+ endpoints\n- Generic REST framework used across all modules\n- Multi-tenant with plan-based access control\n- Dual database strategy for different workloads\n\n**\"How do you ensure code quality and consistency?\"**\n- Generic types enforce consistent patterns\n- Middleware chain ensures security at all endpoints\n- Transaction management prevents data inconsistency\n- Prometheus metrics for performance monitoring\n\n---\n\n*Generated through comprehensive source code analysis of the coffee project.*\n"
  },
  {
    "id": "ANALYSIS_saas_gateway",
    "title": "SaaS Gateway Analysis",
    "category": "analysis",
    "badge": null,
    "content": "# ULTRA-DEEP ANALYSIS: SAAS-GATEWAY API GATEWAY\n\n## PROJECT OVERVIEW\n\n| Attribute | Value |\n|-----------|-------|\n| **Project Name** | SaaS Gateway |\n| **Purpose** | API Gateway for Bulbul Creator Platform (goodcreator.co) |\n| **Architecture** | Reverse proxy with 7-layer middleware pipeline |\n| **Framework** | Gin v1.8.1 (Go) |\n| **Port** | 8009 (main), 6069 (pprof debug) |\n| **Services Proxied** | 13 microservices |\n| **Binary Size** | 23MB |\n\n---\n\n## 1. COMPLETE DIRECTORY STRUCTURE\n\n\\`\\`\\`\n/saas-gateway/\n├── .git/                              # Git repository\n├── .gitignore                         # Git ignore rules\n├── .gitlab-ci.yml                     # CI/CD pipeline (42 lines)\n│\n├── .env                               # Default environment\n├── .env.local                         # Local development config\n├── .env.stage                         # Staging environment config\n├── .env.production                    # Production environment config\n│\n├── go.mod                             # Go module definition\n├── go.sum                             # Dependency checksums\n├── main.go                            # Application entry point\n├── saas-gateway                       # Compiled binary (23MB)\n│\n├── api/                               # API Handlers\n│   └── heartbeat/\n│       └── heartbeat.go               # Health check endpoints\n│\n├── cache/                             # Caching Layer\n│   ├── redis.go                       # Redis cluster client (singleton)\n│   └── ristretto.go                   # In-memory cache (singleton)\n│\n├── client/                            # HTTP Client Layer\n│   ├── client.go                      # Base HTTP client (resty wrapper)\n│   │\n│   ├── entry/                         # Domain Models\n│   │   ├── identity.go                # User, Client, Account entities\n│   │   ├── status.go                  # Response status model\n│   │   └── asset.go                   # Asset information model\n│   │\n│   ├── identity/                      # Identity Service Client\n│   │   ├── identity.go                # Auth API methods\n│   │   └── identity_test.go           # Unit tests\n│   │\n│   ├── input/                         # Request DTOs\n│   │   ├── commons.go                 # Common input types\n│   │   ├── filter.go                  # Filter parameters\n│   │   └── identity.go                # Auth input types\n│   │\n│   ├── partner/                       # Partner Service Client\n│   │   ├── partner.go                 # Partner API methods\n│   │   └── partner_test.go            # Unit tests\n│   │\n│   └── response/                      # Response DTOs\n│       ├── abc.go                     # Generic responses\n│       ├── asset.go                   # Asset responses\n│       ├── identity.go                # Auth responses\n│       ├── partner.go                 # Partner responses\n│       └── status.go                  # Status responses\n│\n├── config/                            # Configuration\n│   └── config.go                      # Config struct and loader\n│\n├── context/                           # Request Context\n│   └── context.go                     # Gateway context utilities\n│\n├── custom/                            # Custom Types\n│   └── error.go                       # Custom error definitions\n│\n├── generator/                         # Utilities\n│   └── requestid.go                   # UUID request ID generator\n│\n├── handler/                           # HTTP Handlers\n│   └── saas/\n│       └── saas.go                    # Reverse proxy handler\n│\n├── header/                            # Constants\n│   └── header.go                      # HTTP header constants (25+ headers)\n│\n├── locale/                            # Localization\n│   ├── locale/\n│   │   └── locale.go                  # Locale utilities\n│   └── localeconfig/\n│       └── locale.go                  # Locale configuration\n│\n├── logger/                            # Logging\n│   └── logger.go                      # Logger setup (logrus + sentry)\n│\n├── metrics/                           # Observability\n│   └── metrics.go                     # Prometheus metrics collectors\n│\n├── middleware/                        # Middleware Layer\n│   ├── auth.go                        # JWT authentication (~150 lines)\n│   ├── requestid.go                   # Request ID injection\n│   └── requestlogger.go               # Request/response logging\n│\n├── route/                             # Route Definitions\n│   ├── heartbeatRoutes.go             # Health check routes\n│   └── route.go                       # Route types\n│\n├── router/                            # Router Setup\n│   └── router.go                      # Main router with middleware (~93 lines)\n│\n├── safego/                            # Safe Concurrency\n│   └── safego.go                      # Panic-safe goroutine wrapper\n│\n├── scripts/                           # Deployment\n│   └── start.sh                       # Graceful deployment script\n│\n├── sentry/                            # Error Tracking\n│   └── sentry.go                      # Sentry initialization\n│\n└── util/                              # Utilities\n    ├── transformer.go                 # Data transformers\n    └── util.go                        # Helper functions\n\\`\\`\\`\n\n---\n\n## 2. TECHNOLOGY STACK\n\n### Core Dependencies\n| Category | Technology | Version | Purpose |\n|----------|-----------|---------|---------|\n| **Web Framework** | Gin | v1.8.1 | HTTP routing & middleware |\n| **JWT Auth** | golang-jwt/jwt | v3.2.0 | Token validation |\n| **HTTP Client** | go-resty/resty | v2.3.0 | Upstream service calls |\n| **In-Memory Cache** | dgraph-io/ristretto | v0.0.3 | Local LFU caching (10M keys) |\n| **Distributed Cache** | go-redis | v7.0.0-beta.6 | Redis cluster client |\n| **Logging** | logrus + zerolog | v1.9.0 / v1.18.0 | Structured logging |\n| **Error Tracking** | getsentry/sentry-go | v0.20.0 | Error monitoring |\n| **Metrics** | prometheus/client_golang | v1.11.1 | Observability |\n| **Config** | joho/godotenv | v1.3.0 | Environment management |\n| **CORS** | rs/cors | v1.7.0 | Cross-origin configuration |\n| **UUID** | google/uuid | v1.3.0 | Request ID generation |\n| **Config Mgmt** | spf13/viper | v1.15.0 | Advanced configuration |\n\n### Go Module\n\\`\\`\\`go\nmodule init.bulbul.tv/bulbul-backend/saas-gateway\ngo 1.12\n\\`\\`\\`\n\n---\n\n## 3. GATEWAY ARCHITECTURE\n\n### Request Flow\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────┐\n│                     SAAS GATEWAY ARCHITECTURE                        │\n└─────────────────────────────────────────────────────────────────────┘\n\nINTERNET / MOBILE APP / WEB\n            ↓\n┌─────────────────────────────────────────────────────────────────────┐\n│                    LOAD BALANCER                                     │\n│  ┌─────────────────┐  ┌─────────────────┐                          │\n│  │   Node 1        │  │   Node 2        │                          │\n│  │   cb1-1:8009    │  │   cb2-1:8009    │                          │\n│  └─────────────────┘  └─────────────────┘                          │\n└─────────────────────────────────────────────────────────────────────┘\n            ↓\n┌─────────────────────────────────────────────────────────────────────┐\n│                    SaaS GATEWAY (Port 8009)                          │\n├─────────────────────────────────────────────────────────────────────┤\n│ MIDDLEWARE PIPELINE (Executed in Order):                             │\n│                                                                      │\n│  1. gin.Recovery()           ← Panic recovery                       │\n│  2. sentrygin.New()          ← Error tracking                       │\n│  3. cors.New()               ← CORS (41 origins)                    │\n│  4. GatewayContextMiddleware ← Context injection                    │\n│  5. RequestIdMiddleware()    ← UUID generation (x-bb-requestid)     │\n│  6. RequestLogger()          ← Request/response logging             │\n│  7. AppAuth()                ← JWT + Redis session validation       │\n│                                                                      │\n├─────────────────────────────────────────────────────────────────────┤\n│ ROUTE HANDLERS:                                                      │\n│                                                                      │\n│  GET  /metrics               ← Prometheus metrics                   │\n│  GET  /heartbeat/            ← Health check (200 OK / 410 Gone)     │\n│  PUT  /heartbeat/?beat=      ← Health state toggle                  │\n│  ANY  /{service}/*           ← Reverse proxy to downstream          │\n│                                                                      │\n└─────────────────────────────────────────────────────────────────────┘\n            ↓\n┌─────────────────────────────────────────────────────────────────────┐\n│                    CACHING LAYER                                     │\n│                                                                      │\n│  ┌────────────────────────┐  ┌────────────────────────┐            │\n│  │ LAYER 1: Ristretto     │  │ LAYER 2: Redis Cluster │            │\n│  │ (In-Memory LFU)        │  │ (Distributed)          │            │\n│  │                        │  │                        │            │\n│  │ - 10M key capacity     │  │ - 3-6 nodes            │            │\n│  │ - 1GB max memory       │  │ - 100 pool size        │            │\n│  │ - Nanosecond lookups   │  │ - session:{id} keys    │            │\n│  │ - Per-instance         │  │ - Shared across nodes  │            │\n│  └────────────────────────┘  └────────────────────────┘            │\n│                                                                      │\n└─────────────────────────────────────────────────────────────────────┘\n            ↓\n┌─────────────────────────────────────────────────────────────────────┐\n│                    EXTERNAL SERVICES                                 │\n│                                                                      │\n│  ┌────────────────────────┐  ┌────────────────────────┐            │\n│  │ Identity Service       │  │ Partner Service        │            │\n│  │ /auth/verify/token     │  │ /partner/{id}          │            │\n│  │ /auth/login            │  │                        │            │\n│  └────────────────────────┘  └────────────────────────┘            │\n│                                                                      │\n└─────────────────────────────────────────────────────────────────────┘\n            ↓\n┌─────────────────────────────────────────────────────────────────────┐\n│                    DOWNSTREAM MICROSERVICES                          │\n│                                                                      │\n│  ┌─────────────────────────────────────────────────────────────┐   │\n│  │                     SAAS_URL (Primary)                       │   │\n│  │  discovery-service      │ leaderboard-service               │   │\n│  │  profile-collection     │ post-collection-service           │   │\n│  │  activity-service       │ collection-analytics-service      │   │\n│  │  genre-insights-service │ content-service                   │   │\n│  │  collection-group       │ keyword-collection-service        │   │\n│  │  partner-usage-service  │ campaign-profile-service          │   │\n│  └─────────────────────────────────────────────────────────────┘   │\n│                                                                      │\n│  ┌─────────────────────────────────────────────────────────────┐   │\n│  │                    SAAS_DATA_URL (Data)                      │   │\n│  │  social-profile-service  (Uses different backend URL)       │   │\n│  └─────────────────────────────────────────────────────────────┘   │\n│                                                                      │\n└─────────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### Proxied Services (13 Total)\n\n| Service | Route Pattern | Backend |\n|---------|--------------|---------|\n| discovery-service | \\`/discovery-service/*\\` | SAAS_URL |\n| leaderboard-service | \\`/leaderboard-service/*\\` | SAAS_URL |\n| profile-collection-service | \\`/profile-collection-service/*\\` | SAAS_URL |\n| activity-service | \\`/activity-service/*\\` | SAAS_URL |\n| collection-analytics-service | \\`/collection-analytics-service/*\\` | SAAS_URL |\n| post-collection-service | \\`/post-collection-service/*\\` | SAAS_URL |\n| genre-insights-service | \\`/genre-insights-service/*\\` | SAAS_URL |\n| content-service | \\`/content-service/*\\` | SAAS_URL |\n| **social-profile-service** | \\`/social-profile-service/*\\` | **SAAS_DATA_URL** |\n| collection-group-service | \\`/collection-group-service/*\\` | SAAS_URL |\n| keyword-collection-service | \\`/keyword-collection-service/*\\` | SAAS_URL |\n| partner-usage-service | \\`/partner-usage-service/*\\` | SAAS_URL |\n| campaign-profile-service | \\`/campaign-profile-service/*\\` | SAAS_URL |\n\n---\n\n## 4. MIDDLEWARE PIPELINE\n\n### Execution Order\n\n\\`\\`\\`go\n// router/router.go (Lines 18-93)\n\nfunc SetupRouter(config config.Config) *gin.Engine {\n    router := gin.New()\n\n    // 1. Panic Recovery\n    router.Use(gin.Recovery())\n\n    // 2. Sentry Error Tracking\n    router.Use(sentrygin.New(sentrygin.Options{Repanic: true}))\n\n    // 3. CORS (41 whitelisted origins)\n    router.Use(cors.New(cors.Options{\n        AllowedOrigins: []string{\n            \"http://staging.app.vidooly.com\",\n            \"https://stage.cf-provider.goodcreator.co\",\n            \"https://cf-provider.goodcreator.co\",\n            \"https://www.instagram.com\",\n            \"https://www.youtube.com\",\n            \"https://suite.goodcreator.co\",\n            \"https://goodcreator.co\",\n            \"http://localhost:3000\",\n            \"http://localhost:3001\",\n            \"http://localhost:8298\",\n            // ... 31 more origins\n        },\n        AllowedMethods:   []string{\"HEAD\", \"GET\", \"POST\", \"OPTIONS\", \"PUT\", \"PATCH\", \"DELETE\"},\n        AllowedHeaders:   []string{\"*\"},\n        AllowCredentials: true,\n    }))\n\n    // 4. Gateway Context Injection\n    router.Use(GatewayContextToContextMiddleware(config))\n\n    // 5. Request ID Generation\n    router.Use(middleware.RequestIdMiddleware())\n\n    // 6. Request/Response Logging\n    router.Use(middleware.RequestLogger(config))\n\n    // Route Groups\n    router.GET(\"/metrics\", gin.WrapH(promhttp.Handler()))\n\n    heartbeatRouter := router.Group(\"/heartbeat\")\n    groupRoutes(config, heartbeatRouter, route.HeartbeatRoutes)\n\n    // 7. Authentication (per-route)\n    router.Any(\"/discovery-service/*any\", middleware.AppAuth(config), saas.ReverseProxy(\"discovery-service\", config))\n    // ... 12 more service routes\n\n    return router\n}\n\\`\\`\\`\n\n### Middleware Details\n\n#### 1. Panic Recovery\n\\`\\`\\`go\nrouter.Use(gin.Recovery())\n// Built-in Gin middleware\n// Catches all panics, returns 500 Internal Server Error\n// Prevents server crash from individual request failures\n\\`\\`\\`\n\n#### 2. Sentry Error Tracking\n\\`\\`\\`go\nrouter.Use(sentrygin.New(sentrygin.Options{Repanic: true}))\n// Captures errors and sends to Sentry\n// Repanic: true means panic is re-raised after capture\n// Works with Recovery() middleware for complete coverage\n\\`\\`\\`\n\n#### 3. CORS Configuration\n\\`\\`\\`go\ncors.New(cors.Options{\n    AllowedOrigins:   []string{...}, // 41 origins\n    AllowedMethods:   []string{\"HEAD\", \"GET\", \"POST\", \"OPTIONS\", \"PUT\", \"PATCH\", \"DELETE\"},\n    AllowedHeaders:   []string{\"*\"},\n    AllowCredentials: true,\n})\n\n// Whitelisted Origins Include:\n// - Production: cf-provider.goodcreator.co, suite.goodcreator.co\n// - Staging: stage.cf-provider.goodcreator.co\n// - Local: localhost:3000, localhost:3001, localhost:8298\n// - External: instagram.com, youtube.com\n\\`\\`\\`\n\n#### 4. Gateway Context Middleware\n\\`\\`\\`go\nfunc GatewayContextToContextMiddleware(config config.Config) gin.HandlerFunc {\n    return func(c *gin.Context) {\n        gc := context.New(c, config)\n        c.Set(\"gatewayContext\", gc)\n        c.Next()\n    }\n}\n\n// context.New() does:\n// - Generates request ID (UUID v4)\n// - Blocks bots (User-Agent filtering)\n// - Creates zerolog logger with request ID\n// - Extracts whitelisted x-bb-* headers\n// - Sets default locale (en)\n\\`\\`\\`\n\n#### 5. Request ID Middleware\n\\`\\`\\`go\n// generator/requestid.go\nfunc SetNXRequestIdOnContext(c *gin.Context) {\n    if c.Keys == nil {\n        c.Keys = make(map[string]interface{})\n    }\n\n    if c.Keys[header.RequestID] == nil {\n        uuid := uuid.New().String()\n        c.Keys[header.RequestID] = uuid\n\n        // Set in request header\n        if c.Request != nil && c.Request.Header != nil {\n            c.Request.Header.Set(header.RequestID, uuid)\n        }\n\n        // Set in response header\n        if c.Writer != nil && c.Writer.Header() != nil {\n            c.Writer.Header().Set(header.RequestID, uuid)\n        }\n    }\n}\n\\`\\`\\`\n\n#### 6. Request Logger Middleware\n\\`\\`\\`go\n// middleware/requestlogger.go\nfunc RequestLogger(config config.Config) gin.HandlerFunc {\n    return func(c *gin.Context) {\n        start := time.Now()\n        path := c.Request.URL.Path\n        query := c.Request.URL.RawQuery\n\n        gc := util.GatewayContextFromGinContext(c, config)\n\n        // Capture request body (non-destructive read)\n        buf, _ := ioutil.ReadAll(c.Request.Body)\n        rdr1 := ioutil.NopCloser(bytes.NewBuffer(buf))\n        rdr2 := ioutil.NopCloser(bytes.NewBuffer(buf))\n        body := readBody(rdr1)\n        c.Request.Body = rdr2\n\n        // Wrap response writer to capture response\n        w := &responseBodyWriter{body: &bytes.Buffer{}, ResponseWriter: c.Writer}\n        c.Writer = w\n\n        c.Next()\n\n        // Log based on LOG_LEVEL\n        gc.Logger.Error().Msg(fmt.Sprintf(\n            \"%s - %s - [%s] \\\\\"%s %s %s %s %d %s %s\\\\\"\\\\n%s\\\\n%s\\\\n\",\n            c.Request.Header.Get(header.RequestID),\n            c.ClientIP(),\n            time.Now().Format(time.RFC1123),\n            c.Request.Method,\n            path,\n            c.Request.Header.Get(header.ApolloOpName),\n            c.Request.Proto,\n            c.Writer.Status(),\n            \"Response time: \", time.Now().Sub(start),\n            c.Request.UserAgent(),\n            c.Request.Header,\n        ))\n    }\n}\n\\`\\`\\`\n\n#### 7. Authentication Middleware\n\\`\\`\\`go\n// middleware/auth.go (Lines 23-151)\nfunc AppAuth(config config.Config) gin.HandlerFunc {\n    return func(c *gin.Context) {\n        gc := util.GatewayContextFromGinContext(c, config)\n        verified := false\n\n        // Step 1: Extract Client ID\n        if clientId := gc.GetHeader(header.ClientID); clientId != \"\" {\n            clientType := gc.GetHeader(header.ClientType)\n            if clientType == \"\" {\n                clientType = \"CUSTOMER\"\n            }\n            gc.GenerateClientAuthorizationWithClientId(clientId, clientType)\n        }\n\n        // Step 2: Skip auth for init device endpoints\n        apolloOp := gc.Context.GetHeader(header.ApolloOpName)\n        if apolloOp == \"initDeviceGoMutation\" || apolloOp == \"initDeviceV2\" ||\n           apolloOp == \"initDeviceV2Mutation\" || apolloOp == \"initDevice\" ||\n           strings.Contains(c.Request.URL.Path, \"/api/auth/init\") {\n            verified = true\n        }\n\n        // Step 3: JWT Token Validation\n        if authorization := gc.Context.GetHeader(\"Authorization\"); authorization != \"\" {\n            splittedAuthHeader := strings.Split(authorization, \" \")\n            if len(splittedAuthHeader) > 1 && splittedAuthHeader[1] != \"\" {\n\n                // Parse JWT with HMAC-SHA256\n                token, err := jwt.Parse(splittedAuthHeader[1], func(token *jwt.Token) (interface{}, error) {\n                    if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {\n                        return nil, fmt.Errorf(\"unexpected signing method: %v\", token.Header[\"alg\"])\n                    }\n                    hmac, _ := b64.StdEncoding.DecodeString(config.HMAC_SECRET)\n                    return hmac, nil\n                })\n\n                if err == nil && token.Valid {\n                    claims := token.Claims.(jwt.MapClaims)\n\n                    // Step 4: Redis Session Lookup\n                    if sessionId, ok := claims[\"sid\"].(string); ok && sessionId != \"\" {\n                        keyExists := cache.Redis(gc.Config).Exists(\"session:\" + sessionId)\n                        if exist, _ := keyExists.Result(); exist == 1 {\n                            verified = true\n                            // Extract user info from claims\n                            gc.UserClientAccount = &entry.UserClientAccount{\n                                UserId: claims[\"uid\"].(int),\n                                Id:     claims[\"userAccountId\"].(int),\n                            }\n                        }\n                    }\n\n                    // Step 5: Fallback to Identity Service\n                    if !verified {\n                        verifyTokenResponse, err := identity.New(gc).VerifyToken(\n                            &input.Token{Token: splittedAuthHeader[1]},\n                            false,\n                        )\n                        if err == nil && verifyTokenResponse.UserClientAccount != nil {\n                            verified = true\n                            gc.UserClientAccount = verifyTokenResponse.UserClientAccount\n                        }\n                    }\n\n                    // Step 6: Partner Plan Validation\n                    if verified && gc.UserClientAccount != nil {\n                        if gc.UserClientAccount.PartnerProfile != nil {\n                            partnerId := strconv.FormatInt(\n                                gc.UserClientAccount.PartnerProfile.PartnerId, 10)\n                            partnerResponse, _ := partner.New(gc).FindPartnerById(partnerId)\n\n                            for _, contract := range partnerResponse.Partners[0].Contracts {\n                                if contract.ContractType == \"SAAS\" {\n                                    currentTime := time.Now()\n                                    startTime := time.Unix(0, contract.StartTime*int64(time.Millisecond))\n                                    endTime := time.Unix(0, contract.EndTime*int64(time.Millisecond))\n\n                                    if currentTime.After(startTime) && currentTime.Before(endTime) {\n                                        gc.UserClientAccount.PartnerProfile.PlanType = contract.Plan\n                                    } else {\n                                        gc.UserClientAccount.PartnerProfile.PlanType = \"FREE\"\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        // Step 7: Generate Authorization Headers\n        gc.GenerateAuthorization()\n\n        if !verified {\n            c.AbortWithStatus(http.StatusUnauthorized)\n            return\n        }\n\n        c.Next()\n    }\n}\n\\`\\`\\`\n\n---\n\n## 5. CACHING ARCHITECTURE\n\n### Two-Layer Caching Strategy\n\n\\`\\`\\`\n┌─────────────────────────────────────────────────────────────────────┐\n│                    TWO-LAYER CACHING                                 │\n├─────────────────────────────────────────────────────────────────────┤\n│                                                                      │\n│  REQUEST → [Layer 1: Ristretto] → HIT? → Return                     │\n│                      ↓ MISS                                          │\n│            [Layer 2: Redis]     → HIT? → Return + Cache L1          │\n│                      ↓ MISS                                          │\n│            [Identity Service]   → Return + Cache L1 + Cache L2      │\n│                                                                      │\n└─────────────────────────────────────────────────────────────────────┘\n\\`\\`\\`\n\n### Layer 1: Ristretto In-Memory Cache\n\n\\`\\`\\`go\n// cache/ristretto.go\nvar (\n    singletonRistretto *ristretto.Cache\n    ristrettoOnce      sync.Once\n)\n\nfunc Ristretto(config config.Config) *ristretto.Cache {\n    ristrettoOnce.Do(func() {\n        singletonRistretto, _ = ristretto.NewCache(&ristretto.Config{\n            NumCounters: 1e7,     // 10,000,000 keys tracked for frequency\n            MaxCost:     1 << 30, // 1GB maximum memory\n            BufferItems: 64,      // 64 keys per Get buffer (batching)\n        })\n    })\n    return singletonRistretto\n}\n\\`\\`\\`\n\n**Characteristics:**\n- **Capacity**: 10 million keys\n- **Memory**: 1GB maximum\n- **Eviction**: LFU (Least Frequently Used)\n- **Scope**: Per-instance (not shared)\n- **Latency**: Nanoseconds\n\n### Layer 2: Redis Cluster\n\n\\`\\`\\`go\n// cache/redis.go\nvar (\n    singletonRedis *redis.ClusterClient\n    redisInit      sync.Once\n)\n\nfunc Redis(config config.Config) *redis.ClusterClient {\n    redisInit.Do(func() {\n        singletonRedis = redis.NewClusterClient(&redis.ClusterOptions{\n            Addrs:    config.RedisClusterAddresses,\n            PoolSize: 100,\n            Password: config.REDIS_CLUSTER_PASSWORD,\n        })\n    })\n    return singletonRedis\n}\n\\`\\`\\`\n\n**Configuration by Environment:**\n\n| Environment | Nodes | Addresses |\n|-------------|-------|-----------|\n| Production | 3 | bulbul-redis-prod-1:6379, -2:6380, -3:6381 |\n| Staging | 3 | bulbul-redis-stage-1:6379, -2:6380, -3:6381 |\n| Local | 6 | localhost:30001-30006 |\n\n**Characteristics:**\n- **Pool Size**: 100 connections\n- **Key Format**: \\`session:{sessionId}\\`\n- **Scope**: Shared across all instances\n- **Latency**: Milliseconds\n\n---\n\n## 6. REVERSE PROXY IMPLEMENTATION\n\n### Proxy Handler\n\n\\`\\`\\`go\n// handler/saas/saas.go\nfunc ReverseProxy(module string, config config.Config) func(c *gin.Context) {\n    return func(c *gin.Context) {\n        appContext := util.GatewayContextFromGinContext(c, config)\n\n        // Select backend URL\n        saasUrl := config.SAAS_URL\n        if module == \"social-profile-service\" {\n            saasUrl = config.SAAS_DATA_URL  // Different backend for data service\n        }\n\n        // Parse target URL\n        remote, err := url.Parse(saasUrl)\n        if err != nil {\n            panic(err)\n        }\n\n        // Extract auth info\n        authHeader := appContext.UserAuthorization\n        var partnerId string\n        planType := \"FREE\"\n\n        if appContext.UserClientAccount != nil {\n            if appContext.UserClientAccount.PartnerProfile != nil {\n                partnerId = strconv.Itoa(int(appContext.UserClientAccount.PartnerProfile.PartnerId))\n                if appContext.UserClientAccount.PartnerProfile.PlanType != \"\" {\n                    planType = appContext.UserClientAccount.PartnerProfile.PlanType\n                }\n            }\n        }\n\n        // Create reverse proxy\n        proxy := httputil.NewSingleHostReverseProxy(remote)\n\n        // Request transformation\n        deviceId := appContext.XHeader.Get(header.Device)\n        proxy.Director = func(req *http.Request) {\n            req.Header = c.Request.Header                    // Copy all headers\n            req.Header.Set(\"Authorization\", authHeader)      // Override auth\n            req.Header.Set(header.PartnerId, partnerId)      // Add partner ID\n            req.Header.Set(header.PlanType, planType)        // Add plan type\n            req.Header.Set(header.Device, deviceId)          // Add device ID\n            req.Header.Del(\"accept-encoding\")                // Remove encoding\n            req.Host = remote.Host\n            req.URL.Scheme = remote.Scheme\n            req.URL.Host = remote.Host\n            req.URL.Path = \"/\" + module + c.Param(\"any\")     // Reconstruct path\n        }\n\n        // Response handling\n        proxy.ModifyResponse = responseHandler(appContext, c.Request.Method,\n            c.Param(\"any\"), c.Request.Header.Get(\"origin\"), ...)\n\n        proxy.ServeHTTP(c.Writer, c.Request)\n    }\n}\n\nfunc responseHandler(appContext *context.Context, method string, path string,\n    origin string, setOrigin bool, originalAuth string) func(*http.Response) error {\n\n    return func(resp *http.Response) error {\n        resp.Header.Del(\"Access-Control-Allow-Origin\")\n        if setOrigin {\n            resp.Header.Set(\"Access-Control-Allow-Origin\", origin)\n        }\n\n        // Log non-200 responses\n        if resp.Status != \"200 OK\" {\n            log.Printf(\"SAAS-Bad-Response - %s, %s, %v : from : %s\",\n                method, path, resp.Status, originalAuth)\n        }\n\n        // Increment Prometheus counter\n        metrics.WinklRequestInc(appContext, method, path, resp.Status)\n\n        return nil\n    }\n}\n\\`\\`\\`\n\n### Request Enrichment Headers\n\n| Header | Source | Example |\n|--------|--------|---------|\n| \\`Authorization\\` | Generated Basic auth | \\`Basic dXNlcjE...\\` |\n| \\`x-bb-partner-id\\` | JWT claims | \\`12345\\` |\n| \\`x-bb-plan-type\\` | Partner contract | \\`PRO\\`, \\`FREE\\`, \\`BUSINESS\\` |\n| \\`x-bb-device\\` | Original request | \\`device-uuid-123\\` |\n| \\`x-bb-requestid\\` | Generated UUID | \\`550e8400-e29b-41d4...\\` |\n\n---\n\n## 7. HTTP HEADERS SPECIFICATION\n\n### Whitelisted Headers (25+)\n\n\\`\\`\\`go\n// header/header.go\nconst (\n    ForwardedFor   = \"x-forwarded-for\"      // Original client IP\n    UID            = \"x-bb-uid\"              // User ID\n    Timestamp      = \"x-bb-timestamp\"        // Request timestamp\n    Os             = \"x-bb-os\"               // Operating system\n    ClientID       = \"x-bb-clientid\"         // Client/app ID\n    RequestID      = \"x-bb-requestid\"        // Unique request ID\n    ClientType     = \"x-bb-clienttype\"       // Client type (CUSTOMER, ERP)\n    Version        = \"x-bb-version\"          // App version number\n    VersionName    = \"x-bb-version-name\"     // App version name\n    Latitude       = \"x-bb-latitude\"         // Device latitude\n    Longitude      = \"x-bb-longitude\"        // Device longitude\n    Location       = \"x-bb-location\"         // Location string\n    Device         = \"x-bb-deviceid\"         // Device ID\n    BBDevice       = \"x-bb-custom-deviceid\"  // Custom device ID\n    Channel        = \"x-bb-channelid\"        // Channel/source ID\n    Country        = \"x-bb-country\"          // Country code\n    Locale         = \"accept-language\"       // Language (en, hi, bn, ta, te)\n    ApolloOpName   = \"x-apollo-operation-name\"   // GraphQL operation\n    ApolloOpID     = \"x-apollo-operation-id\"     // GraphQL operation ID\n    UserRT         = \"x-bb-user-rt\"          // User RT\n    AdvertisingID  = \"x-bb-advertising-id\"   // Advertising ID\n    ABTestInfo     = \"x-ab-test-info\"        // A/B test assignments\n    PartnerId      = \"x-bb-partner-id\"       // Partner ID\n    PlanType       = \"x-bb-plan-type\"        // SaaS plan type\n    NewUser        = \"x-bb-new-user\"         // New user flag\n    PpId           = \"x-bb-pp-id\"            // PP ID\n)\n\\`\\`\\`\n\n---\n\n## 8. DOMAIN MODELS\n\n### User Authentication Models\n\n\\`\\`\\`go\n// client/entry/identity.go\n\n// Client information\ntype Client struct {\n    Id      string  // Client ID (app identifier)\n    AppName string  // Application name\n    AppType string  // \"CUSTOMER\", \"ERP\", etc.\n    Enabled bool    // Client status\n}\n\n// User profile\ntype User struct {\n    Id           int\n    Uidx         string\n    Dob          *int64\n    Emails       []UserEmail\n    Phones       []UserPhone\n    Gender       *Gender  // \"MALE\", \"FEMALE\", \"OTHERS\"\n    ReferralCode *string\n}\n\n// Main authorization entity\ntype UserClientAccount struct {\n    Id                 int\n    ClientId           string\n    ClientAppType      string\n    UserId             int\n    Status             UserClientAccountStatus  // ONBOARDING, ACTIVE, INACTIVE, BLOCKED\n\n    Name               *string\n    Bio                *string\n    Email              *string\n    ProfileImageId     *int\n    ProfileImage       *AssetInfo\n    CoverImageId       *int\n    CoverImage         *AssetInfo\n\n    User               *User\n    HostProfile        *HostProfile        // Influencer profile\n    CustomerProfile    *CustomerProfile    // Buyer profile\n    PartnerProfile     *PartnerUserProfile // SaaS partner profile\n\n    SocialAccounts     []SocialAccount\n    KycData            *UserKycData\n}\n\n// Partner-specific user profile\ntype PartnerUserProfile struct {\n    PartnerId           int64\n    Phone               *string\n    PlanType            string  // \"FREE\", \"PRO\", \"BUSINESS\"\n    VisitedDemo         bool\n    AssignedCampaignIds []int\n    WinklBrand          *WinklBrand\n}\n\n// Social media account\ntype SocialAccount struct {\n    Platform           SocialNetworkType  // GOOGLE, FB, TWITTER, TIKTOK, YOUTUBE, INSTAGRAM\n    Handle             string\n    SocialUserId       *string\n    AccessToken        *string\n    GraphAccessToken   *string\n    AccessTokenExpired *bool\n    Verified           bool\n    Metrics            *SocialAccountMetrics\n}\n\\`\\`\\`\n\n### Partner Models\n\n\\`\\`\\`go\n// client/response/partner.go\n\ntype PartnerResponse struct {\n    Status   entry.Status\n    Partners []Partner\n}\n\ntype Partner struct {\n    ID                   int64\n    Name                 string\n    Status               string\n    InventorySyncEnabled bool\n    Contracts            []Contract\n}\n\ntype Contract struct {\n    ID               int64   // Contract ID\n    PartnerID        int64   // Partner ID\n    ContractType     string  // \"SAAS\"\n    Status           string\n    OnboardingStatus string\n    Plan             string  // Pricing plan (FREE, PRO, BUSINESS)\n    StartTime        int64   // Unix milliseconds\n    EndTime          int64   // Unix milliseconds\n}\n\\`\\`\\`\n\n---\n\n## 9. EXTERNAL SERVICE CLIENTS\n\n### Base HTTP Client\n\n\\`\\`\\`go\n// client/client.go\ntype BaseClient struct {\n    *resty.Client\n    Context *gatewayContext.Context\n}\n\nfunc New(ctx *gatewayContext.Context) *BaseClient {\n    return &BaseClient{\n        resty.New().\n            SetDebug(true).\n            SetTimeout(15 * time.Second).\n            SetLogger(NewLogger(ctx.Logger)).\n            OnRequestLog(func(r *resty.RequestLog) error {\n                if ctx.Config.LOG_LEVEL >= 3 {\n                    r.Body = \"\"  // Don't log body at high log levels\n                }\n                return nil\n            }).\n            OnResponseLog(func(r *resty.ResponseLog) error {\n                if ctx.Config.LOG_LEVEL >= 3 {\n                    r.Body = \"\"\n                }\n                return nil\n            }),\n        ctx,\n    }\n}\n\nfunc (c *BaseClient) GenerateRequest(authorization string) *resty.Request {\n    c.SetAllowGetMethodPayload(true)\n    req := c.NewRequest()\n    req.SetHeader(\"Authorization\", authorization)\n\n    // Propagate X-Headers from context\n    if c.Context.XHeader != nil {\n        for key, vals := range c.Context.XHeader {\n            for _, val := range vals {\n                req.Header.Add(key, val)\n            }\n        }\n    }\n\n    return req\n}\n\\`\\`\\`\n\n### Identity Service Client\n\n\\`\\`\\`go\n// client/identity/identity.go\ntype Client struct {\n    *client.BaseClient\n}\n\nfunc New(ctx *context.Context) *Client {\n    c := &Client{client.New(ctx)}\n    c.SetTimeout(time.Duration(1 * time.Minute))  // Longer timeout for auth\n    return c\n}\n\nfunc (c *Client) VerifyToken(input *input.Token, skipTokenCache bool) (*response.UserAuthResponse, error) {\n    response := &response.UserAuthResponse{}\n    _, err := c.GenerateRequest(c.Context.ClientAuthorization).\n        SetBody(input).\n        SetResult(response).\n        SetQueryParam(\"skipTokenCache\", strconv.FormatBool(skipTokenCache)).\n        Post(c.Context.Config.IDENTITY_URL + \"/auth/verify/token?loadPreferences=true\")\n\n    if err != nil {\n        return nil, err\n    }\n    return response, nil\n}\n\nfunc (c *Client) UserAuth(userAuthInput *input.UserAuth) (*response.UserAuthResponse, error) {\n    response := &response.UserAuthResponse{}\n    _, err := c.GenerateRequest(c.Context.ClientAuthorization).\n        SetBody(userAuthInput).\n        SetResult(response).\n        Post(c.Context.Config.IDENTITY_URL + \"/auth/login\")\n\n    if err != nil {\n        return nil, err\n    }\n    return response, nil\n}\n\\`\\`\\`\n\n### Partner Service Client\n\n\\`\\`\\`go\n// client/partner/partner.go\ntype Client struct {\n    *client.BaseClient\n}\n\nfunc New(ctx *context.Context) *Client {\n    c := &Client{client.New(ctx)}\n    c.SetTimeout(time.Duration(1 * time.Minute))\n    return c\n}\n\nfunc (c *Client) FindPartnerById(partnerId string) (*response.PartnerResponse, error) {\n    response := &response.PartnerResponse{}\n    _, err := c.GenerateRequest(c.Context.ClientAuthorization).\n        SetPathParams(map[string]string{\"partnerId\": partnerId}).\n        SetResult(response).\n        Get(c.Context.Config.PARTNER_URL + \"/partner/{partnerId}\")\n\n    if err != nil {\n        return nil, err\n    }\n    return response, nil\n}\n\\`\\`\\`\n\n---\n\n## 10. OBSERVABILITY\n\n### Prometheus Metrics\n\n\\`\\`\\`go\n// metrics/metrics.go\nvar (\n    WinklRequestCounter          *prometheus.CounterVec\n    LoadPageEmptyResponseCounter *prometheus.CounterVec\n    LoadPageInvalidSlugCounter   *prometheus.CounterVec\n    LoadPageErrorCounter         *prometheus.CounterVec\n)\n\nfunc SetupCollectors(config config.Config) {\n    collectorInit.Do(func() {\n        // Track all proxy requests\n        WinklRequestCounter = promauto.NewCounterVec(prometheus.CounterOpts{\n            Name: \"requests_winkl\",\n            Help: \"Track Requests via Winkl Proxy\",\n        }, []string{\"method\", \"path\", \"channel\", \"clientId\", \"responseStatus\"})\n\n        // Track loadPage calls with 0 results\n        LoadPageEmptyResponseCounter = promauto.NewCounterVec(prometheus.CounterOpts{\n            Name: \"zero_edges_loadPage\",\n            Help: \"Track # of loadPage calls with 0 edges\",\n        }, []string{\"slug\", \"version\", \"channel\", \"pageNo\"})\n\n        // Track invalid slug requests\n        LoadPageInvalidSlugCounter = promauto.NewCounterVec(prometheus.CounterOpts{\n            Name: \"invalid_slug_loadPage\",\n            Help: \"Track # of loadPage calls with invalid slug\",\n        }, []string{\"slug\", \"version\", \"channel\", \"pageNo\"})\n\n        // Track loadPage errors\n        LoadPageErrorCounter = promauto.NewCounterVec(prometheus.CounterOpts{\n            Name: \"errors_loadPage\",\n            Help: \"Track # of loadPage errors\",\n        }, []string{\"slug\", \"version\", \"channel\", \"pageNo\"})\n    })\n}\n\nfunc WinklRequestInc(gc *context.Context, method string, path string, responseStatus string) {\n    if WinklRequestCounter != nil {\n        WinklRequestCounter.With(prometheus.Labels{\n            \"method\":         method,\n            \"path\":           path,\n            \"responseStatus\": responseStatus,\n            \"channel\":        gc.XHeader.Get(header.Channel),\n            \"clientId\":       gc.XHeader.Get(header.ClientID),\n        }).Inc()\n    }\n}\n\\`\\`\\`\n\n**Metrics Endpoint**: \\`GET /metrics\\`\n\n### Sentry Error Tracking\n\n\\`\\`\\`go\n// sentry/sentry.go\nfunc Setup(config config.Config) {\n    if config.Env != \"local\" {\n        if err := sentry.Init(sentry.ClientOptions{\n            Dsn:              config.SENTRY_DSN,\n            Environment:      config.Env,\n            AttachStacktrace: true,\n        }); err != nil {\n            log.Println(\"Sentry init failed\")\n        }\n    }\n}\n\nfunc PatchErrorToSentry(gc *context.Context, e error, err interface{}) {\n    gcInterface := make(map[string]interface{})\n\n    if gc != nil {\n        gcInterface[\"headers\"] = gc.XHeader\n        if gc.UserClientAccount != nil {\n            gcInterface[\"user\"] = gc.UserClientAccount\n        }\n        if gc.Client != nil {\n            gcInterface[\"client\"] = gc.Client\n        }\n\n        sentry.ConfigureScope(func(scope *sentry.Scope) {\n            tags := map[string]string{\n                header.RequestID:    gc.XHeader.Get(header.RequestID),\n                header.ApolloOpName: gc.XHeader.Get(header.ApolloOpName),\n                header.Version:      gc.XHeader.Get(header.Version),\n                header.ClientID:     gc.XHeader.Get(header.ClientID),\n                header.Channel:      gc.XHeader.Get(header.Channel),\n            }\n            if gc.UserClientAccount != nil {\n                tags[\"userId\"] = strconv.Itoa(gc.UserClientAccount.UserId)\n            }\n            scope.SetTags(tags)\n        })\n    }\n\n    if e != nil {\n        sentry.CaptureException(e)\n    }\n    if err != nil {\n        sentry.CaptureException(errors.New(fmt.Sprintf(\"%+v\", err)))\n    }\n}\n\\`\\`\\`\n\n### Health Check Endpoints\n\n\\`\\`\\`go\n// api/heartbeat/heartbeat.go\nvar beat = false  // Health state flag\n\n// GET /heartbeat/\nfunc Beat(config config.Config) func(c *gin.Context) {\n    return func(c *gin.Context) {\n        if beat {\n            c.Status(http.StatusOK)   // 200 - Healthy\n        } else {\n            c.Status(http.StatusGone) // 410 - Out of LB\n        }\n    }\n}\n\n// PUT /heartbeat/?beat=true|false\nfunc ModifyBeat(config config.Config) func(c *gin.Context) {\n    return func(c *gin.Context) {\n        modifyBeat, err := strconv.ParseBool(c.Query(\"beat\"))\n        if err != nil {\n            c.Status(http.StatusBadRequest)\n        } else {\n            beat = modifyBeat\n            c.Status(http.StatusOK)\n        }\n    }\n}\n\\`\\`\\`\n\n---\n\n## 11. CI/CD & DEPLOYMENT\n\n### GitLab CI Pipeline\n\n\\`\\`\\`yaml\n# .gitlab-ci.yml\nvariables:\n  PORT: 8009\n\nstages:\n  - test\n  - build\n  - deploy_staging\n  - deploy_production\n\ntest:\n  stage: test\n  script: echo \"Running tests\"\n\nbuild:\n  stage: build\n  variables:\n    GOOS: linux\n    GOARCH: amd64\n  script:\n    - echo \"Building SaaS gateway\"\n    - env GOOS=$GOOS GOARCH=$GOARCH go build\n  tags:\n    - saas-builder\n  artifacts:\n    paths:\n      - .env, .env.local, .env.stage, .env.production\n      - saas-gateway\n      - scripts/start.sh\n    expire_in: 1 week\n  only:\n    - master\n    - staging\n\ndeploy_staging:\n  stage: deploy_staging\n  variables:\n    ENV: STAGE\n    GOGC: 250  # Aggressive garbage collection\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - deployer-stage-search\n  when: manual\n  only:\n    - master\n    - staging\n\ndeploy_prod_1:\n  stage: deploy_production\n  variables:\n    ENV: PRODUCTION\n    GOGC: 250\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - cb1-1  # Node 1\n  when: manual\n  only:\n    - master\n\ndeploy_prod_2:\n  stage: deploy_production\n  variables:\n    ENV: PRODUCTION\n    GOGC: 250\n  script:\n    - /bin/bash scripts/start.sh\n  tags:\n    - cb2-1  # Node 2\n  when: manual\n  only:\n    - master\n\\`\\`\\`\n\n### Deployment Script\n\n\\`\\`\\`bash\n# scripts/start.sh\n#!/bin/bash\nulimit -n 100000  # Increase file descriptors\n\n# Check if process already running\nPID=$(ps aux | grep saas-gateway | grep -v grep | awk '{print $2}')\n\nif [ -z \"$PID\" ]; then\n    echo \"SaaS gateway is not running\"\nelse\n    # Graceful shutdown\n    echo \"Bringing OOLB (Out of Load Balancer)\"\n    curl -vXPUT http://localhost:$PORT/heartbeat/?beat=false\n\n    # Wait for in-flight requests\n    sleep 15\n\n    echo \"Killing SaaS Gateway\"\n    kill -9 $PID\nfi\n\nsleep 10\n\n# Start new process\necho \"Starting SaaS Gateway\"\nENV=$CI_ENVIRONMENT_NAME ./saas-gateway >> \"logs/out.log\" 2>&1 &\n\n# Wait for startup\nsleep 20\n\n# Bring back into load balancer\necho \"Bringing in LB (Load Balancer)\"\ncurl -vXPUT http://localhost:$PORT/heartbeat/?beat=true\n\\`\\`\\`\n\n---\n\n## 12. CONFIGURATION\n\n### Environment Variables\n\n\\`\\`\\`go\n// config/config.go\ntype Config struct {\n    Port                   string      // Server port (default: 8009)\n    HMAC_SECRET            string      // Base64-encoded JWT signing secret\n    Env                    string      // Environment: local, STAGE, PRODUCTION\n    SAAS_URL               string      // Main backend URL\n    SAAS_DATA_URL          string      // Data service URL\n    IDENTITY_URL           string      // Identity service endpoint\n    PARTNER_URL            string      // Partner service endpoint\n    SENTRY_DSN             string      // Sentry error tracking\n    ERP_CLIENT_ID          string      // ERP client identifier\n    LOG_LEVEL              int         // 0=verbose, 3=silent\n    RedisClusterAddresses  []string    // Redis cluster nodes\n    REDIS_CLUSTER_PASSWORD string      // Redis auth password\n}\n\\`\\`\\`\n\n### Environment Configuration\n\n| Config | Local | Staging | Production |\n|--------|-------|---------|------------|\n| SAAS_URL | localhost:7179 | stage-search:7179 | coffee.goodcreator.co |\n| IDENTITY_URL | sb2:7000/identity-service/api | sb2:7000/identity-service/api | identityservice.bulbul.tv/identity-service/api |\n| PARTNER_URL | sb2:7100/product-service/api | sb2:7100/product-service/api | productservice.bulbul.tv/product-service/api |\n| Redis Nodes | 6 (localhost:30001-6) | 3 (stage:6379-6381) | 3 (prod:6379-6381) |\n| LOG_LEVEL | 2 | 2 | 2 |\n\n---\n\n## 13. KEY METRICS SUMMARY\n\n| Metric | Value |\n|--------|-------|\n| **Framework** | Gin v1.8.1 |\n| **Go Version** | 1.12 |\n| **Binary Size** | 23MB |\n| **Services Proxied** | 13 |\n| **Middleware Layers** | 7 |\n| **CORS Origins** | 41 |\n| **Whitelisted Headers** | 25+ |\n| **Authentication** | JWT + Redis |\n| **Cache Layer 1** | Ristretto (10M keys, 1GB) |\n| **Cache Layer 2** | Redis Cluster (3-6 nodes) |\n| **Prometheus Metrics** | 4 counters |\n| **Production Nodes** | 2 (multi-node) |\n| **Port** | 8009 |\n| **Debug Port** | 6069 (pprof) |\n\n---\n\n## 14. SKILLS DEMONSTRATED\n\n### API Gateway Engineering\n- **Reverse Proxy**: Using Go's \\`httputil.NewSingleHostReverseProxy\\`\n- **Middleware Pipeline**: 7-layer request processing\n- **Request Transformation**: Header enrichment, path rewriting\n- **Response Handling**: CORS, metrics, logging\n\n### Authentication & Security\n- **JWT Validation**: HMAC-SHA256 signature verification\n- **Session Management**: Redis cluster with fallback\n- **Plan-Based Authorization**: Contract date validation\n- **Multi-tenant Support**: Partner ID and plan type propagation\n\n### Caching Architecture\n- **Two-Layer Caching**: Ristretto (local) + Redis (distributed)\n- **LFU Eviction**: Ristretto's frequency-based eviction\n- **Connection Pooling**: 100 connections to Redis cluster\n\n### Observability\n- **Prometheus Metrics**: Request counters with labels\n- **Sentry Integration**: Stack traces and breadcrumbs\n- **Structured Logging**: Zerolog + Logrus\n- **Health Checks**: Graceful LB integration\n\n### DevOps\n- **CI/CD**: GitLab pipeline with multi-node deployment\n- **Graceful Deployment**: Health state toggling\n- **Configuration Management**: Environment-based configs\n\n---\n\n## 15. INTERVIEW TALKING POINTS\n\n### 1. \"Tell me about an API gateway you built\"\n- **Architecture**: 7-layer middleware pipeline with reverse proxy\n- **Authentication**: JWT + Redis session caching with Identity service fallback\n- **Caching**: Two-layer (Ristretto 1GB + Redis cluster)\n- **Scale**: 13 microservices, dual-node production\n\n### 2. \"Describe your approach to authentication\"\n- **JWT**: HMAC-SHA256 with claims extraction\n- **Session**: Redis cluster with \\`session:{id}\\` keys\n- **Fallback**: Identity service API when cache miss\n- **Plan Validation**: Contract date range checking\n\n### 3. \"How do you handle multi-tenancy?\"\n- **Partner ID**: Extracted from JWT, enriched to headers\n- **Plan Type**: Validated against contract dates\n- **Header Propagation**: \\`x-bb-partner-id\\`, \\`x-bb-plan-type\\`\n\n### 4. \"Explain your caching strategy\"\n- **Layer 1**: Ristretto (10M keys, 1GB, LFU, per-instance)\n- **Layer 2**: Redis cluster (3-6 nodes, shared)\n- **Pattern**: Check L1 → Check L2 → Fetch source → Populate both\n\n### 5. \"How do you ensure zero-downtime deployments?\"\n- **Health Endpoint**: \\`/heartbeat/?beat=false\\` to remove from LB\n- **Drain Period**: 15 seconds for in-flight requests\n- **Restart**: Kill and restart with new binary\n- **Re-enable**: \\`/heartbeat/?beat=true\\` after stabilization\n\n---\n\n*Analysis covers the complete SaaS Gateway implementation with 7-layer middleware, two-tier caching, JWT authentication, and multi-node deployment for a creator platform serving 13 microservices.*\n"
  }
];
